{
    "data": [
        {
            "abstract": "Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.", 
            "authors": [
                "M E J Newman"
            ], 
            "fields": [
                "Models of abnormality", 
                "Biological network", 
                "Empirical research", 
                "Social network", 
                "Percolation theory"
            ], 
            "title": "The Structure and Function of Complex Networks", 
            "url": "http://cn.bing.com/academic/profile?id=daba24fc0e69e92d6cd80f94f025a8ba&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.", 
            "authors": [
                "Whitfield Diffie", 
                "Martin E Hellman"
            ], 
            "fields": [
                "Computer hardware", 
                "Public-key cryptography", 
                "Information theory", 
                "Key distribution", 
                "Computer network"
            ], 
            "title": "New directions in cryptography", 
            "url": "http://cn.bing.com/academic/profile?id=75dd1577f13d50ace102583c1ac20aa1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "\"Grid\" computing has emerged as an important new field, distinguished from conventional distributed computing by its focus on large-scale resource sharing, innovative applications, and, in some cases, high performance orientation. In this article, the authors define this new field. First, they review the \"Grid problem,\" which is defined as flexible, secure, coordinated resource sharing among dynamic collections of individuals, institutions, and resources--what is referred to as virtual organizations. In such settings, unique authentication, authorization, resource access, resource discovery, and other challenges are encountered. It is this class of problem that is addressed by Grid technologies. Next, the authors present an extensible and open Grid architecture, in which protocols, services, application programming interfaces, and software development kits are categorized according to their roles in enabling resource sharing. The authors describe requirements that they believe any such mechanisms must satisfy and discuss the importance of defining a compact set of intergrid protocols to enable interoperability among different Grid systems. Finally, the authors discuss how Grid technologies relate to other contemporary technologies, including enterprise integration, application service provider, storage service provider, and peer-to-peer computing. They maintain that Grid concepts and technologies complement and have much to contribute to these other approaches.", 
            "authors": [
                "Ian Foster", 
                "Carl Kesselman", 
                "Steven Tuecke"
            ], 
            "fields": [
                "Computer network", 
                "Enterprise integration", 
                "Application programming interface", 
                "Grid computing", 
                "Application service provider"
            ], 
            "title": "The Anatomy of the Grid: Enabling Scalable Virtual Organizations", 
            "url": "http://cn.bing.com/academic/profile?id=b3236b722f3ec93705cde2e8e409dce7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be multicast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. We study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the max-flow min-cut theorem for network information flow. Contrary to one's intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a \"fluid\" which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved. This finding may have significant impact on future design of switching systems.", 
            "authors": [
                "Rudolf Ahlswede", 
                "Ning Cai", 
                "S Y R Li", 
                "Raymond W Yeung"
            ], 
            "fields": [
                "Index term", 
                "SPINE", 
                "Source code", 
                "Computer network", 
                "Point-to-point"
            ], 
            "title": "Network information flow", 
            "url": "http://cn.bing.com/academic/profile?id=59467a91a532e3052ae6dd97eecac312&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.", 
            "authors": [
                "Leslie Lamport"
            ], 
            "fields": [
                "Computer network", 
                "Distributed algorithm", 
                "Partially ordered set", 
                "Distributed computing", 
                "Total order"
            ], 
            "title": "Time, clocks, and the ordering of events in a distributed system", 
            "url": "http://cn.bing.com/academic/profile?id=62db1ae6a1921f8d6704932060b69cdb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Summary The NMRPipe system is a UNIX software environment of processing, graphics, and analysis tools designed to meet current routine and research-oriented multidimensional processing requirements, and to anticipate and accommodate future demands and developments. The system is based on UNIX pipes, which allow programs running simultaneously to exchange streams of data under user control. In an NMRPipe processing scheme, a stream of spectral data flows through a pipeline of processing programs, each of which performs one component of the overall scheme, such as Fourier transformation or linear prediction. Complete multidimensional processing schemes are constructed as simple UNIX shell scripts. The processing modules themselves maintain and exploit accurate records of data sizes, detection modes, and calibration information in all dimensions, so that schemes can be constructed without the need to explicitly define or anticipate data sizes or storage details of real and imaginary channels during processing. The asynchronous pipeline scheme provides other substantial advantages, including high flexibility, favorable processing speeds, choice of both all-in-memory and disk-bound processing, easy adaptation to different data formats, simpler software development and maintenance, and the ability to distribute processing tasks on multi-CPU computers and computer networks.", 
            "authors": [
                "Frank Delaglio", 
                "Stephan Grzesiek", 
                "G W Vuister", 
                "Guang Zhu", 
                "J Pfeifer", 
                "Ad Bax"
            ], 
            "fields": [
                "Software development", 
                "Computer network", 
                "Data flow diagram", 
                "Principle of maximum entropy", 
                "Fourier transform"
            ], 
            "title": "NMRPipe: A multidimensional spectral processing system based on UNIX pipes*", 
            "url": "http://cn.bing.com/academic/profile?id=c7af8ca980ed9f70a847c9bf56f9b956&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper reviews the origins and definitions of social capital in the writings of Bourdieu, Loury, and Coleman, among other authors. It distinguishes four sources of social capital and examines their dynamics. Applications of the concept in the sociological literature emphasize its role in social control, in family support, and in benefits mediated by extrafamilial networks. I provide examples of each of these positive functions. Negative consequences of the same processes also deserve attention for a balanced picture of the forces at play. I review four such consequences and illustrate them with relevant examples. Recent writings on social capital have extended the concept from an individual asset to a feature of communities and even nations. The final sections describe this conceptual stretch and examine its limitations. I argue that, as shorthand for the positive consequences of sociability, social capital has a definite place in sociological theory. However, excessive extensions of the concept may j...", 
            "authors": [
                "Alejandro Portes"
            ], 
            "fields": [
                "Social capital", 
                "Social dynamics", 
                "Social control", 
                "Conceptual framework", 
                "Community"
            ], 
            "title": "Social Capital: Its Origins and Applications in Modern Sociology", 
            "url": "http://cn.bing.com/academic/profile?id=2eab2a2da0b5b5ec8fc9ac2d3728078f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: A number of recent studies have focused on the statistical properties of networked systems such as social networks and the World-Wide Web. Researchers have concentrated particularly on a few properties which seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this paper, we highlight another property which is found in many networks, the property of community structure, in which network nodes are joined together in tightly-knit groups between which there are only looser connections. We propose a new method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer generated and real-world graphs whose community structure is already known, and find that it detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well-known - a collaboration network and a food web - and find that it detects significant and informative community divisions in both cases.", 
            "authors": [
                "Michelle Girvan", 
                "M E J Newman"
            ], 
            "fields": [
                "Computer simulation", 
                "Power law", 
                "Community structure", 
                "Community", 
                "Degree distribution"
            ], 
            "title": "Community Structure in Social and Biological Networks", 
            "url": "http://cn.bing.com/academic/profile?id=c157da3ad6814c78739004bc888ad22b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF)-based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It combines empirical measurements with signal propagation modeling to determine user location and thereby enable location-aware services and applications. We present experimental results that demonstrate the ability of RADAR to estimate user location with a high degree of accuracy.", 
            "authors": [
                "P Bahl", 
                "Venkat Padmanabhan"
            ], 
            "fields": [
                "Signal processing", 
                "Tracking system", 
                "Mobile computing", 
                "Radar", 
                "Signal strength"
            ], 
            "title": "RADAR: an in-building RF-based user location and tracking system", 
            "url": "http://cn.bing.com/academic/profile?id=48cefd623c6e1ded923c33d56aa336e4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The self-organized map, an architecture suggested for artificial neural networks, is explained by presenting simulation experiments and practical applications. The self-organizing map has the property of effectively creating spatially organized internal representations of various features of input signals and their abstractions. One result of this is that the self-organization process can discover semantic relationships in sentences. Brain maps, semantic maps, and early work on competitive learning are reviewed. The self-organizing map algorithm (an algorithm which order responses spatially) is reviewed, focusing on best matching cell selection and adaptation of the weight vectors. Suggestions for applying the self-organizing map algorithm, demonstrations of the ordering process, and an example of hierarchical clustering of data are presented. Fine tuning the map by learning vector quantization is addressed. The use of self-organized maps in practical speech recognition and a simulation experiment on semantic mapping are discussed. >", 
            "authors": [
                "Teuvo Kohonen"
            ], 
            "fields": [
                "Algorithm", 
                "Vector quantization", 
                "Pattern recognition", 
                "Brain mapping", 
                "Cluster analysis"
            ], 
            "title": "The self-organizing map", 
            "url": "http://cn.bing.com/academic/profile?id=b56c57a80db2a235baee0f149497a5a8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the  directed diffusion  paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.", 
            "authors": [
                "Chalermek Intanagonwiwat", 
                "Ramesh Govindan", 
                "Deborah Estrin"
            ], 
            "fields": [
                "Web application", 
                "Interactivity", 
                "Computer network", 
                "Wireless sensor network", 
                "Evaluation"
            ], 
            "title": "Directed diffusion: a scalable and robust communication paradigm for sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=e25d46e407ed7f9f2ee49811ddcff19d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-and-forward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2-6 times more energy than S-MAC for traffic load with messages sent every 1-10 s.", 
            "authors": [
                "Wei Ye", 
                "John Heidemann", 
                "Deborah Estrin"
            ], 
            "fields": [
                "IEEE 802.11", 
                "Sensor", 
                "Energy conservation", 
                "Wireless Application Protocol", 
                "Message passing"
            ], 
            "title": "An energy-efficient MAC protocol for wireless sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=06faee4189190dc717cb588ea386f4c0&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Today's wireless networks are characterized by a fixed spectrum assignment policy. However, a large portion of the assigned spectrum is used sporadically and geographical variations in the utilization of assigned spectrum ranges from 15% to 85% with a high variance in time. The limited available spectrum and the inefficiency in the spectrum usage necessitate a new communication paradigm to exploit the existing wireless spectrum opportunistically. This new networking paradigm is referred to as NeXt Generation (xG) Networks as well as Dynamic Spectrum Access (DSA) and cognitive radio networks. The term xG networks is used throughout the paper. The novel functionalities and current research challenges of the xG networks are explained in detail. More specifically, a brief overview of the cognitive radio technology is provided and the xG network architecture is introduced. Moreover, the xG network functions such as spectrum management, spectrum mobility and spectrum sharing are explained in detail. The influence of these functions on the performance of the upper layer protocols such as routing and transport are investigated and open research issues in these areas are also outlined. Finally, the cross-layer design challenges in xG networks are discussed.", 
            "authors": [
                "Ian F Akyildiz", 
                "Wonyeol Lee", 
                "Mehmet C Vuran", 
                "Shantidev Mohanty"
            ], 
            "fields": [
                "Computer network", 
                "Public-key cryptography", 
                "Frequency allocation", 
                "Spectrum management", 
                "Routing protocol"
            ], 
            "title": "NeXt generation/dynamic spectrum access/cognitive radio wireless networks: a survey", 
            "url": "http://cn.bing.com/academic/profile?id=ee12cc5fb2071d6895554fc14e7d5107&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An ad hoc mobile network is a collection of mobile nodes that are dynamically and arbitrarily located in such a manner that the interconnections between nodes are capable of changing on a continual basis. In order to facilitate communication within the network, a routing protocol is used to discover routes between nodes. The primary goal of such an ad hoc network routing protocol is correct and efficient route establishment between a pair of nodes so that messages may be delivered in a timely manner. Route construction should be done with a minimum of overhead and bandwidth consumption. This article examines routing protocols for ad hoc networks and evaluates these protocols based on a given set of parameters. The article provides an overview of eight different protocols by presenting their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks.", 
            "authors": [
                "E M Royer", 
                "C K Toh"
            ], 
            "fields": [
                "Bandwidth", 
                "Computer network", 
                "Routing protocol", 
                "Base station", 
                "Mobile telephony"
            ], 
            "title": "A review of current routing protocols for ad hoc mobile wireless networks", 
            "url": "http://cn.bing.com/academic/profile?id=ac0c542a022a274db7c9af11e3154aae&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide area traces, investigating a number of wide area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remote-login and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into \"connection bursts\", the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of wide area traffic. >", 
            "authors": [
                "Vern Paxson", 
                "Sally Floyd"
            ], 
            "fields": [
                "Poisson regression", 
                "Poisson process", 
                "Transport layer", 
                "Probability distribution", 
                "Local area network"
            ], 
            "title": "Wide area traffic: the failure of Poisson modeling", 
            "url": "http://cn.bing.com/academic/profile?id=2415ba38e75aff1e9574bde16be36883&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: In a cell or microorganism the processes that generate mass, energy, information transfer, and cell fate specification are seamlessly integrated through a complex network of various cellular constituents and reactions. However, despite the key role these networks play in sustaining various cellular functions, their large-scale structure is essentially unknown. Here we present the first systematic comparative mathematical analysis of the metabolic networks of 43 organisms representing all three domains of life. We show that, despite significant variances in their individual constituents and pathways, these metabolic networks display the same topologic scaling properties demonstrating striking similarities to the inherent organization of complex non-biological systems. This suggests that the metabolic organization is not only identical for all living organisms, but complies with the design principles of robust and error-tolerant scale-free networks, and may represent a common blueprint for the large-scale organization of interactions among all cellular constituents.", 
            "authors": [
                "Hawoong Jeong", 
                "B Tombor", 
                "Reka Albert", 
                "Zoltan N Oltvai", 
                "A L Barabasi"
            ], 
            "fields": [
                "Developmental biology", 
                "Marine biology", 
                "Small-world network", 
                "Metabolism", 
                "Biotechnology"
            ], 
            "title": "The large-scale organization of metabolic networks.", 
            "url": "http://cn.bing.com/academic/profile?id=235ca47941389e7e413dfaa334a77d85&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Complex networks are studied across many fields of science. To uncover their structural design principles, we defined \u201cnetwork motifs,\u201d patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. We found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. The motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of Escherichia coli and Saccharomyces cerevisiae or from those found in the World Wide Web. Similar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in Caenorhabditis elegans. Motifs may thus define universal classes of networks. This", 
            "authors": [
                "Ron Milo", 
                "Shai Shenorr", 
                "Shalev Itzkovitz", 
                "Nadav Kashtan", 
                "Dmitri B Chklovskii", 
                "Uri Alon"
            ], 
            "fields": [
                "Computer network", 
                "Artificial neural network", 
                "Food web", 
                "Complex network", 
                "Escherichia coli"
            ], 
            "title": "Network Motifs: Simple Building Blocks of Complex Networks", 
            "url": "http://cn.bing.com/academic/profile?id=871ed45d38b6707495ad3fea50afb1c9&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The Globus system is intended to achieve a vertically integrated treatment of application, middleware, and net work. A low-level toolkit provides basic mechanisms such as communication, authentication, network information, and data access. These mechanisms are used to con struct various higher level metacomputing services, such as parallel programming tools and schedulers. The long- term goal is to build an adaptive wide area resource environment AWARE, an integrated set of higher level services that enable applications to adapt to heteroge neous and dynamically changing metacomputing environ ments. Preliminary versions of Globus components were deployed successfully as part of the I-WAY networking experiment.", 
            "authors": [
                "Ian Foster", 
                "Carl Kesselman"
            ], 
            "fields": [
                "Programming", 
                "Vertical integration", 
                "Computer network", 
                "Data access", 
                "Memory management"
            ], 
            "title": "Globus: a Metacomputing Infrastructure Toolkit", 
            "url": "http://cn.bing.com/academic/profile?id=24debe971ebb757cdf37e8cbc0de3b97&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world and spread throughout our environment like smart dust. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances.", 
            "authors": [
                "Jason L Hill", 
                "Robert Szewczyk", 
                "Alec Woo", 
                "Seth Hollar", 
                "David E Culler", 
                "Kristofer S J Pister"
            ], 
            "fields": [
                "Scheduling", 
                "Reactive system", 
                "Microcontroller", 
                "Methodology", 
                "Operating system"
            ], 
            "title": "System architecture directions for networked sensors", 
            "url": "http://cn.bing.com/academic/profile?id=aa96364d962a53a5e9c13255b4f397cf&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We show how to use \"complementary priors\" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.", 
            "authors": [
                "Geoffrey E Hinton", 
                "Simon Osindero", 
                "Yeewhye Teh"
            ], 
            "fields": [
                "Artificial neural network", 
                "Machine", 
                "Prior probability", 
                "Greedy algorithm", 
                "Computer network"
            ], 
            "title": "A fast learning algorithm for deep belief nets", 
            "url": "http://cn.bing.com/academic/profile?id=17d47c184b5c67d42650a0aa38117d73&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract This article builds a theoretical framework to help explain governance patterns in global value chains. It draws on three streams of literature \u2013 transaction costs economics, production networks, and technological capability and firm-level learning \u2013 to identify three variables that play a large role in determining how global value chains are governed and change. These are: (1) the complexity of transactions, (2) the ability to codify transactions, and (3) the capabilities in the supply-base. The theory generates five types of global value chain governance \u2013 hierarchy, captive, relational, modular, and market \u2013 which range from high to low levels of explicit coordination and power asymmetry. The article highlights the dynamic and overlapping nature of global value chain governance through four brief industry case studies: bicycles, apparel, horticulture and electronics.", 
            "authors": [
                "Gary Gereffi", 
                "John Humphrey", 
                "Timothy Sturgeon"
            ], 
            "fields": [
                "Computer network", 
                "Transaction cost", 
                "Value chain", 
                "Corporate governance"
            ], 
            "title": "The governance of global value chains", 
            "url": "http://cn.bing.com/academic/profile?id=6630515e6b6e76a7bd35f5311f95658b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A wireless sensor network (WSN) has important applications such as remote environmental monitoring and target tracking. This has been enabled by the availability, particularly in recent years, of sensors that are smaller, cheaper, and intelligent. These sensors are equipped with wireless interfaces with which they can communicate with one another to form a network. The design of a WSN depends significantly on the application, and it must consider factors such as the environment, the application's design objectives, cost, hardware, and system constraints. The goal of our survey is to present a comprehensive review of the recent literature since the publication of [I.F. Akyildiz, W. Su, Y. Sankarasubramaniam, E. Cayirci, A survey on sensor networks, IEEE Communications Magazine, 2002]. Following a top-down approach, we give an overview of several new applications and then review the literature on various aspects of WSNs. We classify the problems into three different categories: (1) internal platform and underlying operating system, (2) communication protocol stack, and (3) network services, provisioning, and deployment. We review the major development in these three categories and outline new challenges.", 
            "authors": [
                "Jennifer Yick", 
                "Biswanath Mukherjee", 
                "Dipak Ghosal"
            ], 
            "fields": [
                "Wireless sensor network", 
                "Telecommunications service", 
                "Communications protocol", 
                "Sensor array", 
                "Availability"
            ], 
            "title": "Wireless sensor network survey", 
            "url": "http://cn.bing.com/academic/profile?id=6d923e8489f031d748950d0e182c6ab3&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Wireless mesh networks (WMNs) consist of mesh routers and mesh clients, where mesh routers have minimal mobility and form the backbone of WMNs. They provide network access for both mesh and conventional clients. The integration of WMNs with other networks such as the Internet, cellular, IEEE 802.11, IEEE 802.15, IEEE 802.16, sensor networks, etc., can be accomplished through the gateway and bridging functions in the mesh routers. Mesh clients can be either stationary or mobile, and can form a client mesh network among themselves and with mesh routers. WMNs are anticipated to resolve the limitations and to significantly improve the performance of ad hoc networks, wireless local area networks (WLANs), wireless personal area networks (WPANs), and wireless metropolitan area networks (WMANs). They are undergoing rapid progress and inspiring numerous deployments. WMNs will deliver wireless services for a large variety of applications in personal, local, campus, and metropolitan areas. Despite recent advances in wireless mesh networking, many research challenges remain in all protocol layers. This paper presents a detailed study on recent advances and open research issues in WMNs. System architectures and applications of WMNs are described, followed by discussing the critical factors influencing protocol design. Theoretical network capacity and the state-of-the-art protocols for WMNs are explored with an objective to point out a number of open research issues. Finally, testbeds, industrial practice, and current standard activities related to WMNs are highlighted.", 
            "authors": [
                "Ian F Akyildiz", 
                "Xudong Wang", 
                "Weilin Wang"
            ], 
            "fields": [
                "Routing", 
                "Sensor array", 
                "Transport layer", 
                "Wireless sensor network", 
                "Mesh networking"
            ], 
            "title": "Wireless mesh networks: a survey", 
            "url": "http://cn.bing.com/academic/profile?id=531779e6909e25caaf89ff5eebb74643&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.", 
            "authors": [
                "Vincent D Blondel", 
                "Jeanloup Guillaume", 
                "Renaud Lambiotte", 
                "Etienne Lefebvre"
            ], 
            "fields": [
                "Community structure", 
                "Random graph", 
                "Computer network", 
                "Statistical mechanics"
            ], 
            "title": "Fast unfolding of communities in large networks", 
            "url": "http://cn.bing.com/academic/profile?id=7906ec51c10bfa93ac5a597d97a3e37a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper describes systems that examine and react to an individual's changing context. Such systems can promote and mediate people's interactions with devices, computers, and other people, and they can help navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate environment is most important for this form of computing since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four catagories of context-aware applications: proximate selection, automatic contextual reconfiguration, contextual information and commands, and contex-triggered actions. Instances of these application types have been prototyped on the PARCTAB, a wireless, palm-sized computer.", 
            "authors": [
                "Bill N Schilit", 
                "N I Adams", 
                "Roy Want"
            ], 
            "fields": [
                "Mobile telephony", 
                "User interface", 
                "Reactive system", 
                "Embedded system", 
                "Navigation"
            ], 
            "title": "Context-Aware Computing Applications", 
            "url": "http://cn.bing.com/academic/profile?id=dd88a7bdc9e92af10ac04ebc7c06e45e&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper addresses the Internet of Things. Main enabling factor of this promising paradigm is the integration of several technologies and communications solutions. Identification and tracking technologies, wired and wireless sensor and actuator networks, enhanced communication protocols (shared with the Next Generation Internet), and distributed intelligence for smart objects are just the most relevant. As one can easily imagine, any serious contribution to the advance of the Internet of Things must necessarily be the result of synergetic activities conducted in different fields of knowledge, such as telecommunications, informatics, electronics and social science. In such a complex scenario, this survey is directed to those who want to approach this complex discipline and contribute to its development. Different visions of this Internet of Things paradigm are reported and enabling technologies reviewed. What emerges is that still major issues shall be faced by the research community. The most relevant among them are addressed in details.", 
            "authors": [
                "Luigi Atzori", 
                "Antonio Iera", 
                "Giacomo Morabito"
            ], 
            "fields": [
                "Sensor array", 
                "Radio-frequency identification", 
                "Remote sensing", 
                "Communications protocol", 
                "Ubiquitous computing"
            ], 
            "title": "The Internet of Things: A survey", 
            "url": "http://cn.bing.com/academic/profile?id=f83fbb7f3c8de6dd7055714382d9396c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#Previous research suggests that knowledge diffusion occurs more quickly within Toyota\u2019s production network than in competing automaker networks. In this paper we examine the \u2018black box\u2019 of knowledge sharing within Toyota\u2019s network and demonstrate that Toyota\u2019s ability to effectively create and manage network-level knowledge-sharing processes at least partially explains the relative productivity advantages enjoyed by Toyota and its suppliers. We provide evidence that suppliers do learn more quickly after participating in Toyota\u2019s knowledge-sharing network. Toyota\u2019s network has solved three fundamental dilemmas with regard to knowledge sharing by devising methods to (1) motivate members to participate and openly share valuable knowledge (while preventing undesirable spillovers to competitors), (2) prevent free riders, and (3) reduce the costs associated with finding and accessing different types of valuable knowledge. Toyota has done this by creating a strong network identity with rules for participation and entry into the network. Most importantly, production knowledge is viewed as the property of the network. Toyota\u2019s highly interconnected, strong tie network has established a variety of institutionalized routines that facilitate multidirectional knowledge flows among suppliers. Our study suggests that the notion of a dynamic learning capability that creates competitive advantage needs to be extended beyond firm boundaries. Indeed, if the network can create a strong identity and coordinating rules, then it will be superior to a firm as an organizational form at creating and recombining knowledge due to the diversity of knowledge that resides within a network. Copyright \u00a9 2000 John Wiley & Sons, Ltd.", 
            "authors": [
                "Jeffrey H Dyer", 
                "Kentaro Nobeoka"
            ], 
            "fields": [
                "Computer network", 
                "Knowledge management", 
                "Organization", 
                "Production system", 
                "Competitive advantage"
            ], 
            "title": "Creating and managing a high\u2010performance knowledge\u2010sharing network: the Toyota case", 
            "url": "http://cn.bing.com/academic/profile?id=6aa09d0ac0438d215a0d32144370e60a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In the 1990s the concept of social capital defined here as the norms and networks that enable people to act collectively enjoyed a remarkable rise to prominence across all the social science disciplines. The authors trace the evolution of social capital research as it pertains to economic development and identify four distinct approaches the research has taken : communitarian, networks, institutional, and synergy. The evidence suggests that of the four, the synergy view, with its emphasis on incorporating different levels and dimensions of social capital and its recognition of the positive and negative outcomes that social capital can generate, has the greatest empirical support and lends itself best to comprehensive and coherent policy prescriptions. The authors argue that a significant virtue of the idea of and discourse on social capital is that it helps to bridge orthodox divides among scholars, practitioners, and policymakers.", 
            "authors": [
                "Michael Woolcock", 
                "Deepa Narayan"
            ], 
            "fields": [
                "Stakeholder", 
                "Social science", 
                "Public good", 
                "Decision-making", 
                "Capitalism"
            ], 
            "title": "Social Capital: Implications for Development Theory, Research, and Policy", 
            "url": "http://cn.bing.com/academic/profile?id=fae20a02bfa160b75cd9b6dca23edfe5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A learning algorithm for multilayer feedforward networks, RPROP (resilient propagation), is proposed. To overcome the inherent disadvantages of pure gradient-descent, RPROP performs a local adaptation of the weight-updates according to the behavior of the error function. Contrary to other adaptive techniques, the effect of the RPROP adaptation process is not blurred by the unforeseeable influence of the size of the derivative, but only dependent on the temporal behavior of its sign. This leads to an efficient and transparent adaptation process. The capabilities of RPROP are shown in comparison to other adaptive techniques. >", 
            "authors": [
                "Martin Riedmiller", 
                "Heinrich Braun"
            ], 
            "fields": [
                "Gradient descent", 
                "Supervised learning", 
                "Convergence", 
                "Adaptive system", 
                "Error function"
            ], 
            "title": "A direct adaptive method for faster backpropagation learning: the RPROP algorithm", 
            "url": "http://cn.bing.com/academic/profile?id=37507390bc03fc84d523827ebce79c00&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Generative communication is the basis of a new distributed programming langauge that is intended for systems programming in distributed settings generally and on integrated network computers in particular. It differs from previous interprocess communication models in specifying that messages be added in tuple-structured form to the computation environment, where they exist as named, independent entities until some process chooses to receive them. Generative communication results in a number of distinguishing properties in the new language, Linda, that is built around it. Linda is fully distributed in space and distributed in time; it allows distributed sharing, continuation passing, and structured naming. We discuss these properties and their implications, then give a series of examples. Linda presents novel implementation problems that we discuss in Part II. We are particularly concerned with implementation of the dynamic global name space that the generative communication model requires.", 
            "authors": [
                "David Gelernter"
            ], 
            "fields": [
                "System programming", 
                "Computer network", 
                "Programming language", 
                "Models of communication", 
                "Distributed computing"
            ], 
            "title": "Generative communication in Linda", 
            "url": "http://cn.bing.com/academic/profile?id=d81a4146b6343e947e402c35f2271703&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: Many complex systems in nature and society can be described in terms of networks capturing the intricate web of connections among the units they are made of. A key question is how to interpret the global organization of such networks as the coexistence of their structural subunits (communities) associated with more highly interconnected parts. Identifying these a priori unknown building blocks (such as functionally related proteins, industrial sectors and groups of people) is crucial to the understanding of the structural and functional properties of networks. The existing deterministic methods used for large networks find separated communities, whereas most of the actual networks are made of highly overlapping cohesive groups of nodes. Here we introduce an approach to analysing the main statistical features of the interwoven sets of overlapping communities that makes a step towards uncovering the modular structure of complex systems. After defining a set of new characteristic quantities for the statistics of communities, we apply an efficient technique for exploring overlapping communities on a large scale. We find that overlaps are significant, and the distributions we introduce reveal universal features of networks. Our studies of collaboration, word-association and protein interaction graphs show that the web of communities has non-trivial correlations and specific scaling properties.", 
            "authors": [
                "Gergely Palla", 
                "Imre Derenyi", 
                "Illes J Farkas", 
                "Tamas Vicsek"
            ], 
            "fields": [
                "The Internet", 
                "Computational biology", 
                "Genetics", 
                "Cell cycle", 
                "Earth science"
            ], 
            "title": "Uncovering the overlapping community structure of complex networks in nature and society", 
            "url": "http://cn.bing.com/academic/profile?id=6da17a242a745b4e0624b8fc33bd515a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Questions of belief are essential in analysing protocols for the authentication of principals in distributed computing systems. In this paper we motivate, set out, and exemplify a logic specifically designed for this analysis: we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. Our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. It has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. The reasoning about some protocols has been mechanically verified. This paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. Some of the examples are chosen because of their practical importance, whereas others serve to illustrate subtle points of the logic and to explain how we use it. We discuss extensions of the logic motivated by actual practice; for example, to account for the use of hash functions in signatures. The final sections contain a formal semantics of the logic and some conclusions.", 
            "authors": [
                "Michael T Burrows", 
                "Martin Abadi", 
                "Roger M Needham"
            ], 
            "fields": [
                "Verification", 
                "Reason", 
                "Distributed computing", 
                "Hash function", 
                "Logic"
            ], 
            "title": "A Logic of Authentication", 
            "url": "http://cn.bing.com/academic/profile?id=0681f61aa55fd106298e9fea457bbe59&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.", 
            "authors": [
                "Scott E Fahlman", 
                "Christian Lebiere"
            ], 
            "fields": [
                "Topology", 
                "Algorithm", 
                "Structures", 
                "Chain propagation", 
                "Computer architecture"
            ], 
            "title": "The cascade-correlation learning architecture", 
            "url": "http://cn.bing.com/academic/profile?id=89001590657b4ee6a450d508272df083&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We propose  B-MAC , a carrier sense media access protocol for wireless sensor networks that provides a flexible interface to obtain ultra low power operation, effective collision avoidance, and high channel utilization. To achieve low power operation,  B-MAC  employs an adaptive preamble sampling scheme to reduce duty cycle and minimize idle listening.  B-MAC  supports on-the-fly reconfiguration and provides bidirectional interfaces for system services to optimize performance, whether it be for throughput, latency, or power conservation. We build an analytical model of a class of sensor network applications. We use the model to show the effect of changing  B-MAC 's parameters and predict the behavior of sensor network applications. By comparing  B-MAC  to conventional 802.11-inspired protocols, specifically SMAC, we develop an experimental characterization of  B-MAC  over a wide range of network conditions. We show that  B-MAC 's flexibility results in better packet delivery rates, throughput, latency, and energy consumption than S-MAC. By deploying a real world monitoring application with multihop networking, we validate our protocol design and model. Our results illustrate the need for flexible protocols to effectively realize energy efficient sensor network applications.", 
            "authors": [
                "Joseph Polastre", 
                "Jason L Hill", 
                "David E Culler"
            ], 
            "fields": [
                "Computer network", 
                "Efficient energy use", 
                "Duty cycle", 
                "Wireless sensor network"
            ], 
            "title": "Versatile low power media access for wireless sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=d4e6622016b557f4a4927fc35b1851f1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The radial basis function network offers a viable alternative to the two-layer neural network in many applications of signal processing. A common learning algorithm for radial basis function networks is based on first choosing randomly some data points as radial basis function centers and then using singular-value decomposition to solve for the weights of the network. Such a procedure has several drawbacks, and, in particular, an arbitrary selection of centers is clearly unsatisfactory. The authors propose an alternative learning procedure based on the orthogonal least-squares method. The procedure chooses radial basis function centers one by one in a rational way until an adequate network has been constructed. In the algorithm, each selected center maximizes the increment to the explained variance or energy of the desired output and does not suffer numerical ill-conditioning problems. The orthogonal least-squares learning strategy provides a simple and efficient means for fitting radial basis function networks. This is illustrated using examples taken from two different signal processing applications. >", 
            "authors": [
                "S Chen", 
                "C F N Cowan", 
                "P M Grant"
            ], 
            "fields": [
                "Algorithm", 
                "Signal processing", 
                "Mathematical logic", 
                "Least squares", 
                "Singular value decomposition"
            ], 
            "title": "Orthogonal least squares learning algorithm for radial basis function networks", 
            "url": "http://cn.bing.com/academic/profile?id=431c50a4d7065595010b8b8e1de14d33&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Boschma R. A. (2005) Proximity and innovation: a critical assessment, Regional Studies39, 61-74. A key issue in economic geography is to determine the impact of geographical proximity on interactive learning and innovation. We argue that the importance of geographical proximity cannot be assessed in isolation, but should always be examined in relation to other dimensions of proximity that may provide alternative solutions to the problem of coordination. We claim that geographical proximity per se is neither a necessary nor a sufficient condition for learning to take place. Nevertheless, it facilitates interactive learning, most likely by strengthening the other dimensions of proximity. However, proximity may also have negative impacts on innovation due to the problem of lock-in. Accordingly, not only too little, but also too much proximity may be detrimental to interactive learning and innovation. This may be the case for all five dimensions of proximity discussed in the paper, i.e. cognitive, organizatio...", 
            "authors": [
                "Ron Boschma"
            ], 
            "fields": [
                "Innovation", 
                "Distance", 
                "Economic geography", 
                "Social network", 
                "Cognition"
            ], 
            "title": "Proximity and Innovation: A Critical Assessment", 
            "url": "http://cn.bing.com/academic/profile?id=72eebcfec23fe5cfc2f4f04ca9c978df&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In 1998, the Council of Logistics Management modified its definition of logistics to indicate that logistics is a subset of supply chain management and that the two terms are not synonymous. Now that this difference has been recognized by the premier logistics professional organization, the challenge is to determine how to successfully implement supply chain management. This paper concentrates on operationalizing the supply chain management framework suggested in a 1997 article. Case studies conducted at several companies and involving multiple members of supply chains are used to illustrate the concepts described.", 
            "authors": [
                "Douglas M Lambert", 
                "Martha C Cooper", 
                "Janus D Pagh"
            ], 
            "fields": [
                "Management", 
                "Strategic management", 
                "Computer network", 
                "Logistics", 
                "Supply chain management"
            ], 
            "title": "Supply Chain Management: Implementation Issues and Research Opportunities", 
            "url": "http://cn.bing.com/academic/profile?id=64978279ccbbbbf5343c3f245d6c9f9f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The notion of self-similarity has been shown to apply to wide-area and local-area network traffic. We show evidence that the subset of network traffic that is due to World Wide Web (WWW) transfers can show characteristics that are consistent with self-similarity, and we present a hypothesized explanation for that self-similarity. Using a set of traces of actual user executions of NCSA Mosaic, we examine the dependence structure of WWW traffic. First, we show evidence that WWW traffic exhibits behavior that is consistent with self-similar traffic models. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user \"think time\", and the superimposition of many such transfers in a local-area network. To do this, we rely on empirically measured distributions both from client traces and from data independently collected at WWW servers.", 
            "authors": [
                "Mark Crovella", 
                "Azer Bestavros"
            ], 
            "fields": [
                "Heavy-tailed distribution", 
                "Statistics", 
                "Self-similarity", 
                "Computer network", 
                "Local area network"
            ], 
            "title": "Self-similarity in World Wide Web traffic: evidence and possible causes", 
            "url": "http://cn.bing.com/academic/profile?id=90a546b336927fddd571f1d2bb65921c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Use of encryption to achieve authenticated communication in computer networks is discussed. Example protocols are presented for the establishment of authenticated connections, for the management of authenticated mail, and for signature verification and document integrity guarantee. Both conventional and public-key encryption algorithms are considered as the basis for protocols.", 
            "authors": [
                "Roger M Needham", 
                "Michael D Schroeder"
            ], 
            "fields": [
                "Information security", 
                "Communications protocol", 
                "Computer network", 
                "Encryption", 
                "Public-key cryptography"
            ], 
            "title": "Using encryption for authentication in large networks of computers", 
            "url": "http://cn.bing.com/academic/profile?id=115fbca2af9ec86908bad9d5fab50853&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper proposes S-MAC, a medium access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with nodes remaining largely inactive for long time, but becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in several ways: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses a few novel techniques to reduce energy consumption and support self-configuration. It enables low-duty-cycle operation in a multihop network. Nodes form virtual clusters based on common sleep schedules to reduce control overhead and enable traffic-adaptive wake-up. S-MAC uses in-channel signaling to avoid overhearing unnecessary traffic. Finally, S-MAC applies message passing to reduce contention latency for applications that require in-network data processing. The paper presents measurement results of S-MAC performance on a sample sensor node, the UC Berkeley Mote, and reveals fundamental tradeoffs on energy, latency and throughput. Results show that S-MAC obtains significant energy savings compared with an 802.11-like MAC without sleeping.", 
            "authors": [
                "Wei Ye", 
                "John Heidemann", 
                "Deborah Estrin"
            ], 
            "fields": [
                "Environmental monitoring", 
                "Wireless sensor network", 
                "Adaptive control", 
                "IEEE 802.11", 
                "Wireless network"
            ], 
            "title": "Medium access control with coordinated adaptive sleeping for wireless sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=a5990d7db1a0704ac4eac32c6a204e91&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Advances in processor, memory, and radio technology will enable small and cheap nodes capable of sensing, communication, and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the  directed-diffusion  paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed-diffusion-based network are application aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network (e.g., data aggregation). We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network analytically and experimentally. Our evaluation indicates that directed diffusion can achieve significant energy savings and can outperform idealized traditional schemes (e.g., omniscient multicast) under the investigated scenarios.", 
            "authors": [
                "Chalermek Intanagonwiwat", 
                "Ramesh Govindan", 
                "Deborah Estrin", 
                "John Heidemann", 
                "Fabio Silva"
            ], 
            "fields": [
                "Computer Science", 
                "Robustness", 
                "Data aggregator", 
                "Sensor fusion", 
                "Remote sensing"
            ], 
            "title": "Directed diffusion for wireless sensor networking", 
            "url": "http://cn.bing.com/academic/profile?id=dea1ae2a08431ac6a7296058b96e71a2&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "While the recent focus on knowledge has undoubtedly benefited organizational studies, the literature still presents a sharply contrasting and even contradictory view of knowledge, which at times is described as \"sticky\" and at other times \"leaky.\" This paper is written on the premise that there is more than a problem with metaphors at issue here, and more than accounts of different types of knowledge (such as \"tacit\" and \"explicit\") can readily explain. Rather, these contrary descriptions of knowledge reflect different, partial, and sometimes \"balkanized\" perspectives from which knowledge and organization are viewed. Taking the community of practice as a unifying unit of analysis for understanding knowledge in the firm, the paper suggests that often too much attention is paid to the idea of community, too little to the implications of practice. Practice, we suggest, creates epistemic differences among the communities within a firm, and the firm's advantage over the market lies in dynamically coordinating the knowledge produced by these communities despite such differences. In making this argument, we argue that analyses of systemic innovation should be extended to embrace all firms in a knowledge economy, not just the classically innovative. This extension will call for a transformation of conventional ideas coordination and of the trade-off between exploration and exploitation.", 
            "authors": [
                "John Seely Brown", 
                "Paul Duguid"
            ], 
            "fields": [
                "Computer network", 
                "Knowledge"
            ], 
            "title": "Knowledge and Organization: A Social-Practice Perspective", 
            "url": "http://cn.bing.com/academic/profile?id=f2a21a67fcaddc988cc2168214cfed21&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An iterative mincut heuristic for partitioning networks is presented whose worst case computation time, per pass, grows linearly with the size of the network. In practice, only a very small number of passes are typically needed, leading to a fast approximation algorithm for mincut partitioning. To deal with cells of various sizes, the algorithm progresses by moving one cell at a time between the blocks of the partition while maintaining a desired balance based on the size of the blocks rather than the number of cells per block. Efficient data structures are used to avoid unnecessary searching for the best cell to move and to minimize unnecessary updating of cells affected by each move.", 
            "authors": [
                "Charles M Fiduccia", 
                "R M Mattheyses"
            ], 
            "fields": [
                "Polynomial", 
                "Time complexity", 
                "Data structure", 
                "Electronic design automation", 
                "Approximation algorithm"
            ], 
            "title": "A Linear-Time Heuristic for Improving Network Partitions", 
            "url": "http://cn.bing.com/academic/profile?id=7f705411843558931b912413871dac4d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The joint equihbrmm distribution of queue sizes in a network of queues containing N service centers and R classes of customers m derived The equilibrium state probabillUes have the general form P(S) = Cd(S) fl(xl)f2(x2) . fN(x~), where S is the state of the system, x, is the con- figuration of customers at the ~th service center, d(S) is a function of the state of the model, f, is a function that depends on the type of the zth service center, and C is a normalizing constant It is assumed that the eqmhbrlum probabfl~tles exmt and are unique Four types of service centers to model central processors, data channels, terminals, and routing delays are considered The queuemg dlSclphnes associated with these service centers include first-come-first -served, processor sharing, no queueing, and last-come-first- served Each customer belongs to a single class of customers while awaiting or receiving serwce at a service center, but may change classes and service centers according to fixed probabditms at the completion of a service request For open networks, state dependent arrival processes are considered Closed networks are those with no exogenous arrivals A network may be closed with respect to some classes of customers and open with respect to other classes of customers At three of the four types of serwce centers, the service times of customers are governed by probablhty dmtrlbutions hawng ratmnM Laplace transforms, different classes of customers hawng different distributions At first-come-first -served-type service centers, the service time distribution must be identical and exponentml for all classes of customers. Examples show how different classes of customers can affect models of computer systems. KEY woads AND PHRASES\" networks of queues, theory of queues, queuemg theory, multiprogram- ming, time-sharing, processor sharing, Markov processes", 
            "authors": [
                "Forest Baskett", 
                "K Mani Chandy", 
                "Richard R Muntz", 
                "Fernando G Palacios"
            ], 
            "fields": [
                "Input/output", 
                "Data processing", 
                "Scheduling", 
                "Integral transform", 
                "Queueing theory"
            ], 
            "title": "Open, Closed, and Mixed Networks of Queues with Different Classes of Customers", 
            "url": "http://cn.bing.com/academic/profile?id=b26cc4d442bbdb420ee7c244dc850770&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A resource reservation protocol (RSVP), a flexible and scalable receiver-oriented simplex protocol, is described. RSVP provides receiver-initiated reservations to accommodate heterogeneity among receivers as well as dynamic membership changes; separates the filters from the reservation, thus allowing channel changing behavior; supports a dynamic and robust multipoint-to-multipoint communication model by taking a soft-state approach in maintaining resource reservations; and decouples the reservation and routing functions. A simple network configuration with five hosts connected by seven point-to-point links and three switches is presented to illustrate how RSVP works. Related work and unresolved issues are discussed. >", 
            "authors": [
                "Lcs L Zhang", 
                "Stephen Deering", 
                "Deborah Estrin", 
                "Scott Shenker", 
                "D Zappala"
            ], 
            "fields": [
                "Point-to-point", 
                "Packet switching", 
                "Communications protocol", 
                "Models of communication"
            ], 
            "title": "RSVP: a new resource ReSerVation Protocol", 
            "url": "http://cn.bing.com/academic/profile?id=d1939e28df3db189f10519ef2212b9a8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "LT-codes are a new class of codes introduced by Luby for the purpose of scalable and fault-tolerant distribution of data over computer networks. In this paper, we introduce Raptor codes, an extension of LT-codes with linear time encoding and decoding. We will exhibit a class of universal Raptor codes: for a given integer  k  and any real e > 0, Raptor codes in this class produce a potentially infinite stream of symbols such that any subset of symbols of size  k (1 + e) is sufficient to recover the original  k  symbols with high probability. Each output symbol is generated using  O (log(1/ e)) operations, and the original symbols are recovered from the collected ones with  O ( k  log(1/e)) operations.We will also introduce novel techniques for the analysis of the error probability of the decoder for finite length Raptor codes. Moreover, we will introduce and analyze systematic versions of Raptor codes, i.e., versions in which the first output elements of the coding system coincide with the original  k  elements.", 
            "authors": [
                "Amin Shokrollahi"
            ], 
            "fields": [
                "Computer network", 
                "Luby transform code"
            ], 
            "title": "Raptor codes", 
            "url": "http://cn.bing.com/academic/profile?id=bc08c7db6794f71e65f0f717fa946771&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This article discusses the challenges in computer systems research posed by the emerging field of pervasive computing. It first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. It then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. Next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. The article closes with a discussion of the research necessary to develop these capabilities.", 
            "authors": [
                "Mahadev Satyanarayanan"
            ], 
            "fields": [
                "Wireless Application Protocol", 
                "Distributed computing", 
                "Ubiquitous computing", 
                "Transcoding", 
                "Computer network"
            ], 
            "title": "Pervasive computing: vision and challenges", 
            "url": "http://cn.bing.com/academic/profile?id=36a55f8b70beb2b3fc01da4f19dc7cf7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Authentication protocols are the basis of security in many distributed systems, and it is therefore essential to ensure that these protocols function correctly. Unfortunately, their design has been extremely error prone. Most of the protocols found in the literature contain redundancies or security flaws. A simple logic has allowed us to describe the beliefs of trustworthy parties involved in authentication protocols and the evolution of these beliefs as a consequence of communication. We have been able to explain a variety of authentication protocols formally, to discover subtleties and errors in them, and to suggest improvements. In this paper we present the logic and then give the results of our analysis of four published protocols, chosen either because of their practical importance or because they serve to illustrate our method.", 
            "authors": [
                "Michael T Burrows", 
                "Martin Abadi", 
                "Roger M Needham"
            ], 
            "fields": [
                "Distributed computing", 
                "Cryptography", 
                "Verification", 
                "Key distribution", 
                "Cryptographic protocol"
            ], 
            "title": "A logic of authentication", 
            "url": "http://cn.bing.com/academic/profile?id=d0be8b4cb90818f1841e9dd0bf096c97&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Summary: The Biological Networks Gene Ontology tool (BiNGO) is an open-source Java tool to determine which Gene Ontology (GO) terms are significantly overrepresented in a set of genes. BiNGO can be used either on a list of genes, pasted as text, or interactively on subgraphs of biological networks visualized in Cytoscape. BiNGO maps the predominant functional themes of the tested gene set on the GO hierarchy, and takes advantage of Cytoscape's versatile visualization environment to produce an intuitive and customizable visual representation of the results.#R##N##R##N#Availability: http://www.psb.ugent.be/cbd/papers/BiNGO/#R##N##R##N#Contact: martin.kuiper@psb.ugent.be", 
            "authors": [
                "Steven Maere", 
                "Karel Heymans", 
                "Martin Kuiper"
            ], 
            "fields": [
                "Gene", 
                "Ontology", 
                "Natural environment", 
                "Visualization", 
                "Genitive case"
            ], 
            "title": "BiNGO : a Cytoscape plugin to assess overrepresentation of Gene Ontology categories in Biological Networks", 
            "url": "http://cn.bing.com/academic/profile?id=5bad4d53e4fbc6d75d763f59e5337067&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The contemporary relevance of the so\u2010called Uppsala Internationalisation Model is discussed. This is a framework advanced by a number of Swedish colleagues describing the typical process of \u201cgoing international\u201d. Johanson and Vahlne respond to the criticisms of the model they proposed in the 1970s and relate it to the Eclectic Paradigm Model and the Networking literature. The concepts of the advantage package and the advantage cycle in the internationalisation context are also introduced.", 
            "authors": [
                "Jan Johanson", 
                "Janerik Vahlne"
            ], 
            "fields": [
                "Resizing", 
                "International trade", 
                "Computer network"
            ], 
            "title": "The Mechanism of Internationalisation", 
            "url": "http://cn.bing.com/academic/profile?id=c8fb375c02bb06f73d5a1b26b7447e4c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The synchronization task between loosely coupled cyclic sequential processes (as can be distinguished in, for instance, operating systems) can be viewed as keeping the relation \u201cthe system is in a legitimate state\u201d invariant. As a result, each individual process step that could possibly cause violation of that relation has to be preceded by a test deciding whether the process in question is allowed to proceed or has to be delayed. The resulting design is readily\u2014and quite systematically\u2014implemented if the different processes can be granted mutually exclusive access to a common store in which \u201cthe current system state\u201d is recorded.", 
            "authors": [
                "Edsger W Dijkstra"
            ], 
            "fields": [
                "Synchronization", 
                "Telecom infrastructure sharing", 
                "Robustness", 
                "Self-stabilization", 
                "Mutual exclusion"
            ], 
            "title": "Self-stabilizing systems in spite of distributed control", 
            "url": "http://cn.bing.com/academic/profile?id=f11f0c6f8e03ba9ef7a0c4469e8b9fa0&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Radio communication is considered as a method for providing remote terminal access to computers. Digital byte streams from each terminal are partitioned into packets (blocks) and transmitted in a burst mode over a shared radio channel. When many terminals operate in this fashion, transmissions may conflict with and destroy each other. A means for controlling this is for the terminal to sense the presence of other transmissions; this leads to a new method for multiplexing in a packet radio environment: carrier sense multiple access (CSMA). Two protocols are described for CSMA and their throughput-delay characteristics are given. These results show the large advantage CSMA provides as compared to the random ALOHA access modes.", 
            "authors": [
                "Leonard Kleinrock", 
                "Fouad A Tobagi"
            ], 
            "fields": [
                "Packet radio", 
                "Packet switching", 
                "Bandwidth", 
                "Radio Broadcasting", 
                "Telephony"
            ], 
            "title": "Packet Switching in Radio Channels: Part I--Carrier Sense Multiple-Access Modes and Their Throughput-Delay Characteristics", 
            "url": "http://cn.bing.com/academic/profile?id=c775120edfac71418827aea22404c5f5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds, improving over today's wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics.Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON's routing mechanism was able to detect, recover, and route around  all  of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5% of the transfers doubled their TCP throughput and 5% of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems.", 
            "authors": [
                "David G Andersen", 
                "Hari Balakrishnan", 
                "Frans Kaashoek", 
                "Robert Morris"
            ], 
            "fields": [
                "Sampling", 
                "Routing protocol", 
                "The Internet", 
                "Overlay", 
                "Metric"
            ], 
            "title": "Resilient overlay networks", 
            "url": "http://cn.bing.com/academic/profile?id=c125b0bcba5696523fbc170974c4bb14&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Random networks with complex topology are common in Nature, describing systems as diverse as the world wide web or social and business networks. Recently, it has been demonstrated that most large networks for which topological information is available display scale-free features. Here we study the scaling properties of the recently introduced scale-free model, that can account for the observed power-law distribution of the connectivities. We develop a mean-field method to predict the growth dynamics of the individual vertices, and use this to calculate analytically the connectivity distribution and the scaling exponents. The mean-field method can be used to address the properties of two variants of the scale-free model, that do not display power-law scaling.", 
            "authors": [
                "Albertlaszlo Barabasi", 
                "Reka Albert", 
                "Hawoong Jeong"
            ], 
            "fields": [
                "Pareto distribution", 
                "Critical phenomena", 
                "Scale-free network", 
                "Mean field theory", 
                "World Wide Web"
            ], 
            "title": "Mean-field theory for scale-free random networks", 
            "url": "http://cn.bing.com/academic/profile?id=360bad8d6ec34d0f0ed0ef27b15bf2ab&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A general formulation of the feeder reconfiguration problem for loss reduction and load balancing is given, and a novel solution method is presented. The solution uses a search over different radial configurations created by considering switchings of the branch exchange type. To guide the search, two different power flow approximation methods with varying degrees of accuracy have been developed and tested. The methods are used to calculate the new power flow in the system after a branch exchange and they make use of the power flow equations developed for radial distribution systems. Both accuracy analysis and the test results show that estimation methods can be used in searches to reconfigure a given system even if the system is not well compensated and reconfiguring involves load transfer between different substations. For load balancing, a load balance index is defined and it is shown that the search and power flow estimation methods developed for power loss reduction can also be used for load balancing since the two problems are similar. >", 
            "authors": [
                "Miroslaw Baran", 
                "Felix F Wu"
            ], 
            "fields": [
                "Intelligent Network", 
                "Computer network", 
                "Network switch", 
                "Mathematical optimization", 
                "Load balancing"
            ], 
            "title": "Network reconfiguration in distribution systems for loss reduction and load balancing", 
            "url": "http://cn.bing.com/academic/profile?id=2d9c59ad3c7cd68476cc4829d2a49e11&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone.", 
            "authors": [
                "Xin Yao"
            ], 
            "fields": [
                "Evolutionary computation", 
                "Intelligent decision support system", 
                "Intelligent Network", 
                "Technology forecasting", 
                "Competitive intelligence"
            ], 
            "title": "Evolving artificial neural networks", 
            "url": "http://cn.bing.com/academic/profile?id=eb3921356579b685af4ee1c74cb87a3d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A deadlock-free routing algorithm can be generated for arbitrary interconnection networks using the concept of virtual channels. A necessary and sufficient condition for deadlock-free routing is the absence of cycles in a channel dependency graph. Given an arbitrary network and a routing function, the cycles of the channel dependency graph can be removed by splitting physical channels into groups of virtual channels. This method is used to develop deadlock-free routing algorithms for k-ary n-cubes, for cube-connected cycles, and for shuffle-exchange networks.", 
            "authors": [
                "Dally", 
                "Seitz"
            ], 
            "fields": [
                "Intelligent Network", 
                "Computer graphics (images)", 
                "Concurrent computing", 
                "Computer graphics", 
                "Telecommunications network"
            ], 
            "title": "Deadlock-Free Message Routing in Multiprocessor Interconnection Networks", 
            "url": "http://cn.bing.com/academic/profile?id=24bf7ddb78d81eee593452fd37340a74&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We present an overview of ad hoc routing protocols that make forwarding decisions based on the geographical position of a packet's destination. Other than the destination's position, each node need know only its own position and the position of its one-hop neighbors in order to forward packets. Since it is not necessary to maintain explicit routes, position-based routing does scale well even if the network is highly dynamic. This is a major advantage in a mobile ad hoc network where the topology may change frequently. The main prerequisite for position-based routing is that a sender can obtain the current position of the destination. Therefore, previously proposed location services are discussed in addition to position-based packet forwarding strategies. We provide a qualitative comparison of the approaches in both areas and investigate opportunities for future research.", 
            "authors": [
                "Martin Mauve", 
                "Jorg Widmer", 
                "Hannes Hartenstein"
            ], 
            "fields": [
                "Packet forwarding", 
                "Network topology", 
                "Mobile ad hoc network", 
                "Communications protocol"
            ], 
            "title": "A survey on position-based routing in mobile ad hoc networks", 
            "url": "http://cn.bing.com/academic/profile?id=5a992106076b0263e755deaadba60c44&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We describe how several optimization problems can be rapidly solved by highly interconnected networks of simple analog processors. Analog-to-digital (A/D) conversion was considered as a simple optimization problem, and an A/D converter of novel architecture was designed. A/D conversion is a simple example of a more general class of signal-decision problems which we show could also be solved by appropriately constructed networks. Circuits to solve these problems were designed using general principles which result from an understanding of the basic collective computational properties of a specific class of analog-processor networks. We also show that a network which solves linear programming problems can be understood from the same concepts.", 
            "authors": [
                "David W Tank", 
                "J J Hopfield"
            ], 
            "fields": [
                "Optimization problem", 
                "Computer network", 
                "Analog computer", 
                "Artificial neural network", 
                "Decision problem"
            ], 
            "title": "Simple 'neural' optimization networks: An A/D converter, signal decision circuit, and a linear programming circuit", 
            "url": "http://cn.bing.com/academic/profile?id=d981d5d110f4ca2a4c9393296b3eea63&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Knowledge, once generated, spills only imperfectly among firms and nations. We posit that since institutions and labor networks vary by region, there should be regional variations in the localization of spillovers. We investigate the relationship between the mobility of major patent holders and the localization of technological knowledge through the analysis of patent citations of important semiconductor innovations. We find that knowledge localization is specific to only certain regions (particularly Silicon Valley) and that the degree of localization varies across regions. By analyzing data on the interfirm mobility of patent holders, we empirically show that the interfirm mobility of engineers influences the local transfer of knowledge. The flow of knowledge is embedded in regional labor networks.", 
            "authors": [
                "Paul Almeida", 
                "Bruce Kogut"
            ], 
            "fields": [
                "Knowledge", 
                "Social network", 
                "Regression analysis", 
                "Mobile computing", 
                "Computer network"
            ], 
            "title": "Localization of Knowledge and the Mobility of Engineers in Regional Networks", 
            "url": "http://cn.bing.com/academic/profile?id=6b0dc1dee35ea270f55abec53f754b0d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative explanations for the findings.", 
            "authors": [
                "James E Rauch"
            ], 
            "fields": [
                "Product differentiation", 
                "Developing country", 
                "International trade", 
                "Computer network"
            ], 
            "title": "Networks versus Markets in International Trade", 
            "url": "http://cn.bing.com/academic/profile?id=356ffd669e52272b8594cedb9ac8782f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A major problem worldwide is the potential loss of fisheries, forests, and water resources. Understanding of the processes that lead to improvements in or deterioration of natural resources is limited, because scientific disciplines use different concepts and languages to describe and explain complex social-ecological systems (SESs). Without a common framework to organize findings, isolated knowledge does not cumulate. Until recently, accepted theory has assumed that resource users will never self-organize to maintain their resources and that governments must impose solutions. Research in multiple disciplines, however, has found that some government policies accelerate resource destruction, whereas some resource users have invested their time and energy to achieve sustainability. A general framework is used to identify 10 subsystem variables that affect the likelihood of self-organization in efforts to achieve a sustainable SES.", 
            "authors": [
                "Elinor Ostrom"
            ], 
            "fields": [
                "Food web", 
                "Cumulant", 
                "Ecology", 
                "Resource management", 
                "Complex systems"
            ], 
            "title": "A General Framework for Analyzing Sustainability of Social-Ecological Systems", 
            "url": "http://cn.bing.com/academic/profile?id=0f426b66b76a09c89ad727216251d7e6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N#Networks of dozens to hundreds of permanently operating precision Global Positioning System (GPS) receivers are emerging at spatial scales that range from 100 to 103 km. To keep the computational burden associated with the analysis of such data economically feasible, one approach is to first determine precise GPS satellite positions and clock corrections from a globally distributed network of GPS receivers. Then, data from the local network are analyzed by estimating receiver-specific parameters with receiver-specific data; satellite parameters are held fixed at their values determined in the global solution. This \u201cprecise point positioning\u201d allows analysis of data from hundreds to thousands of sites every day with 40-Mflop computers, with results comparable in quality to the simultaneous analysis of all data. The reference frames for the global and network solutions can be free of distortion imposed by erroneous fiducial constraints on any sites.", 
            "authors": [
                "J F Zumberge", 
                "Michael B Heflin", 
                "D C Jefferson", 
                "Michael M Watkins", 
                "Frank H Webb"
            ], 
            "fields": [
                "Spatial ecology", 
                "Computer network", 
                "Distortion", 
                "Precise Point Positioning", 
                "Estimation"
            ], 
            "title": "Precise point positioning for the efficient and robust analysis of GPS data from large networks", 
            "url": "http://cn.bing.com/academic/profile?id=18f545250c61c6597813eb6a50e30910&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliament's protocol provides a new way of implementing the state machine approach to the design of distributed systems.", 
            "authors": [
                "Leslie Lamport"
            ], 
            "fields": [
                "Finite-state machine", 
                "Database", 
                "Fault tolerance", 
                "Implementation", 
                "Computer network"
            ], 
            "title": "The part-time parliament", 
            "url": "http://cn.bing.com/academic/profile?id=ea942a04ff200b7c26e3aa1847755900&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet's shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience with an operating Ethernet of 100 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error controlled communication are included for completeness.", 
            "authors": [
                "Robert M Metcalfe", 
                "David R Boggs"
            ], 
            "fields": [
                "Packet switching", 
                "Communications system", 
                "Broadcast communication network", 
                "Distributed computing", 
                "Computer network"
            ], 
            "title": "Ethernet: distributed packet switching for local computer networks", 
            "url": "http://cn.bing.com/academic/profile?id=695673c9f3c2bba7bdcc4608a3f32fd6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the  preflow  concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense graphs, achieving an  O ( n  3 ) time bound on an  n -vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in  O ( nm  log( n  2 / m )) time on an  n -vertex,  m -edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efficient distributed and parallel implementations. A parallel implementation running in  O ( n  2 log  n ) time using  n  processors and  O ( m ) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses  n  processors but requires  O ( n  2 ) space.", 
            "authors": [
                "Andrew V Goldberg", 
                "Robert E Tarjan"
            ], 
            "fields": [
                "Shortest path problem", 
                "Maximum flow problem", 
                "Directed graph", 
                "Computer network", 
                "Maxima and minima"
            ], 
            "title": "A new approach to the maximum-flow problem", 
            "url": "http://cn.bing.com/academic/profile?id=2873099ca422583f3a8c26bf75e4235d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An Information Dispersal Algorithm (IDA) is developed that breaks a file  F  of length  L  = u  F u into  n  pieces  F i  , l \u2264  i  \u2264  n , each of length u F i  u =  L / m , so that every  m  pieces suffice for reconstructing  F . Dispersal and reconstruction are computationally efficient. The sum of the lengths u F  i u is ( n / m ) \u00b7  L . Since  n / m  can be chosen to be close to l, the IDA is space efficient.  IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the  n -cube is achieved, using just constant size buffers.", 
            "authors": [
                "Michael O Rabin"
            ], 
            "fields": [
                "Information theory", 
                "Distributed computing", 
                "Fault tolerance", 
                "Coding", 
                "Load balancing"
            ], 
            "title": "Efficient dispersal of information for security, load balancing, and fault tolerance", 
            "url": "http://cn.bing.com/academic/profile?id=4a8f75ca7ee6db77e904869da68b6ffd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Several new architectures have been developed for supporting multimedia applications such as digital video and audio. However, quality-of-service (QoS) routing is an important element that is still missing from these architectures. In this paper, we consider a number of issues in QoS routing. We first examine the basic problem of QoS routing, namely, finding a path that satisfies multiple constraints, and its implications on routing metric selection, and then present three path computation algorithms for source routing and for hop-by-hop routing.", 
            "authors": [
                "Zheng Wang", 
                "Jon Crowcroft"
            ], 
            "fields": [
                "Computer architecture", 
                "Digital audio", 
                "Bandwidth", 
                "Satisfiability", 
                "Scalability"
            ], 
            "title": "Quality-of-service routing for supporting multimedia applications", 
            "url": "http://cn.bing.com/academic/profile?id=549852901d36e9ab5c7d2af4b7f51381&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper describes a self-organizing, multihop, mobile radio network which relies on a code-division access scheme for multimedia support. In the proposed network architecture, nodes are organized into nonoverlapping clusters. The clusters are independently controlled, and are dynamically reconfigured as the nodes move. This network architecture has three main advantages. First, it provides spatial reuse of the bandwidth due to node clustering. Second, bandwidth can be shared or reserved in a controlled fashion in each cluster. Finally, the cluster algorithm is robust in the face of topological changes caused by node motion, node failure, and node insertion/removal. Simulation shows that this architecture provides an efficient, stable infrastructure for the integration of different types of traffic in a dynamic radio network.", 
            "authors": [
                "Changshou Lin", 
                "Mario Gerla"
            ], 
            "fields": [
                "Network architecture", 
                "Computer performance", 
                "Spread spectrum", 
                "Self-organization", 
                "Bandwidth"
            ], 
            "title": "Adaptive clustering for mobile wireless networks", 
            "url": "http://cn.bing.com/academic/profile?id=5bd348f24c779be0c29694b4ad846231&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called \"hidden causes.\" It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.", 
            "authors": [
                "Judea Pearl"
            ], 
            "fields": [
                "Knowledge representation and reasoning", 
                "Probability theory", 
                "Tree structure", 
                "Computer network", 
                "Bayesian network"
            ], 
            "title": "Fusion, propagation, and structuring in belief networks", 
            "url": "http://cn.bing.com/academic/profile?id=a0734fd694e0cc7576616b05c22fcf43&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "From the Publisher:#R##N#Probabilistic expert systems are graphical networks that support the modelling of uncertainty and decisions in large complex domains, while retaining ease of calculation. Building on original research by the authors over a number of years, this book gives a thorough and rigorous mathematical treatment of the underlying ideas, structures, and algorithms, emphasizing those cases in which exact answers are obtainable. The book will be of interest to researchers and graduate students in artificial intelligence who desire an understanding of the mathematical and statistical basis of probabilistic expert systems, and to students and research workers in statistics wanting an introduction to this fascinating and rapidly developing field. The careful attention to detail will also make this work an important reference source for all those involved in the theory and applications of probabilistic expert systems.", 
            "authors": [
                "Robert G Cowell", 
                "Steffen L Lauritzen", 
                "A Philip David", 
                "D J Spiegelhalter", 
                "V Nair", 
                "J Lawless", 
                "Michael I Jordan"
            ], 
            "fields": [
                "Computer network", 
                "Expert system", 
                "System"
            ], 
            "title": "Probabilistic Networks and Expert Systems", 
            "url": "http://cn.bing.com/academic/profile?id=173ab6e3f18051fad7ebad51c16bc020&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Congestion avoidance mechanisms allow a network to operate in the optimal region of low delay and high throughput, thereby, preventing the network from becoming congested. This is different from the traditional congestion control mechanisms that allow the network to recover from the congested state of high delay and low throughput. Both con- gestion avoidance and congestion control mechanisms are basi- cally resource management problems. They can be formulated as system control problems in which the system senses its state and feeds this back to its users who adjust their controls. The key component of any congestion avoidance scheme is the algorithm (or control function) used by the users to in- crease or decrease their load (window or rate). We abstractly characterize a wide class of such increase/decreas e algorithms and compare them using several different performance metrics. They key metrics are efficiency, fairness, convergence time, and size of oscillations. It is shown that a simple additive increase and multiplicative decrease algorithm satisfies the sufficient conditions for con- vergence to an efficient and fair state regardless of the starting state of the network. This is the algorithm finally chosen for implementation in the congestion avoidance scheme recom- mended for Digital Networking Architecture and OSI Trans- port Class 4 Networks.", 
            "authors": [
                "Dahming Chiu", 
                "Raj Jain"
            ], 
            "fields": [
                "Network performance", 
                "Flow control", 
                "Computer network", 
                "Oscillation", 
                "Resource management"
            ], 
            "title": "Analysis of the increase and decrease algorithms for congestion avoidance in computer networks", 
            "url": "http://cn.bing.com/academic/profile?id=dce8512df3a106a6ebb3386d26efbed6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity; large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior. We present an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.", 
            "authors": [
                "Karl Czajkowski", 
                "Steven M Fitzgerald", 
                "Ian Foster", 
                "Carl Kesselman"
            ], 
            "fields": [
                "Computer network", 
                "Troubleshooting", 
                "Communications protocol", 
                "Resource management", 
                "Distributed computing"
            ], 
            "title": "Grid information services for distributed resource sharing", 
            "url": "http://cn.bing.com/academic/profile?id=8c09554ccd4c1c7dc5f9ff4c7ac1a6b4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An important issue in multiobjective optimization is the quantitative comparison of the performance of different algorithms. In the case of multiobjective evolutionary algorithms, the outcome is usually an approximation of the Pareto-optimal set, which is denoted as an approximation set, and therefore the question arises of how to evaluate the quality of approximation sets. Most popular are methods that assign each approximation set a vector of real numbers that reflect different aspects of the quality. Sometimes, pairs of approximation sets are also considered. In this study, we provide a rigorous analysis of the limitations underlying this type of quality assessment. To this end, a mathematical framework is developed which allows one to classify and discuss existing techniques.", 
            "authors": [
                "Eckart Zitzler", 
                "Lothar Thiele", 
                "Marco Laumanns", 
                "Carlos M Fonseca", 
                "V G Da Fonseca"
            ], 
            "fields": [
                "Multi-objective optimization", 
                "Evolutionary algorithm", 
                "Computer network", 
                "Quality control", 
                "Helium"
            ], 
            "title": "Performance assessment of multiobjective optimizers: an analysis and review", 
            "url": "http://cn.bing.com/academic/profile?id=c98a8feee3b7b76b5fd32a7b1bf2776a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The article describes an active map service (AMS) that supports context-aware computing by providing clients with information about located-objects and how those objects change over time. The authors focus on the communication issues of disseminating information from an active map server to its clients, and in particular, address how to deal with various overload situations that can occur. Simple unicast callbacks to interested clients work well enough if only a few located-objects are moving at any given time and only a few clients wish to know about any given move. However, if many people are moving about in the same region and many clients are interested in their motion, then the AMS may experience overload due to the quadratic nature of the communications involved. This overload affects both the server as well as any slow communications links being used. Mobile distributed computing enables users to interact with many different mobile and stationary computers over the course of the day. Navigating a mobile environment can be aided by active maps that describe the location and characteristics of objects within some region as they change over time. >", 
            "authors": [
                "Bill N Schilit", 
                "Marvin M Theimer"
            ], 
            "fields": [
                "Computer network", 
                "Information system", 
                "Distributed computing", 
                "Tracking"
            ], 
            "title": "Disseminating active map information to mobile hosts", 
            "url": "http://cn.bing.com/academic/profile?id=8f26d2a9f51c5cbecb5f8e30844d8300&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The authors present a time-delay neural network (TDNN) approach to phoneme recognition which is characterized by two important properties: (1) using a three-layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces, which the TDNN learns automatically using error backpropagation; and (2) the time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independently of position in time and therefore not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes B, D, and G in varying phonetic contexts was chosen. For comparison, several discrete hidden Markov models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5% correct while the rate obtained by the best of the HMMs was only 93.7%. >", 
            "authors": [
                "Alex Waibel", 
                "Toshiyuki Hanazawa", 
                "G E Hinton", 
                "Kiyohiro Shikano", 
                "Kevin J Lang"
            ], 
            "fields": [
                "Time delay neural network", 
                "Hidden Markov model", 
                "Backpropagation", 
                "Psychology", 
                "Computer network"
            ], 
            "title": "Phoneme recognition using time-delay neural networks", 
            "url": "http://cn.bing.com/academic/profile?id=9406da3da6afcbb1cd0f15a38652321b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: Molecular networks guide the biochemistry of a living cell on multiple levels: its metabolic and signalling pathways are shaped by the network of interacting proteins, whose production, in turn, is controlled by the genetic regulatory network. To address topological properties of these two networks we quantify correlations between connectivities of interacting nodes and compare them to a null model of a network, in which al links were randomly rewired. We find that for both interaction and regulatory networks, links between highly connected proteins are systematically suppressed, while those between a highly-connected and low-connected pairs of proteins are favored. This effect decreases the likelihood of cross talk between different functional modules of the cell, and increases the overall robustness of a network by localizing effects of deleterious perturbations.", 
            "authors": [
                "Sergei Maslov", 
                "Kim Sneppen"
            ], 
            "fields": [
                "Genitive case", 
                "Stability", 
                "Biochemistry", 
                "Regulation", 
                "Gene"
            ], 
            "title": "Specificity and stability in topology of protein networks", 
            "url": "http://cn.bing.com/academic/profile?id=3858366a32b75d586a8b6b0f88b9b370&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The Internet is rapidly growing in number of users, traffic levels, and topological complexity. At the same time it is increasingly driven by economic competition. These developments render the characterization of network usage and workloads more difficult, and yet more critical. Few recent studies have been published reporting Internet backbone traffic usage and characteristics. At MCI, we have implemented a high-performance, low-cost monitoring system that can capture traffic and perform analyses. We have deployed this monitoring tool on OC-3 trunks within the Internet MCI's backbone and also within the NSF-sponsored vBNS. This article presents observations on the patterns and characteristics of wide-area Internet traffic, as recorded by MCI's OC-3 traffic monitors. We report on measurements from two OC-3 trunks in MCI's commercial Internet backbone over two time ranges (24-hour and 7-day) in the presence of up to 240,000 flows. We reveal the characteristics of the traffic in terms of packet sizes, flow duration, volume, and percentage composition by protocol and application, as well as patterns seen over the two time scales.", 
            "authors": [
                "Kevin Thompson", 
                "Gregory John Miller", 
                "Rick Wilder"
            ], 
            "fields": [
                "The Internet", 
                "Internet traffic"
            ], 
            "title": "Wide-area Internet traffic patterns and characteristics", 
            "url": "http://cn.bing.com/academic/profile?id=b1d1d42b5351fb6f304fdbf3e8c20b50&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Peer-to-peer (P2P) online communities are commonly perceived as an environment offering both opportunities and threats. One way to minimize threats in such communities is to use community-based reputations to help estimate the trustworthiness of peers. We present PeerTrust - a reputation-based trust supporting framework, which includes a coherent adaptive trust model for quantifying and comparing the trustworthiness of peers based on a transaction-based feedback system, and a decentralized implementation of such a model over a structured P2P network. PeerTrust model has two main features. First, we introduce three basic trust parameters and two adaptive factors in computing trustworthiness of peers, namely, feedback a peer receives from other peers, the total number of transactions a peer performs, the credibility of the feedback sources, transaction context factor, and the community context factor. Second, we define a general trust metric to combine these parameters. Other contributions of the paper include strategies used for implementing the trust model in a decentralized P2P environment, evaluation mechanisms to validate the effectiveness and cost of PeerTrust model, and a set of experiments that show the feasibility and benefit of our approach.", 
            "authors": [
                "Li Xiong", 
                "Ling Liu"
            ], 
            "fields": [
                "Data security", 
                "Information security", 
                "Data management", 
                "Peer-to-peer", 
                "Trustworthy computing"
            ], 
            "title": "PeerTrust: supporting reputation-based trust for peer-to-peer electronic communities", 
            "url": "http://cn.bing.com/academic/profile?id=a170e2c349443b576f5539ae9eac69c2&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#What explains differences in firms\u2019 abilities to acquire competitive capabilities? In this paper we propose that a firm\u2019s embeddedness in a network of ties is an important source of variation in the acquisition of competitive capabilities. We argue that firms in geographical clusters that maintain networks rich in bridging ties and sustain ties to regional institutions are well-positioned to access new information, ideas, and opportunities. Hypotheses based on these ideas were tested on a stratified random sample of 227 job shop manufacturers located in the Midwest United States. Data were gathered using a mailed questionnaire. Results from structural equation modeling broadly support the embeddedness hypotheses and suggest a number of insights about the link between firms\u2019 networks and the acquisition of competitive capabilities. Copyright \u00a9 1999 John Wiley & Sons, Ltd.", 
            "authors": [
                "Akbar Zaheer", 
                "Bill Mcevily"
            ], 
            "fields": [
                "Geographical cluster", 
                "Resource", 
                "Computer network", 
                "Basic Rate Interface", 
                "Sampling"
            ], 
            "title": "Bridging ties: a source of firm heterogeneity in competitive capabilities", 
            "url": "http://cn.bing.com/academic/profile?id=0ec3dffbf0e4d05edf7b8a77c74904d1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The network time protocol (NTP), which is designed to distribute time information in a large, diverse system, is described. It uses a symmetric architecture in which a distributed subnet of time servers operating in a self-organizing, hierarchical configuration synchronizes local clocks within the subnet and to national time standards via wire, radio, or calibrated atomic clock. The servers can also redistribute time information within a network via local routing algorithms and time daemons. The NTP synchronization system, which has been in regular operation in the Internet for the last several years, is described, along with performance data which show that timekeeping accuracy throughout most portions of the Internet can be ordinarily maintained to within a few milliseconds, even in cases of failure or disruption of clocks, time servers, or networks. >", 
            "authors": [
                "David Mills"
            ], 
            "fields": [
                "Algorithm", 
                "Self-organization", 
                "The Internet", 
                "Communications protocol", 
                "Atomic clock"
            ], 
            "title": "Internet time synchronization: the network time protocol", 
            "url": "http://cn.bing.com/academic/profile?id=1892308368c3ca9eecbf3a15f05ea3a7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Objective.\u2003To describe the frequency of selected antimicrobial resistance patterns among pathogens causing device\u2010associated and procedure\u2010associated healthcare\u2010associated infections (HAIs) reported by hospitals in the National Healthcare Safety Network (NHSN). Methods.\u2003Data are included on HAIs (ie, central line\u2013associated bloodstream infections, catheter\u2010associated urinary tract infections, ventilator\u2010associated pneumonia, and surgical site infections) reported to the Patient Safety Component of the NHSN between January 2006 and October 2007. The results of antimicrobial susceptibility testing of up to 3 pathogenic isolates per HAI by a hospital were evaluated to define antimicrobial\u2010resistance in the pathogenic isolates. The pooled mean proportions of pathogenic isolates interpreted as resistant to selected antimicrobial agents were calculated by type of HAI and overall. The incidence rates of specific device\u2010associated infections were calculated for selected antimicrobial\u2010resistant pathogens according...", 
            "authors": [
                "Alicia I Hidron", 
                "Jonathan R Edwards", 
                "Jean B Patel", 
                "Mph Teresa Horan", 
                "Dawn M Sievert", 
                "Daniel A Pollock", 
                "Scott K Fridkin"
            ], 
            "fields": [
                "Scientific control", 
                "Escherichia coli", 
                "Antibiotic resistance", 
                "Multiple drug resistance", 
                "Infection control"
            ], 
            "title": "Antimicrobial-Resistant Pathogens Associated With Healthcare-Associated Infections: Annual Summary of Data Reported to the National Healthcare Safety Network at the Centers for Disease Control and Prevention, 2006\u20132007", 
            "url": "http://cn.bing.com/academic/profile?id=f18190ccdbc71822026b8d5953ae9d93&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Gephi is an open source software for graph and network analysis. It uses a 3D render engine to display large networks in real-time and to speed up the exploration. A flexible and multi-task architecture brings new possibilities to work with complex data sets and produce valuable visual results.\u00a0 We present several key features of Gephi in the context of interactive exploration and interpretation of networks. It provides easy and broad access to network data and allows for spatializing, filtering, navigating, manipulating and clustering. Finally, by presenting dynamic features of Gephi, we highlight key aspects of dynamic network visualization.", 
            "authors": [
                "Mathieu Bastian", 
                "Sebastien Heymann", 
                "Mathieu Jacomy"
            ], 
            "fields": [
                "Force", 
                "Java", 
                "Computer network", 
                "Network analysis", 
                "User-centered design"
            ], 
            "title": "Gephi: An Open Source Software for Exploring and Manipulating Networks", 
            "url": "http://cn.bing.com/academic/profile?id=50bad735aa7f98773cf7bf77ae5cac45&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We describe a new active queue management scheme, random exponential marking (REM), that aims to achieve both high utilization and negligible loss and delay in a simple and scalable manner. The key idea is to decouple the congestion measure from the performance measure such as loss, queue length, or delay. While the congestion measure indicates excess demand for bandwidth and must track the number of users, the performance measure should be stabilized around their targets independent of the number of users. We explain the design rationale behind REM and present simulation results of its performance in wireline and wireless networks.", 
            "authors": [
                "Sanjeewa Athuraliya", 
                "Steven H Low", 
                "V H Li", 
                "Qinghe Yin"
            ], 
            "fields": [
                "Technology management", 
                "Algorithm design", 
                "Queueing theory", 
                "Active queue management", 
                "Length measurement"
            ], 
            "title": "REM: active queue management", 
            "url": "http://cn.bing.com/academic/profile?id=699336355eb597f2c7c236b0bd6b6bed&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N#The use of transition metal complexes of bridging bidentate ligands to construct predictable, multi-dimensional infinite networks is an area of chemistry which has received ever-increasing attention over recent years. This article will review the advances that have been made in this field of research and will illustrate how ligand design and the properties of the transition metal and counter-anion can be used to control network geometry and thus crystal structure. The range of network topologies and structural motifs that have been constructed thus far will be outlined with particular emphasis upon how specific arrays can be prepared via rational design of molecular building-blocks. The unusual phenomenon of interpenetration, or polycatenation, will be discussed and methods to achieve control over this effect will be highlighted.", 
            "authors": [
                "Alexander J Blake", 
                "Neil R Champness", 
                "Peter Hubberstey", 
                "Wansheung Li", 
                "Matthew A Withersby", 
                "Martin Schroder"
            ], 
            "fields": [
                "Crystal structure", 
                "Self-assembly", 
                "Control network", 
                "Network topology", 
                "Chain"
            ], 
            "title": "Inorganic crystal engineering using self-assembly of tailored building-blocks", 
            "url": "http://cn.bing.com/academic/profile?id=9731a60558d8504c2aa5ad3d4d3c301b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In a new algorithm for maintaining replicated data, every copy of a replicated file is assigned some number of votes. Every transaction collects a read quorum of  r votes to read a file, and a write quorum of  w votes to write a file, such that  r + w  is greater than the total number of votes assigned to the file. This ensures that there is a non-null intersection between every read quorum and every write quorum. Version numbers make it possible to determine which copies are current. The reliability and performance characteristics of a replicated file can be controlled by appropriately choosing  r, w,  and the file's voting configuration. The algorithm guarantees serial consistency, admits temporary copies in a natural way by the introduction of copies with no votes, and has been implemented in the context of an application system called Violet.", 
            "authors": [
                "David K Gifford"
            ], 
            "fields": [
                "Weighted voting", 
                "Computer network", 
                "Database transaction"
            ], 
            "title": "Weighted voting for replicated data", 
            "url": "http://cn.bing.com/academic/profile?id=e475a5f30c3ab8765a5bab9761432d31&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The components of a loosely coupled system are typically designed to operate by generating and responding to asynchronous events. An  event notification service  is an application-independent infrastructure that supports the construction of event-based systems, whereby generators of events publish event notifications to the infrastructure and consumers of events subscribe with the infrastructure to receive relevant notifications. The two primary services that should be provided to components by the infrastructure are notification selection (i. e., determining which notifications match which subscriptions) and notification delivery (i.e., routing matching notifications from publishers to subscribers). Numerous event notification services have been developed for local-area networks, generally based on a centralized server to select and deliver event  notifications. Therefore, they suffer from an inherent inability to scale to wide-area networks, such as the Internet, where the number and physical distribution of the service's clients can quickly overwhelm a centralized solution. The critical challenge in the setting of a wide-area network is to maximize the expressiveness in the selection mechanism without sacrificing scalability in the delivery mechanism. This paper presents SIENA, an event notification service that we have designed and implemented to exhibit both expressiveness and scalability. We describe the service's interface to applications, the algorithms used by networks of servers to select and deliver event notifications, and the strategies used to optimize performance. We also present results of simulation studies that  examine the scalability and performance of the service.", 
            "authors": [
                "Antonio Carzaniga", 
                "David S Rosenblum", 
                "Alexander L Wolf"
            ], 
            "fields": [
                "Client\u2013server model", 
                "Computer network", 
                "Local area network", 
                "The Internet", 
                "Simulation"
            ], 
            "title": "Design and evaluation of a wide-area event notification service", 
            "url": "http://cn.bing.com/academic/profile?id=0b4d20bbd190bc88b5fc0a72825ea751&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The use of infrared radiation as a medium for high-speed short-range wireless digital communication is discussed. Available infrared links and local-area networks are described. Advantages and drawbacks of the infrared medium are compared to those of radio and microwave media. The physical characteristics of infrared channels using intensity modulation with direct detection (IM/DD) are presented including path losses and multipath responses. Natural and artificial ambient infrared noise sources are characterized. Strategies for designs of transmitter and receivers that maximize link signal-to-noise ratio (SNR) are described. Several modification formats are discussed in detail, including on-off keying (OOK) pulse-position modulation (PPM), and subcarrier modulation. The performance of these techniques in the presence of multipath distortion is quantified. Techniques for multiplexing the transmissions of different users are reviewed. The performance of an experimental 50-Mb/s on-off-keyed diffuse infrared link is described.", 
            "authors": [
                "Joseph M Kahn", 
                "J R Barry"
            ], 
            "fields": [
                "Modulation", 
                "Local area network", 
                "Data transmission", 
                "Intensity modulation", 
                "Pulse-width modulation"
            ], 
            "title": "Wireless infrared communications", 
            "url": "http://cn.bing.com/academic/profile?id=334647f9f66ffd712c498954e7108707&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A number of library based parallel and sequential network simulators have been designed. The paper describes a library, called GloMoSim (Global Mobile system Simulator), for parallel simulation of wireless networks. GloMoSim has been designed to be extensible and composable: the communication protocol stack for wireless networks is divided into a set of layers, each with its own API. Models of protocols at one layer interact with those at a lower (or higher) layer only via these APIs. The modular implementation enables consistent comparison of multiple protocols at a given layer. The parallel implementation of GloMoSim can be executed using a variety of conservative synchronization protocols, which include the null message and conditional event algorithms. The paper describes the GloMoSim library, addresses a number of issues relevant to its parallelization, and presents a set of experimental results on the IBM 9076 SP, a distributed memory multicomputer. These experiments use models constructed from the library modules.", 
            "authors": [
                "X Zeng", 
                "Rajive Bagrodia", 
                "Mario Gerla"
            ], 
            "fields": [
                "Parallel computing", 
                "Distributed memory", 
                "Cellular network", 
                "Application programming interface", 
                "Computer network"
            ], 
            "title": "GloMoSim: a library for parallel simulation of large-scale wireless networks", 
            "url": "http://cn.bing.com/academic/profile?id=d6ae808f951822a056d00f8750fe08d1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#This study examines the relationship between interfirm asset specificity and performance in the auto industry. More specifically, I examine the extent to which differences in supplier\u2013automaker asset specialization may explain performance differences between Japanese automakers (Nissan and Toyota) and U.S. automakers (Chrysler, Ford, General Motors). The findings indicate a positive relationship between supplier\u2013automaker specialization and performance. In particular, the data suggest a positive relationship between interfirm human asset cospecialization and both quality and new model cycle time. Moreover, site specialization is found to be positively associated with lower inventory costs. The findings suggest that in the auto industry a tightly integrated production network characterized by proximity and a high level of human cospecialization will outperform a loosely integrated production network characterized by low levels of interfirm specialization.", 
            "authors": [
                "Jeffrey H Dyer"
            ], 
            "fields": [
                "Automotive industry", 
                "Competitive advantage", 
                "Business", 
                "Data analysis", 
                "Performance"
            ], 
            "title": "SPECIALIZED SUPPLIER NETWORKS AS A SOURCE OF COMPETITIVE ADVANTAGE -- EVIDENCE FROM THE AUTO INDUSTRY", 
            "url": "http://cn.bing.com/academic/profile?id=b4c1dcd06f0c0eaacc783b6b7b4e7a7c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "LT-codes are a new class of codes introduced by Luby for the purpose of scalable and fault-tolerant distribution of data over computer networks. In this paper, we introduce Raptor codes, an extension of LT-codes with linear time encoding and decoding. We will exhibit a class of universal Raptor codes: for a given integer k and any real epsiv>0, Raptor codes in this class produce a potentially infinite stream of symbols such that any subset of symbols of size k(1+epsiv) is sufficient to recover the original k symbols with high probability. Each output symbol is generated using O(log(1/epsiv)) operations, and the original symbols are recovered from the collected ones with O(klog(1/epsiv)) operations. We will also introduce novel techniques for the analysis of the error probability of the decoder for finite length Raptor codes. Moreover, we will introduce and analyze systematic versions of Raptor codes, i.e., versions in which the first output elements of the coding system coincide with the original k elements", 
            "authors": [
                "A Shokrollahi"
            ], 
            "fields": [
                "Computer network", 
                "Fault tolerance", 
                "Decoding methods"
            ], 
            "title": "Raptor codes", 
            "url": "http://cn.bing.com/academic/profile?id=beabd4815e60aefcbf6cf80cafd7af0f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Two simple models of queueing on an N \\times N space-division packet switch are examined. The switch operates synchronously with fixed-length packets; during each time slot, packets may arrive on any inputs addressed to any outputs. Because packet arrivals to the switch are unscheduled, more than one packet may arrive for the same output during the same time slot, making queueing unavoidable. Mean queue lengths are always greater for queueing on inputs than for queueing on outputs, and the output queues saturate only as the utilization approaches unity. Input queues, on the other hand, saturate at a utilization that depends on N , but is approximately (2 -\\sqrt{2}) = 0.586 when N is large. If output trunk utilization is the primary consideration, it is possible to slightly increase utilization of the output trunks-upto (1 - e^{-1}) = 0.632 as N \\rightarrow \\infty -by dropping interfering packets at the end of each time slot, rather than storing them in the input queues. This improvement is possible, however, only when the utilization of the input trunks exceeds a second critical threshold-approximately ln (1 +\\sqrt{2}) = 0.881 for large N .", 
            "authors": [
                "M J Karol", 
                "Michael G Hluchyj", 
                "Samuel P Morgan"
            ], 
            "fields": [
                "Queue", 
                "Computer network", 
                "Throughput", 
                "Concurrent computing", 
                "Packet switching"
            ], 
            "title": "Input Versus Output Queueing on a Space-Division Packet Switch", 
            "url": "http://cn.bing.com/academic/profile?id=eb695dbae6a197a885cb65b7055be036&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to end performance in wireless and lossy systems. We compare several schemes designed to improve the performance of TCP in such networks. We classify these schemes into three broad categories: end-to-end protocols, where loss recovery is performed by the sender; link-layer protocols that provide local reliability; and split-connection protocols that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison. Our results show that a reliable link-layer protocol that is TCP-aware provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.", 
            "authors": [
                "Hamsa Balakrishnan", 
                "Venkat Padmanabhan", 
                "S Seshan", 
                "Randy H Katz"
            ], 
            "fields": [
                "Control system", 
                "Degradation", 
                "Base station", 
                "Bit error rate", 
                "Throughput"
            ], 
            "title": "A comparison of mechanisms for improving TCP performance over wireless links", 
            "url": "http://cn.bing.com/academic/profile?id=b08ca59f58d79d53c202f79abda0f757&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We present a framework for the emerging Internet quality of service (QoS). All the important components of this framework-integrated services, RSVP, differentiated services, multiprotocol label switching (MPLS), and constraint-based routing-are covered. We describe what integrated services and differentiated services are, how they can be implemented, and the problems they have. We then describe why MPLS and constraint-based routing have been introduced into this framework, how they differ from and relate to each other, and where they fit into the differentiated services architecture. Two likely service architectures are presented, and the end-to-end service deliveries in these two architectures are illustrated. We also compare ATM networks to router networks with differentiated services and MPLS. Putting all these together, we give the readers a grasp of the big picture of the emerging Internet QoS.", 
            "authors": [
                "Xipeng Xiao", 
                "Lingli Ni"
            ], 
            "fields": [
                "Differentiated service", 
                "Integrated services", 
                "Service-oriented architecture", 
                "The Internet", 
                "Communications protocol"
            ], 
            "title": "Internet QoS: a big picture", 
            "url": "http://cn.bing.com/academic/profile?id=3e60f312f6d095a401d8e12cd2984fe1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Current standard security practices do not provide substantial assurance that the end-to-end behavior of a computing system satisfies important security policies such as confidentiality. An end-to-end confidentiality policy might assert that secret input data cannot be inferred by an attacker through the attacker's observations of system output; this policy regulates information flow. Conventional security mechanisms such as access control and encryption do not directly address the enforcement of information-flow policies. Previously, a promising new approach has been developed: the use of programming-language techniques for specifying and enforcing information-flow policies. In this paper, we survey the past three decades of research on information-flow security, particularly focusing on work that uses static program analysis to enforce information-flow policies. We give a structured view of work in the area and identify some important open challenges.", 
            "authors": [
                "Andrei Sabelfeld", 
                "Andrew C Myers"
            ], 
            "fields": [
                "Computer network", 
                "Confidentiality", 
                "Covert channel", 
                "Data security", 
                "Information flow"
            ], 
            "title": "Language-based information-flow security", 
            "url": "http://cn.bing.com/academic/profile?id=49e7b5f904044aee89e8042008dad857&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Sensor networks hold the promise of facilitating large-scale, real-time data processing in complex environments, helping to protect and monitor military, environmental, safety-critical, or domestic infrastructures and resources, Denial-of-service attacks against such networks, however, may permit real world damage to public health and safety. Without proper security mechanisms, networks will be confined to limited, controlled environments, negating much of the promise they hold. The limited ability of individual sensor nodes to thwart failure or attack makes ensuring network availability more difficult. To identify denial-of-service vulnerabilities, the authors analyzed two effective sensor network protocols that did not initially consider security. These examples demonstrate that consideration of security at design time is the best way to ensure successful network deployment.", 
            "authors": [
                "Anthony D Wood", 
                "John A Stankovic"
            ], 
            "fields": [
                "Node", 
                "Data processing", 
                "Information security", 
                "Computer network", 
                "Occupational safety and health"
            ], 
            "title": "Denial of service in sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=b61f8ec22862a0960b8c1ba27c643f66&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A recursive analysis of network and institutional evolution is offered to account for the decentralized structure of the commercial field of the life sciences. Four alternative logics of attachment\u2014accumulative advantage, homophily, follow\u2010the\u2010trend, and multiconnectivity\u2014are tested to explain the structure and dynamics of interorganizational collaboration in biotechnology. Using multiple novel methods, the authors demonstrate how different rules for affiliation shape network evolution. Commercialization strategies pursued by early corporate entrants are supplanted by universities, research institutes, venture capital, and small firms. As organizations increase their collaborative activities and diversify their ties to others, cohesive subnetworks form, characterized by multiple, independent pathways. These structural components, in turn, condition the choices and opportunities available to members of a field, thereby reinforcing an attachment logic based on differential connections to diverse partners.", 
            "authors": [
                "Walter W Powell", 
                "Douglas R White", 
                "Kenneth W Koput", 
                "Jason Owensmith"
            ], 
            "fields": [
                "Venture capital", 
                "Cooperation", 
                "Statistical model", 
                "Computer network", 
                "Field"
            ], 
            "title": "Network Dynamics and Field Evolution: The Growth of Interorganizational Collaboration in the Life Sciences1", 
            "url": "http://cn.bing.com/academic/profile?id=975320a631ab591fc19e40f2f6d95343&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Data on social networks may be gathered for all ties linking elements of a closed population (\u201ccomplete\u201d network data) or for the sets of ties surrounding sampled individual units (\u201cegocentric\u201d network data). Network data have been obtained via surveys and questionnaires, archives, observation, diaries, electronic traces, and experiments. Most methodological research on data quality concerns surveys and questionnaires. The question of the accuracy with which informants can provide data on their network ties is nontrivial, but survey methods can make some claim to reliability. Unresolved issues include whether to measure perceived social ties or actual exchanges, how to treat temporal elements in the definition of relationships, and whether to seek accurate descriptions or reliable indicators. Continued research on data quality is needed; beyond improved samples and further investigation of the informant accuracy/reliability issue, this should cover common indices of network structure, address the conseque...", 
            "authors": [
                "Peter V Marsden"
            ], 
            "fields": [
                "Data", 
                "Measurement", 
                "Computer network"
            ], 
            "title": "NETWORK DATA AND MEASUREMENT", 
            "url": "http://cn.bing.com/academic/profile?id=de4ab5cd85a68cc5063a5278e7715715&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#The imputation problem is how to account for the sources of the value of the firm. I propose that part of the value of the firm derives from its participation in a network that emerges from the operation of generative rules that instruct the decision to cooperate. Whereas the value of firm-level capabilities is coincidental with the firm as the unit of accrual, ownership claims to the value of coordination in a network pit firms potentially in opposition with one another. We analyze the work on network structure to suggest two types of mechanisms by which rents are distributed. This approach is applied to an analysis of the Toyota Production System to show how a network emerged, the rents were divided to support network capabilities, and capabilities were transferred to the United States. Copyright \u00a9 2000 John Wiley & Sons, Ltd.", 
            "authors": [
                "Bruce Kogut"
            ], 
            "fields": [
                "Initial value problem", 
                "Property rights", 
                "Production system", 
                "Cooperation", 
                "Profitability index"
            ], 
            "title": "The network as knowledge: generative rules and the emergence of structure", 
            "url": "http://cn.bing.com/academic/profile?id=b014302eeb0f889df7382abd7fe8e3c5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In multihop packet radio networks with randomly distributed terminals, the optimal transmission radii to maximize the expected progress of packets in desired directions are determined with a variety of transmission protocols and network configurations. It is shown that the FM capture phenomenon with slotted ALOHA greatly improves the expected progress over the system without capture due to the more limited area of possibly interfering terminals around the receiver. The (mini)slotted nonpersistent carrier-sense-multiple-access (CSMA) only slightly outperforms ALOHA, unlike the single-hop case (where a large improvement is available), because of a large area of \"hidden\" terminals and the long vulnerable period generated by them. As an example of an inhomogeneous terminal distribution, the effect of a gap in an otherwise randomly distributed terminal population on the expected progress of packets crossing the gap is considered. In this case, the disadvantage of using a large transmission radius is demonstrated.", 
            "authors": [
                "Hideaki Takagi", 
                "Leonard Kleinrock"
            ], 
            "fields": [
                "Computer Science", 
                "Packet switching", 
                "Channel", 
                "Transmitter", 
                "Spread spectrum"
            ], 
            "title": "Optimal Transmission Ranges for Randomly Distributed Packet Radio Terminals", 
            "url": "http://cn.bing.com/academic/profile?id=c3e3b3884309d304ed8b34bc8607f318&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Shape memory polymer compositions, articles of manufacture thereof, and methods of preparation and use thereof are described. The shape memory polymer compositions can hold more than one shape in memory. Suitable compositions include at least one hard segment and at least one soft segment. The Ttrans of the hard segment is preferably between \u221230 and 270\u00b0 C. At least one of the hard or soft segments can contain a crosslinkable group, and the segments can be linked by formation of an interpenetrating network or a semi-interpenetrating network, or by physical interactions of the blocks. Objects can be formed into a given shape at a temperature above the Ttrans of the hard segment, and cooled to a temperature below the Ttrans of the soft segment. If the object is subsequently formed into a second shape, the object can return to its original shape by heating the object above the Ttrans of the soft segment and below the Ttrans of the hard segment. The compositions can also include two soft segments which are linked via functional groups which are cleaved in response to application of light, electric field, magnetic field or ultrasound. The cleavage of these groups causes the object to return to its original shape.", 
            "authors": [
                "Robert Langer", 
                "Andreas Lendlein"
            ], 
            "fields": [
                "Elastomer", 
                "Shape-memory polymer", 
                "Computer network", 
                "Copolymer"
            ], 
            "title": "Shape\u2010Memory Polymers", 
            "url": "http://cn.bing.com/academic/profile?id=7866b4376d633ad9f77f2ed082c474b5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The very broad bandwidth of low-loss optical transmission in a single-mode fiber and the recent improvements in single-frequency tunable lasers have stimulated significant advances in dense wavelength division multiplexed optical networks. This technology, including wavelength-sensitive optical switching and routing elements and passive optical elements, has made it possible to consider the use of wavelength as another dimension, in addition to time and space, in network and switch design. The independence of optical signals at different wavelengths makes this a natural choice for multiple-access networks, for applications which benefit from shared transmission media, and for networks in which very large throughputs are required. Recent progress in multiwavelength networks are reviewed, some of the limitations which affect the performance of such networks are discussed, and examples of several network and switch proposals based on these ideas are presented. Discussed also are critical technologies that are essential to progress in this field. >", 
            "authors": [
                "C A Brackett"
            ], 
            "fields": [
                "Telecommunications network", 
                "Stimulated emission", 
                "Performance", 
                "Optical communication", 
                "Tunable laser"
            ], 
            "title": "Dense wavelength division multiplexing networks: principles and applications", 
            "url": "http://cn.bing.com/academic/profile?id=7b84b1fefc08da061f7227c29cf713f3&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Methods We searched relevant sources from inception to M arch, 2007, and contacted investigators and manufacturers to identify randomised controlled trials in patients with coronary artery disease that compared drug-eluting with bare-metal stents, or that compared sirolimus-eluting stents head-to-head with paclitaxel-eluting stents. Safety outcomes included mortality, myocardial infarction, and defi nite stent thrombosis; the eff ectiveness outcome was target lesion revascularisation. We included 38 trials (18 023 patients) with a follow-up of up to 4 years. Trialists and manufacturers provided additional data on clinical outcomes for 29 trials. We did a network meta-analysis with a mixed-treatment comparison method to combine direct within-trial comparisons between stents with indirect evidence from other trials while maintaining randomisation. Findings Mortality was similar in the three groups: hazard ratios (HR) were 1\u00b700 (95% credibility interval 0\u00b782\u20131\u00b725) for sirolimus-eluting versus bare-metal stents, 1\u00b703 (0\u00b784\u20131\u00b722) for paclitaxel-eluting versus bare-metal stents, and 0\u00b796 (0\u00b783\u20131\u00b724) for sirolimus-eluting versus paclitaxel-eluting stents. S irolimus-eluting stents were associated with the lowest risk of myocardial infarction (HR 0\u00b781, 95% credibility interval 0\u00b766\u20130\u00b797, p=0\u00b7030 vs bare-metal stents; 0\u00b783, 0\u00b771\u20131\u00b700, p=0\u00b7045 vs paclitaxel-eluting stents). There were no signifi cant diff erences in the risk of defi nite stent thrombosis (0 days to 4 years). However, the risk of late defi nite stent thrombosis (>30 days) was increased with paclitaxel-eluting stents (HR 2\u00b711, 95% credibility interval 1\u00b719\u20134\u00b723, p=0\u00b7017 vs bare-metal stents; 1\u00b785, 1\u00b702\u20133\u00b785, p=0\u00b7041 vs sirolimus-eluting stents). The reduction in target lesion revascularisation seen with drug-eluting stents compared with bare-metal stents was more pronounced with sirolimus-eluting stents than with paclitaxel-eluting stents (0\u00b770, 0\u00b756\u20130\u00b784; p=0\u00b70021).", 
            "authors": [
                "Christoph Stettler", 
                "Simon Wandel", 
                "Sabin Allemann", 
                "Adnan Kastrati", 
                "M C Morice", 
                "Albert Schomig", 
                "Matthias Pfisterer", 
                "Gregg W Stone", 
                "Martin B Leon", 
                "Jose Suarez De Lezo", 
                "Jeanjacques Goy", 
                "Seungjung Park", 
                "Manel Sabate", 
                "Maarten J Suttorp", 
                "Henning Kelbaek", 
                "Christian Spaulding", 
                "M Menichelli", 
                "Paul Vermeersch", 
                ""
            ], 
            "fields": [
                "Computer network", 
                "Meta-analysis", 
                "Association", 
                "Medicine", 
                "Evolution"
            ], 
            "title": "Outcomes associated with drug-eluting and bare-metal stents: a collaborative network meta-analysis", 
            "url": "http://cn.bing.com/academic/profile?id=4d09ead2affebe349e8eabbf5b428a45&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "There has been considerable recent interest in algorithms for finding communities in networks\u2014 groups of vertices within which connections are dense, but between which connections are sparser. Here we review the progress that has been made towards this end. We begin by describing some traditional methods of community detection, such as spectral bisection, the Kernighan-Lin algorithm and hierarchical clustering based on similarity measures. None of these methods, however, is ideal for the types of real-world network data with which current research is concerned, such as Internet and web data and biological and social networks. We describe a number of more recent algorithms that appear to work well with these data, including algorithms based on edge betweenness scores, on counts of short loops in networks and on voltage differences in resistor networks.", 
            "authors": [
                "M E J Newman"
            ], 
            "fields": [
                "Physics", 
                "Complex systems", 
                "Community structure", 
                "Mathematics", 
                "Computer network"
            ], 
            "title": "Detecting community structure in networks", 
            "url": "http://cn.bing.com/academic/profile?id=90390ec42295ddb1b4acf3b0b0c2bd62&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Computer networks are social networks. Social affordances of computer-supported social networks - broader bandwidth, wireless portability, globalized connectivity, personalization - are fostering the movement from door-to-door and place-to-place communities to person-to-person and role-to-role communities. People connect in social networks rather than in communal groups. In-person and computer-mediated communication are integrated in communities characterized by personalized networking. Copyright Joint Editors and Blackwell Publishers Ltd 2001.", 
            "authors": [
                "Barry Wellman"
            ], 
            "fields": [
                "Drainage divide", 
                "Social network", 
                "Bibliography", 
                "Management", 
                "Knowledge"
            ], 
            "title": "Physical Place and Cyberplace: The Rise of Personalized Networking", 
            "url": "http://cn.bing.com/academic/profile?id=96b6c415d812cadf9126061bcd5e52ff&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "People use weak ties\u2014relationships with acquaintances or strangers\u2014to seek help unavailable from friends or colleagues. Yet in the absence of personal relationships or the expectation of direct reciprocity, help from weak ties might not be forthcoming or could be of low quality. We examined the practice of distant employees (strangers) exchanging technical advice through a large organizational computer network. A survey of advice seekers and those who replied was conducted to test hypotheses about the viability and usefulness of such electronic weak tie exchanges. Theories of organizational motivation suggest that positive regard for the larger organization can substitute for direct incentives or personal relationships in motivating people to help others. Theories of weak ties suggest that the usefulness of this help may depend on the number of ties, the diversity of ties, or the resources of help providers. We hypothesized that, in an organizational context, the firm-specific resources and organizational...", 
            "authors": [
                "David Constant", 
                "Lee Sproull", 
                "Sara Kiesler"
            ], 
            "fields": [
                "Helping behavior", 
                "Information system", 
                "Social network", 
                "Altruism", 
                "Organizational behavior"
            ], 
            "title": "The Kindness of Strangers: The Usefulness of Electronic Weak Ties for Technical Advice", 
            "url": "http://cn.bing.com/academic/profile?id=0327b00de90aaea6199b0114c9758d97&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Intrusion detection is a new, retrofit approach for providing a sense of security in existing computers and data networks, while allowing them to operate in their current \"open\" mode. The goal of intrusion detection is to identify unauthorized use, misuse, and abuse of computer systems by both system insiders and external penetrators. The intrusion detection problem is becoming a challenging task due to the proliferation of heterogeneous computer networks since the increased connectivity of computer systems gives greater access to outsiders and makes it easier for intruders to avoid identification. Intrusion detection systems (IDSs) are based on the beliefs that an intruder's behavior will be noticeably different from that of a legitimate user and that many unauthorized actions are detectable. Typically, IDSs employ statistical anomaly and rulebased misuse models in order to detect intrusions. A number of prototype IDSs have been developed at several institutions, and some of them have also been deployed on an experimental basis in operational systems. In the present paper, several host-based and network-based IDSs are surveyed, and the characteristics of the corresponding systems are identified. The host-based systems employ the host operating system's audit trails as the main source of input to detect intrusive activity, while most of the network-based IDSs build their detection mechanism on monitored network traffic, and some employ host audit trails as well. An outline of a statistical anomaly detection algorithm employed in a typical IDS is also included. >", 
            "authors": [
                "B Mukherjee", 
                "L T Heberlein", 
                "K N Levitt"
            ], 
            "fields": [
                "Computer network", 
                "Open system"
            ], 
            "title": "Network intrusion detection", 
            "url": "http://cn.bing.com/academic/profile?id=5adf804efcd3a1b47fd429544fa2e8d0&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In this article, we seek to address a simple question: \u201cHow prevalent are denial-of-service attacks in the Internet?\u201d Our motivation is to quantitatively understand the nature of the current threat as well as to enable longer-term analyses of trends and recurring patterns of attacks. We present a new technique, called \u201cbackscatter analysis,\u201d that provides a conservative estimate of  worldwide  denial-of-service activity. We use this approach on 22 traces (each covering a week or more) gathered over three years from 2001 through 2004. Across this corpus we quantitatively assess the number, duration, and focus of attacks, and qualitatively characterize their behavior. In total, we observed over 68,000 attacks directed at over 34,000 distinct victim IP addresses---ranging from well-known e-commerce companies such as Amazon and Hotmail to small foreign ISPs and dial-up connections. We believe our technique is the first to provide quantitative estimates of Internet-wide denial-of-service activity and that this article describes the most comprehensive public measurements of such activity to date.", 
            "authors": [
                "David J Moore", 
                "Colleen Shannon", 
                "Douglas J Brown", 
                "Geoffrey M Voelker", 
                "Stefan Savage"
            ], 
            "fields": [
                "Network security", 
                "Backscatter", 
                "Computer security", 
                "Computer network", 
                "The Internet"
            ], 
            "title": "Inferring Internet denial-of-service activity", 
            "url": "http://cn.bing.com/academic/profile?id=ebe72e46368dc4730a41f3e06cdd7334&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In this paper we consider a physical model in which a buffer receives messages from a finite number of statistically independent and identical information sources that asynchronously alternate between exponentially distributed periods in the \u2018on\u2019 and \u2018off\u2019 states. While on, a source transmits at a uniform rate. The buffer depletes through an output channel with a given maximum rate of transmission. This model is useful for a data-handling switch in a computer network. The equilibrium buffer distribution is described by a set of differential equations, which are analyzed herein. The mathematical results render trivial the computation of the distribution and its moments and thus also the waiting time moments. The main result explicitly gives all the system's eigenvalues. While the insertion of boundary conditions requires the solution of a matrix equation, even this step is eliminated since the matrix inverse is given in closed form. Finally, the simple expression given here for the asymptotic behavior of buffer content is insightful, for purposes of design, and numerically useful. Numerical results for a broad range of system parameters are presented graphically.", 
            "authors": [
                "David J Anick", 
                "Debasis Mitra", 
                "M M Sondhi"
            ], 
            "fields": [
                "Computer network", 
                "Stochastic process", 
                "Eigenvalues and eigenvectors", 
                "Physical model", 
                "Differential equation"
            ], 
            "title": "Stochastic theory of a data-handling system with multiple sources", 
            "url": "http://cn.bing.com/academic/profile?id=43f767265fc076244dd11ac59e2882ae&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The upcoming gigabit-per-second high-speed networks are expected to support a wide range of communication-intensive real-time multimedia applications. The requirement for timely delivery of digitized audio-visual information raises new challenges for next-generation integrated services broadband networks. One of the key issues is QoS routing. It selects network routes with sufficient resources for the requested QoS parameters. The goal of routing solutions is twofold: (1) satisfying the QoS requirements for every admitted connection, and (2) achieving global efficiency in resource utilization. Many unicast/multicast QoS routing algorithms have been published, and they work with a variety of QoS requirements and resource constraints. Overall, they can be partitioned into three broad classes: (1) source routing, (2) distributed routing, and (3) hierarchical routing algorithms. We give an overview of the QoS routing problem as well as the existing solutions. We present the strengths and weaknesses of different routing strategies, and outline the challenges. We also discuss the basic algorithms in each class, classify and compare them, and point out possible future directions in the QoS routing area.", 
            "authors": [
                "Shigang Chen", 
                "Klara Nahrstedt"
            ], 
            "fields": [
                "Broadband networks", 
                "In situ resource utilization", 
                "Hierarchical routing", 
                "Satisfiability", 
                "Routing"
            ], 
            "title": "An overview of quality of service routing for next-generation high-speed networks: problems and solutions", 
            "url": "http://cn.bing.com/academic/profile?id=72af8a9cb00736d1febef807d16f5a25&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The authors propose a computationally simple approximate expression for the equivalent capacity or bandwidth requirement of both individual and multiplexed connections, based on their statistical characteristics and the desired grade-of-service (GOS). The purpose of such an expression is to provide a unified metric to represent the effective bandwidth used by connections and the corresponding effective load of network links. These link metrics can then be used for efficient bandwidth management, routing, and call control procedures aimed at optimizing network usage. While the methodology proposed can provide an exact approach to the computation of the equivalent capacity, the associated complexity makes it infeasible for real-time network traffic control applications. Hence, an approximation is required. The validity of the approximation developed is verified by comparison to both exact computations and simulation results. >", 
            "authors": [
                "Roch Guerin", 
                "Hamid Ahmadi", 
                "Mahmoud Naghshineh"
            ], 
            "fields": [
                "Channel allocation schemes", 
                "Passband", 
                "Real-time computing", 
                "Intelligent Network", 
                "Bandwidth"
            ], 
            "title": "Equivalent capacity and its application to bandwidth allocation in high-speed networks", 
            "url": "http://cn.bing.com/academic/profile?id=a64f91c90f404c10375738cb688492f5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The technologies, architectures, and methodologies traditionally used to develop distributed applications exhibit a variety of limitations and drawbacks when applied to large scale distributed settings (e.g., the Internet). In particular, they fail in providing the desired degree of configurability, scalability, and customizability. To address these issues, researchers are investigating a variety of innovative approaches. The most promising and intriguing ones are those based on the ability of moving code across the nodes of a network, exploiting the notion of mobile code. As an emerging research field, code mobility is generating a growing body of scientific literature and industrial developments. Nevertheless, the field is still characterized by the lack of a sound and comprehensive body of concepts and terms. As a consequence, it is rather difficult to understand, assess, and compare the existing approaches. In turn, this limits our ability to fully exploit them in practice, and to further promote the research work on mobile code. Indeed, a significant symptom of this situation is the lack of a commonly accepted and sound definition of the term mobile code itself. This paper presents a conceptual framework for understanding code mobility. The framework is centered around a classification that introduces three dimensions: technologies, design paradigms, and applications. The contribution of the paper is two-fold. First, it provides a set of terms and concepts to understand and compare the approaches based on the notion of mobile code. Second, it introduces criteria and guidelines that support the developer in the identification of the classes of applications that can leverage off of mobile code, in the design of these applications, and, finally, in the selection of the most appropriate implementation technologies. The presentation of the classification is intertwined with a review of state-of-the-art in the field. Finally, the use of the classification is exemplified in a case study.", 
            "authors": [
                "Alfonso Fuggetta", 
                "Gian Pietro Picco", 
                "Giovanni Vigna"
            ], 
            "fields": [
                "Object-oriented programming", 
                "Computer hardware", 
                "Conceptual framework", 
                "Code mobility", 
                "Appropriate technology"
            ], 
            "title": "Understanding code mobility", 
            "url": "http://cn.bing.com/academic/profile?id=306694bb51b0ec342227bb51e1009fcd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Centrality measures, or at least popular interpretations of these measures, make implicit assumptions about the manner in which traffic flows through a network. For example, some measures count only geodesic paths, apparently assuming that whatever flows through the network only moves along the shortest possible paths. This paper lays out a typology of network flows based on two dimensions of variation, namely the kinds of trajectories that traffic may follow (geodesics, paths, trails, or walks) and the method of spread (broadcast, serial replication, or transfer). Measures of centrality are then matched to the kinds of flows that they are appropriate for. Simulations are used to examine the relationship between type of flow and the differential importance of nodes with respect to key measurements such as speed of reception of traffic and frequency of receiving traffic. It is shown that the off-the-shelf formulas for centrality measures are fully applicable only for the specific flow processes they are designed for, and that when they are applied to other flow processes they get the \u201cwrong\u201d answer. It is noted that the most commonly used centrality measures are not appropriate for most of the flows we are routinely interested in. A key claim made in this paper is that centrality measures can be regarded as generating expected values for certain kinds of node outcomes (such as speed and frequency of reception) given implicit models of how traffic flows, and that this provides a new and useful way of thinking about centrality. \u00a9 2004 Elsevier B.V. All rights reserved.", 
            "authors": [
                "Stephen P Borgatti"
            ], 
            "fields": [
                "Traffic flow", 
                "Centrality", 
                "Flow network", 
                "Flow", 
                "Computer network"
            ], 
            "title": "Centrality and network flow", 
            "url": "http://cn.bing.com/academic/profile?id=93bffe4825af23a1426e218e9d69619b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Multimedia communication involving digital audio and/or digital video has rather strict delay requirements. A real-time channel is defined as a simplex connection between a source and a destination characterized by parameters representing the performance requirements of the client. A study is made of the feasibility of providing real-time services on a packet-switched store-and-forward wide-area network with general topology. A description is given of a scheme for the establishment of channels with deterministic or statistical delay bounds, and the results of the simulation experiments run to evaluate it are presented. The results are judged encouraging: the approach satisfies the guarantees even in worst case situations, uses the network's resources to a fair extent, and efficiently handles channels with a variety of offered load and burstiness characteristics. Also, the packet transmission overhead is quite low, and the channel establishment overhead is small enough to be acceptable in most practical cases. >", 
            "authors": [
                "D Ferrari", 
                "D C Verma"
            ], 
            "fields": [
                "Real-time computing", 
                "Performance", 
                "Packet switching", 
                "Digital audio", 
                "Bandwidth"
            ], 
            "title": "A scheme for real-time channel establishment in wide-area networks", 
            "url": "http://cn.bing.com/academic/profile?id=8bda3897830ff32ea4c9429611d70470&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An architectural approach that meets high bandwidth requirements by introducing a communication architecture based on lightpaths, optical transmission paths in the network, is introduced. Since lightpaths form the building block of the proposed architecture, its performance hinges on their efficient establishment and management. It is shown that although the problem of optimally establishing lightpaths is NP-complete, simple heuristics provide near optimal substitutes for several of the basic problems motivated by a lightpath-based architecture. >", 
            "authors": [
                "Imrich Chlamtac", 
                "Aura Ganz", 
                "Gadi Karmi"
            ], 
            "fields": [
                "Bandwidth", 
                "Throughput", 
                "Computer network", 
                "Wavelength-division multiplexing", 
                "NP-complete"
            ], 
            "title": "Lightpath communications: an approach to high bandwidth optical WAN's", 
            "url": "http://cn.bing.com/academic/profile?id=0fb1732b0acd509492983120e0bd3213&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#I argue that the linkage-formation propensity of firms is explained by simultaneously examining both inducement and opportunity factors. Drawing upon resource-based and social network theory literatures I identify three forms of accumulated capital\u2014technical, commercial, and social\u2014that can affect a firm\u2019s inducements and opportunities to form linkages. Firms possessing these capital stocks enjoy advantages in linkages formation. However, firms lacking these accumulated resources can still form linkages if they generate a radical technological breakthrough. Thus, I identify paths to linkage formation for leading as well as peripheral firms. I test these arguments with longitudinal data on technical collaborative linkages in the global chemicals industry. Copyright \u00a9 2000 John Wiley & Sons, Ltd.", 
            "authors": [
                "Gautam Ahuja"
            ], 
            "fields": [
                "Capital", 
                "Social network", 
                "Business", 
                "Innovation", 
                "Social capital"
            ], 
            "title": "The duality of collaboration: inducements and opportunities in the formation of interfirm linkages", 
            "url": "http://cn.bing.com/academic/profile?id=5f086e109ec1806f1ca34122b55bbfe8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Fair queuing is a technique that allows each flow passing through a network device to have a fair share of network resources. Previous schemes for fair queuing that achieved nearly perfect fairness were expensive to implement; specifically, the work required to process a packet in these schemes was O(log(n)), where n is the number of active flows. This is expensive at high speeds. On the other hand, cheaper approximations of fair queuing reported in the literature exhibit unfair behavior. In this paper, we describe a new approximation of fair queuing, that we call deficit round-robin. Our scheme achieves nearly perfect fairness in terms of throughput, requires only O(1) work to process a packet, and is simple enough to implement in hardware. Deficit round-robin is also applicable to other scheduling problems where servicing cannot be broken up into smaller units (such as load balancing) and to distributed queues.", 
            "authors": [
                "M Shreedhar", 
                "George Varghese"
            ], 
            "fields": [
                "Packet switching", 
                "Computer Science", 
                "Process control", 
                "Scheduling", 
                "Throughput"
            ], 
            "title": "Efficient fair queueing using deficit round-robin", 
            "url": "http://cn.bing.com/academic/profile?id=d9898e133cf2958f50f10b4e0e60095b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "When using authentication based on cryptography, an attacker listening to the network gains no information that would enable it to falsely claim another's identity. Kerberos is the most commonly used example of this type of authentication technology. The authors concentrate on authentication for real-time, interactive services that are offered on computer networks. They use the term real-time loosely to mean that a client process is waiting for a response to a query or command so that it can display the results to the user, or otherwise continue performing its intended function. This class of services includes remote login, file system reads and writes, and information retrieval for applications like Mosaic. >", 
            "authors": [
                "B C Neuman", 
                "T Tso"
            ], 
            "fields": [
                "Computer network", 
                "Cryptography", 
                "Information retrieval", 
                "Real-time computing", 
                "Message authentication code"
            ], 
            "title": "Kerberos: an authentication service for computer networks", 
            "url": "http://cn.bing.com/academic/profile?id=1cea3988b303e71a5367a21ec5b46215&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Bayesian belief networks provide a natural, efficient method for representing probabilistic dependencies among a set of variables. For these reasons, numerous researchers are exploring the use of belief networks as a knowledge representation m artificial intelligence. Algorithms have been developed previously for efficient probabilistic inference using special classes of belief networks. More general classes of belief networks, however, have eluded efforts to develop efficient inference algorithms. We show that probabilistic inference using belief networks is NP-hard. Therefore, it seems unlikely that an exact algorithm can be developed to perform probabilistic inference efficiently over all classes of belief networks. This result suggests that research should be directed away from the search for a general, efficient probabilistic inference algorithm, and toward the design of efficient special-case, average-case, and approximation algorithms.", 
            "authors": [
                "Gregory F Cooper"
            ], 
            "fields": [
                "Computer network", 
                "Expert system", 
                "Belief", 
                "NP-hard", 
                "Algorithm"
            ], 
            "title": "The computational complexity of probabilistic inference using Bayesian belief networks", 
            "url": "http://cn.bing.com/academic/profile?id=750f681ebbb0e70da6a0b91b1a9f8566&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "When computer networks link people as well as machines, they become social networks. Such computer-supported social networks (CSSNs) are becoming important bases of virtual communities, computer-supported cooperative work, and telework. Computer-mediated communication such as electronic mail and computerized conferencing is usually text-based and asynchronous. It has limited social presence, and on-line communications are often more uninhibited, creative, and blunt than in-person communication. Nevertheless, CSSNs sustain strong, intermediate, and weak ties that provide information and social support in both specialized and broadly based relationships. CSSNs foster virtual communities that are usually partial and narrowly focused, although some do become encompassing and broadly based. CSSNs accomplish a wide variety of cooperative work, connecting workers within and between organizations who are often physically dispersed. CSSNs also link teleworkers from their homes or remote work centers to main organi...", 
            "authors": [
                "Barry Wellman", 
                "Janet W Salaff", 
                "Dimitrina Dimitrova", 
                "Laura Garton", 
                "Milena Gulia", 
                "Caroline Haythornthwaite"
            ], 
            "fields": [
                "Social network", 
                "Information system", 
                "Computer Science", 
                "Computer-mediated communication", 
                "Computer-supported cooperative work"
            ], 
            "title": "Computer Networks as Social Networks: Collaborative Work, Telework, and Virtual Community", 
            "url": "http://cn.bing.com/academic/profile?id=9444131057a3572d44c3221139b6e052&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "While today's computer networks support only best-effort service, future packet-switching integrated-services networks will have to support real-time communication services that allow clients to transport information with performance guarantees expressed in terms of delay, delay jitter, throughput, and loss rate. An important issue in providing guaranteed performance service is the choice of the packet service discipline at switching nodes. In this paper, we survey several service disciplines that are proposed in the literature to provide per-connection end-to-end performance guarantees in packet-switching networks. We describe their mechanisms, their similarities and differences and the performance guarantees they can provide. Various issues and tradeoffs in designing service disciplines for guaranteed performance service are discussed, and a general framework for studying and comparing these disciplines are presented. >", 
            "authors": [
                "Hui Zhang"
            ], 
            "fields": [
                "Integrated Services Digital Network", 
                "Real-time computing", 
                "Intelligent Network", 
                "Best-effort delivery", 
                "Channel capacity"
            ], 
            "title": "Service disciplines for guaranteed performance service in packet-switching networks", 
            "url": "http://cn.bing.com/academic/profile?id=dc66daa4732fc126422af734553fb27f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "From the Publisher:#R##N#The most up-to-date introduction to the field of computer networking, this book's top-down approach starts at the application layer and works down the protocol stack. It also uses the Internet as the main example of networks. This all creates a book relevant to those interested in networking today. By starting at the application-layer and working down the protocol stack, this book provides a relevant introduction of important concepts. Based on the rationale that once a reader understands the applications of networks they can understand the network services needed to support these applications, this book takes a \"top-down\" approach that exposes readers first to a concrete application and then draws into some of the deeper issues surrounding networking. This book focuses on the Internet as opposed to addressing it as one of many computer network technologies, further motivating the study of the material. This book is designed for programmers who need to learn the fundamentals of computer networking. It also has extensive material making it of great interest to networking professionals.", 
            "authors": [
                "Jim Kurose", 
                "Keith W Ross"
            ], 
            "fields": [
                "Top-down and bottom-up design", 
                "Knowledge", 
                "Business intelligence", 
                "Computer network", 
                "Computer security"
            ], 
            "title": "Computer Networking: A Top-Down Approach Featuring the Internet", 
            "url": "http://cn.bing.com/academic/profile?id=750c981d9d5ce544b55a433d936b9e4c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "reported under AM1.5 (AM: air mass) illumination, this efficiency is not sufficient to meet realistic specifications for commercialization. The need to improve the light-to-electricity conversion efficiency requires the implementation of new materials and the exploration of new device architectures. Polymer-based photovoltaic cells are thin-film devices fabricated in the metal-insulator-metal configuration sketched in Figure 1a. The absorbing and charge-separating bulk-heterojunction layer with a thickness of approximately 100 nm is sandwiched between two charge-selective electrodes; a transparent bilayer electrode comprising poly(3,4-ethylenedioxylenethiophene):polystyrene sulfonic acid (PEDOT:PSS) on indium tin oxide (ITO) glass for collecting the holes and a lower-work-function metal (here, Al) for collecting the electrons. The work-function difference between the two electrodes provides a built-in potential that breaks the symmetry, thereby providing a driving force for the photogenerated electrons and holes toward their respective electrodes. Because of optical interference between the incident (from the ITO side) and back-reflected light, the intensity of the light is zero at the metallic (Al) electrode; Figure 1a shows a schematic representation of the spatial distribution of the squared optical electric-field strength. [9\u201311] Thus, a relatively large fraction of the active layer is in a dead-zone in which the photogeneration of carriers is significantly reduced. Moreover, this effect causes more electron\u2013hole pairs to be produced near the ITO/PEDOT:PSS electrode, a distribution which is known to reduce the photovoltaic conversion efficiency. [12,13] This \u201coptical interference effect\u201d is especially important for thin-film structures where layer thicknesses are comparable to the absorption depth and the wavelength of the incident light, as is the case for photovoltaic cells fabricated from semiconducting polymers. In order to overcome these problems, one might simply increase the thickness of the active layer to absorb more light. Because of the low mobility of the charge carriers in the polymer:C60 composites, however, the increased internal resistance of thicker films will inevitably lead to a reduced fill factor. An alternative approach is to change the device architecture with the goal of spatially redistributing the light intensity inside the device by introducing an optical spacer between the active layer and the Al electrode as sketched in Figure 1a. [11] Although this revised architecture would appear to solve the problem, the prerequisites for an ideal optical spacer limit the choice of materials: the layer must be a good acceptor and an electron-transport material with a conduction band edge lower in energy than that of the lowest unoccupied molecular orbital (LUMO) of C60; the LUMO must be above (or close to) the Fermi energy of the collecting metal electrode; and it must be transparent to light with wavelengths within the solar spectrum.", 
            "authors": [
                "J Y Kim", 
                "S H Kim", 
                "Hajin Lee", 
                "K Lee", 
                "Wanli Ma", 
                "X Gong", 
                "A J Heeger"
            ], 
            "fields": [
                "Work function", 
                "Electric field", 
                "Spectrum", 
                "Geographic coordinate conversion", 
                "Titanium oxide"
            ], 
            "title": "New Architecture for High-Efficiency Polymer Photovoltaic Cells Using Solution-Based Titanium Oxide as an Optical Spacer\u2020", 
            "url": "http://cn.bing.com/academic/profile?id=c2dea2cdbff3de20f568cd65b3a0ec4d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Despite evidence pointing to a ubiquitous tendency of human minds to wander, little is known about the neural operations that support this core component of human cognition. Using both thought sampling and brain imaging, the current investigation demonstrated that mind-wandering is associated with activity in a default network of cortical regions that are active when the brain is \u201cat rest.\u201d In addition, individuals\u2019 reports of the tendency of their minds to wander were correlated with activity in this network.", 
            "authors": [
                "Malia F Mason", 
                "Michael I Norton", 
                "John D Van Horn", 
                "Daniel M Wegner", 
                "Scott T Grafton", 
                "C Neil Macrae"
            ], 
            "fields": [
                "Central nervous system", 
                "Repurchase agreement", 
                "Thought", 
                "Cognition", 
                "Medical imaging"
            ], 
            "title": "Wandering minds: The default network and stimulus-independent thought", 
            "url": "http://cn.bing.com/academic/profile?id=dcb21e6a81acfc611e41d2763352a27d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We introduce NeighborNet, a network construction and data representation method that combines aspects of the neighbor joining (NJ) and SplitsTree. Like NJ, NeighborNet uses agglomeration: taxa are combined into progressively larger and larger overlapping clusters. Like SplitsTree, NeighborNet constructs networks rather than trees, and so can be used to represent multiple phylogenetic hypotheses simultaneously, or to detect complex evolutionary processes like recombination, lateral transfer and hybridization. NeighborNet tends to produce networks that are substantially more resolved than those made with SplitsTree. The method is efficient (O(n3) time) and is well suited for the preliminary analyses of complex phylogenetic data. We report results of three case studies: one based on mitochondrial gene order data from early branching eukaryotes, another based on nuclear sequence data from New Zealand alpine buttercups (Ranunculi), and a third on poorly corrected synthetic data.", 
            "authors": [
                "David M Bryant", 
                "Vincent Moulton"
            ], 
            "fields": [
                "Phylogenetic network", 
                "Recombination", 
                "Computer network", 
                "Neighbor joining"
            ], 
            "title": "Neighbor-Net: An Agglomerative Method for the Construction of Phylogenetic Networks", 
            "url": "http://cn.bing.com/academic/profile?id=433925aaf81886aa9b19e8d509afa03a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "ALLIANCE is a software architecture that facilitates the fault tolerant cooperative control of teams of heterogeneous mobile robots performing missions composed of loosely coupled subtasks that may have ordering dependencies. ALLIANCE allows teams of robots, each of which possesses a variety of high-level functions that it can perform during a mission, to individually select appropriate actions throughout the mission based on the requirements of the mission, the activities of other robots, the current environmental conditions, and the robot's own internal states. ALLIANCE is a fully distributed, behaviour-based architecture that incorporates the use of mathematically-modeled motivations (such as impatience and acquiescence) within each robot to achieve adaptive action selection. Since cooperative robotic teams usually work in dynamic and unpredictable environments, this software architecture allows the robot team members to respond robustly, reliably, flexibly, and coherently to unexpected environmental changes and modifications in the robot team that may occur due to mechanical failure, the learning of new skills, or the addition or removal of robots from the team by human intervention. The feasibility of this architecture is demonstrated in an implementation on a team of mobile robots performing a laboratory version of hazardous waste cleanup.", 
            "authors": [
                "Lynne E Parker"
            ], 
            "fields": [
                "Computer network", 
                "Environmental change", 
                "Action selection", 
                "Adaptive control", 
                "Index term"
            ], 
            "title": "ALLIANCE: an architecture for fault tolerant multirobot cooperation", 
            "url": "http://cn.bing.com/academic/profile?id=4ac57985b5f3fe24e7394676bf96c0a5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Motivated by applications to sensor, peer-to-peer, and ad hoc networks, we study distributed algorithms, also known as gossip algorithms, for exchanging information and for computing in an arbitrarily connected network of nodes. The topology of such networks changes continuously as new nodes join and old nodes leave the network. Algorithms for such networks need to be robust against changes in topology. Additionally, nodes in sensor networks operate under limited computational, communication, and energy resources. These constraints have motivated the design of \"gossip\" algorithms: schemes which distribute the computational burden and in which a node communicates with a randomly chosen neighbor. We analyze the averaging problem under the gossip constraint for an arbitrary network graph, and find that the averaging time of a gossip algorithm depends on the second largest eigenvalue of a doubly stochastic matrix characterizing the algorithm. Designing the fastest gossip algorithm corresponds to minimizing this eigenvalue, which is a semidefinite program (SDP). In general, SDPs cannot be solved in a distributed fashion; however, exploiting problem structure, we propose a distributed subgradient method that solves the optimization problem over the network. The relation of averaging time to the second largest eigenvalue naturally relates it to the mixing time of a random walk with transition probabilities derived from the gossip algorithm. We use this connection to study the performance and scaling of gossip algorithms on two popular networks: Wireless Sensor Networks, which are modeled as Geometric Random Graphs, and the Internet graph under the so-called Preferential Connectivity (PC) model.", 
            "authors": [
                "Stephen Boyd", 
                "Arpita Ghosh", 
                "Balaji Prabhakar", 
                "Devavrat Shah"
            ], 
            "fields": [
                "Semidefinite programming", 
                "Network topology", 
                "Markov chain", 
                "Index term", 
                "Information exchange"
            ], 
            "title": "Randomized gossip algorithms", 
            "url": "http://cn.bing.com/academic/profile?id=9a1c9786a4ef9e7b63212ad249902c59&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The nodal method has been widely used for formulating circuit equations in computer-aided network analysis and design programs. However, several limitations exist in this method including the inability to process voltage sources and current-dependent circuit elements in a simple and efficient manner. A modified nodal analysis (MNA) method is proposed here which retains the simplicity and other advantages of nodal analysis while removing its limitations. A simple and effective pivoting scheme is also given. Numerical examples are used to compare the MNA method with the tableau method. Favorable results are observed for the MNA method in terms of the dimension, number of nonzeros, and fill-ins for comparable circuit matrices.", 
            "authors": [
                "Chungwen Ho", 
                "A E Ruehli", 
                "P Brennan"
            ], 
            "fields": [
                "Modified nodal analysis", 
                "Helium", 
                "Admittance", 
                "Network analysis", 
                "Computer network"
            ], 
            "title": "The modified nodal approach to network analysis", 
            "url": "http://cn.bing.com/academic/profile?id=ba170857369dd1987ce6158032d5c1ed&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "article Here, we demonstrate that subject motion produces substantial changes in the timecourses of resting state functional connectivity MRI (rs-fcMRI) data despite compensatory spatial registration and regression of mo- tion estimates from the data. These changes cause systematic but spurious correlation structures throughout the brain. Specifically, many long-distance correlations are decreased by subject motion, whereas many short-distance correlations are increased. These changes in rs-fcMRI correlations do not arise from, nor are they adequately countered by, some common functional connectivity processing steps. Two indices of data quality are proposed, and a simple method to reduce motion-related effects in rs-fcMRI analyses is demon- strated that should be flexibly implementable across a variety of software platforms. We demonstrate how application of this technique impacts our own data, modifying previous conclusions about brain develop- ment. These results suggest the need for greater care in dealing with subject motion, and the need to critically revisit previous rs-fcMRI work that may not have adequately controlled for effects of transient subject movements.", 
            "authors": [
                "Jonathan D Power", 
                "Kelly Anne Barnes", 
                "Abraham Z Snyder", 
                "Bradley L Schlaggar", 
                "Steven E Petersen"
            ], 
            "fields": [
                "Noise", 
                "Motion", 
                "Movement", 
                "Computer network", 
                "Algorithm"
            ], 
            "title": "Spurious but systematic correlations in functional connectivity MRI networks arise from subject motion.", 
            "url": "http://cn.bing.com/academic/profile?id=a248f518639944a8a18f0c5845d8db87&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Digital audio, video, images, and documents are flying through cyberspace to their respective owners. Unfortunately, along the way, individuals may choose to intervene and take this content for themselves. Digital watermarking and steganography technology greatly reduces the instances of this by limiting or eliminating the ability of third parties to decipher the content that he has taken. The many techiniques of digital watermarking (embedding a code) and steganography (hiding information) continue to evolve as applications that necessitate them do the same. The authors of this second edition provide an update on the framework for applying these techniques that they provided researchers and professionals in the first well-received edition. Steganography and steganalysis (the art of detecting hidden information) have been added to a robust treatment of digital watermarking, as many in each field research and deal with the other. New material includes watermarking with side information, QIM, and dirty-paper codes. The revision and inclusion of new material by these influential authors has created a must-own book for anyone in this profession.#R##N##R##N#*This new edition now contains essential information on steganalysis and steganography#R##N#*New concepts and new applications including QIM introduced#R##N#*Digital watermark embedding is given a complete update with new processes and applications", 
            "authors": [
                "Ingemar J Cox", 
                "Matthew L Miller", 
                "Jeffrey A Bloom", 
                "Jessica Fridrich", 
                "Ton Kalker"
            ], 
            "fields": [
                "Computer network", 
                "Information security"
            ], 
            "title": "Digital Watermarking and Steganography", 
            "url": "http://cn.bing.com/academic/profile?id=29af710f72c63a53f2ca7be906840858&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The 1990s have seen a rapid growth of research interests in mobile ad hoc networking. The infrastructureless and the dynamic nature of these networks demands new set of networking strategies to be implemented in order to provide efficient end-to-end communication. This, along with the diverse application of these networks in many different scenarios such as battlefield and disaster recovery, have seen MANETs being researched by many different organisations and institutes. MANETs employ the traditional TCP/IP structure to provide end-to-end communication between nodes. However, due to their mobility and the limited resource in wireless networks, each layer in the TCP/IP model require redefinition or modifications to function efficiently in MANETs. One interesting research area in MANET is routing. Routing in the MANETs is a challenging task and has received a tremendous amount of attention from researches. This has led to development of many different routing protocols for MANETs, and each author of each proposed protocol argues that the strategy proposed provides an improvement over a number of different strategies considered in the literature for a given network scenario. Therefore, it is quite difficult to determine which protocols may perform best under a number of different network scenarios, such as increasing node density and traffic. In this paper, we provide an overview of a wide range of routing protocols proposed in the literature. We also provide a performance comparison of all routing protocols and suggest which protocols may perform best in large networks. 2003 Elsevier B.V. All rights reserved.", 
            "authors": [
                "Mehran Abolhasan", 
                "Tadeusz A Wysocki", 
                "Eryk Dutkiewicz"
            ], 
            "fields": [
                "Computer network", 
                "Disaster recovery", 
                "Wireless network", 
                "Communications protocol", 
                "Routing"
            ], 
            "title": "A review of routing protocols for mobile ad hoc networks", 
            "url": "http://cn.bing.com/academic/profile?id=48b89e2e540c357d64e345a92945b48a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "IP multicast offers the scalable point-to-multipoint delivery necessary for using group communication applications on the Internet. However, the IP multicast service has seen slow commercial deployment by ISPs and carriers. The original service model was designed without a clear understanding of commercial requirements or a robust implementation strategy. The very limited number of applications and the complexity of the architectural design-which we believe is a consequence of the open service model-have deterred widespread deployment as well. We examine the issues that have limited the commercial deployment of IP multicast from the viewpoint of carriers. We analyze where the model fails and what it does not offer, and we discuss requirements for successful deployment of multicast services.", 
            "authors": [
                "Christophe Diot", 
                "Brian Neil Levine", 
                "B Lyles", 
                "H Kassem", 
                "D Balensiefen"
            ], 
            "fields": [
                "IP multicast", 
                "The Internet", 
                "Communication in small groups", 
                "Service-oriented modeling"
            ], 
            "title": "Deployment issues for the IP multicast service and architecture", 
            "url": "http://cn.bing.com/academic/profile?id=82b8b034917134842ef2e4557dd9e84e&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The world-wide web forms a large directed graph, whose vertices are documents and edges are links pointing from one document to another. Here we demonstrate that despite its apparent random character, the topology of this graph has a number of universal scale-free characteristics. We introduce a model that leads to a scale-free network, capturing in a minimal fashion the self-organization processes governing the world-wide web.", 
            "authors": [
                "Albertlaszlo Barabasi", 
                "Reka Albert", 
                "Hawoong Jeong"
            ], 
            "fields": [
                "Complexity", 
                "Directed graph", 
                "World Wide Web", 
                "Scaling", 
                "Scale-free network"
            ], 
            "title": "Scale-free characteristics of random networks: the topology of the world-wide web", 
            "url": "http://cn.bing.com/academic/profile?id=4e1f2a1f6bc88b4a6da6c14f8933b80c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The Smart Dust project is probing microfabrication technology's limitations to determine whether an autonomous sensing, computing, and communication system can be packed into a cubic millimeter mote (a small particle or speck) to form the basis of integrated, massively distributed sensor networks. Although we've chosen a somewhat arbitrary size for our sensor systems, exploring microfabrication technology's limitations is our fundamental goal. Because of its discrete size, substantial functionality, connectivity, and anticipated low cost, Smart Dust will facilitate innovative methods of interacting with the environment, providing more information from more places less intrusively.", 
            "authors": [
                "B Warneke", 
                "B Liebowitz", 
                "K S J Pister"
            ], 
            "fields": [
                "Intelligent sensor", 
                "Distributed computing", 
                "Energy management", 
                "Computer architecture", 
                "Ubiquitous computing"
            ], 
            "title": "Smart Dust: communicating with a cubic-millimeter computer", 
            "url": "http://cn.bing.com/academic/profile?id=4dc45e7469945aeada5408364f993298&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: Many systems, ranging from engineering to medical to societal, can only be properly characterized by multiple interdependent networks whose normal functioning depends on one another. Failure of a fraction of nodes in one network may lead to a failure in another network. This in turn may cause further malfunction of additional nodes in the first network and so on. Such a cascade of failures, triggered by a failure of a small faction of nodes in only one network, may lead to the complete fragmentation of all networks. We introduce a model and an analytical framework for studying interdependent networks. We obtain interesting and surprising results that should significantly effect the design of robust real-world networks. For two interdependent Erdos-Renyi (ER) networks, we find that the critical average degree below which both networks collapse is  =2.445, compared to  =1 for a single ER network. Furthermore, while for a single network a broader degree distribution of the network nodes results in higher robustness to random failure, for interdependent networks, the broader the distribution is, the more vulnerable the networks become to random failure.", 
            "authors": [
                "Sergey V Buldyrev", 
                "Roni Parshani", 
                "Gerald Paul", 
                "H Eugene Stanley", 
                "Shlomo Havlin"
            ], 
            "fields": [
                "Pharmacology", 
                "Medical research", 
                "Cell cycle", 
                "Astrophysics", 
                "Structural biology"
            ], 
            "title": "Catastrophic cascade of failures in interdependent networks", 
            "url": "http://cn.bing.com/academic/profile?id=355b117133a702b5f7f662d19f4e5137&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.  This paper introduces a new model of memory consistency, called  release consistency , that allows for more buffering and pipelining than previously proposed models. A framework for classifying shared accesses and reasoning about event ordering is developed. The release consistency model is shown to be equivalent to the sequential consistency model for parallel programs with sufficient synchronization. Possible performance gains from the less strict constraints of the release consistency model are explored. Finally, practical implementation issues are discussed, concentrating on issues relevant to scalable architectures.", 
            "authors": [
                "Kourosh Gharachorloo", 
                "Daniel E Lenoski", 
                "James P Laudon", 
                "Phillip B Gibbons", 
                "Anoop Gupta", 
                "John Hennessy"
            ], 
            "fields": [
                "Parallel computing", 
                "Weak ordering", 
                "Distributed computing", 
                "Out-of-order execution", 
                "Distributed memory"
            ], 
            "title": "Memory consistency and event ordering in scalable shared-memory multiprocessors", 
            "url": "http://cn.bing.com/academic/profile?id=e5e1eb2a23710949acbf0a328a351b31&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Large networks, having thousands of vertices and lines, can be found in many different areas, e. g: genealogies, flo w graphs of programs, molecule, computer networks, transportation networks, social networks, intra/inter organisational networks ... Many standard network algorithms are very time and space consuming and therefore unsuitable for analysis of such networks. In the article we present some approaches to analysis and visualisation of large networks implemented in programPajek. Some typical examples are also given.", 
            "authors": [
                "Vladimir Batagelj", 
                "Andrej Mrvar"
            ], 
            "fields": [
                "Computer network", 
                "Social network", 
                "Network analysis"
            ], 
            "title": "Pajek - Program for Large Network Analysis", 
            "url": "http://cn.bing.com/academic/profile?id=ae97bbc52dc8eb50567885156695e92f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The n-dimensional hypercube is a highly concurrent loosely coupled multiprocessor based on the binary n-cube topology. Machines based on the hypercube topology have been advocated as ideal parallel architectures for their powerful interconnection features. The authors examine the hypercube from the graph-theory point of view and consider those features that make its connectivity so appealing. Among other things, they propose a theoretical characterization of the n-cube as a graph and and show how to map various other topologies into a hypercube. >", 
            "authors": [
                "Youcef Saad", 
                "Martin H Schultz"
            ], 
            "fields": [
                "Geometry", 
                "Topology", 
                "Fault tolerance", 
                "Very-large-scale integration", 
                "Network topology"
            ], 
            "title": "Topological properties of hypercubes", 
            "url": "http://cn.bing.com/academic/profile?id=1d02f5b874a5bd613a67a8c98c1f978a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Network throughput can be increased by dividing the buffer storage associated with each network channel into several virtual channels. Each physical channel is associated with several small queues, virtual channels, rather than a single deep queue. The virtual channels associated with one physical channel are allocated independently but compete with each other for physical bandwidth. Virtual channels decouple buffer resources from transmission resources. This decoupling allows active messages to pass blocked messages using network bandwidth that would otherwise be left idle. The paper studies the performance of networks using virtual channels using both analysis and simulation. These studies show that virtual channels increase network throughput, by a factor of four for 10-stage networks, and reduce the dependence of throughput on the depth of the network. >", 
            "authors": [
                "W J Dally"
            ], 
            "fields": [
                "Bandwidth", 
                "Simulation", 
                "Resource management", 
                "Network topology", 
                "Flow control"
            ], 
            "title": "Virtual-channel flow control", 
            "url": "http://cn.bing.com/academic/profile?id=7f14b918924da2d704c40301b2a55353&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Cyber-Physical Systems (CPS) are integrations of computation and physical processes. Embedded computers and networks monitor and control the physical processes, usually with feedback loops where physical processes affect computations and vice versa. The economic and societal potential of such systems is vastly greater than what has been realized, and major investments are being made worldwide to develop the technology. There are considerable challenges, particularly because the physical components of such systems introduce safety and reliability requirements qualitatively different from those in general- purpose computing. Moreover, physical components are qualitatively different from object-oriented software components. Standard abstractions based on method calls and threads do not work. This paper examines the challenges in designing such systems, and in particular raises the question of whether today's computing and networking technologies provide an adequate foundation for CPS. It concludes that it will not be sufficient to improve design processes, raise the level of abstraction, or verify (formally or otherwise) designs that are built on today's abstractions. To realize the full potential of CPS, we will have to rebuild computing and networking abstractions. These abstractions will have to embrace physical dynamics and computation in a unified way.", 
            "authors": [
                "Edward A Lee"
            ], 
            "fields": [
                "Process control", 
                "Affective computing", 
                "Feedback", 
                "Computer network", 
                "Systems analysis"
            ], 
            "title": "Cyber Physical Systems: Design Challenges", 
            "url": "http://cn.bing.com/academic/profile?id=f33f5914acbb3be4191202e273bc342f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Networks of workstations are poised to become the primary computing infrastructure for science and engineering. NOWs may dramatically improve virtual memory and file system performance; achieve cheap, highly available, and scalable file storage: and provide multiple CPUs for parallel computing. Hurdles that remain include efficient communication hardware and software, global coordination of multiple workstation operating systems, and enterprise-scale network file systems. Our 100-node NOW prototype aims to demonstrate practical solutions to these challenges. >", 
            "authors": [
                "Thomas Anderson", 
                "David E Culler", 
                "D Patterson"
            ], 
            "fields": [
                "Local area network", 
                "Virtual memory", 
                "Workstation", 
                "Parallel processing", 
                "Availability"
            ], 
            "title": "A case for NOW (Networks of Workstations)", 
            "url": "http://cn.bing.com/academic/profile?id=828246115491a31058033d59057e24e6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We study data transfer opportunities between wireless devices carried by humans. We observe that the distribution of the intercontact time (the time gap separating two contacts between the same pair of devices) may be well approximated by a power law over the range [10 minutes; 1 day]. This observation is confirmed using eight distinct experimental data sets. It is at odds with the exponential decay implied by the most commonly used mobility models. In this paper, we study how this newly uncovered characteristic of human mobility impacts one class of forwarding algorithms previously proposed. We use a simplified model based on the renewal theory to study how the parameters of the distribution impact the performance in terms of the delivery delay of these algorithms. We make recommendations for the design of well-founded opportunistic forwarding algorithms in the context of human-carried devices", 
            "authors": [
                "Augustin Chaintreau", 
                "Pan Hu", 
                "Jon Crowcroft", 
                "Christophe Diot", 
                "Richard Gass", 
                "James F Scott"
            ], 
            "fields": [
                "Mobile telephony", 
                "Mobile computing", 
                "Routing", 
                "Information and Communications Technology", 
                "Renewal theory"
            ], 
            "title": "Impact of Human Mobility on Opportunistic Forwarding Algorithms", 
            "url": "http://cn.bing.com/academic/profile?id=bc91c22a49105fb03ecb55c098480cd1&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "There is often the need to update an installed intrusion detection system (IDS) due to new attack methods or upgraded computing environments. Since many current IDSs are constructed by manual encoding of expert knowledge, changes to IDSs are expensive and slow. We describe a data mining framework for adaptively building Intrusion Detection (ID) models. The central idea is to utilize auditing programs to extract an extensive set of features that describe each network connection or host session, and apply data mining programs to learn rules that accurately capture the behavior of intrusions and normal activities. These rules can then be used for misuse detection and anomaly detection. New detection models are incorporated into an existing IDS through a meta-learning (or co-operative learning) process, which produces a meta detection model that combines evidence from multiple models. We discuss the strengths of our data mining programs, namely, classification, meta-learning, association rules, and frequent episodes. We report on the results of applying these programs to the extensively gathered network audit data for the 1998 DARPA Intrusion Detection Evaluation Program.", 
            "authors": [
                "Wenke Lee", 
                "S J Stolfo", 
                "Kui W Mok"
            ], 
            "fields": [
                "Computer Science", 
                "Anomaly detection", 
                "Audit", 
                "Expert system", 
                "Intrusion detection system"
            ], 
            "title": "A data mining framework for building intrusion detection models", 
            "url": "http://cn.bing.com/academic/profile?id=0bfdacea4f49ebd774e67760fed3e9aa&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "As the recent denial-of-service attacks on several major Internet sites have shown us, no open computer network is immune from intrusions. The wireless ad-hoc network is particularly vulnerable due to its features of open medium, dynamic changing topology, cooperative algorithms, lack of centralized monitoring and management point, and lack of a clear line of defense. Many of the intrusion detection techniques developed on a fixed wired network are not applicable in this new environment. How to do it differently and effectively is a challenging research problem. In this paper, we first examine the vulnerabilities of a wireless ad-hoc network, the reason why we need intrusion detection, and the reason why the current methods cannot be applied directly. We then describe the new intrusion detection and response mechanisms that we are developing for wireless ad-hoc networks.", 
            "authors": [
                "Yongguang Zhang", 
                "Wenke Lee"
            ], 
            "fields": [
                "Denial-of-service attack", 
                "Web application", 
                "Intrusion detection system", 
                "Computer network", 
                "Evaluation"
            ], 
            "title": "Intrusion detection in wireless ad-hoc networks", 
            "url": "http://cn.bing.com/academic/profile?id=ede8917b3c45a632f60d472a31f20041&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Crohn's disease and ulcerative colitis, the two common forms of inflammatory bowel disease (IBD), affect over 2.5 million people of European ancestry, with rising prevalence in other populations. Genome-wide association studies and subsequent meta-analyses of these two diseases as separate phenotypes have implicated previously unsuspected mechanisms, such as autophagy, in their pathogenesis and showed that some IBD loci are shared with other inflammatory diseases. Here we expand on the knowledge of relevant pathways by undertaking a meta-analysis of Crohn's disease and ulcerative colitis genome-wide association scans, followed by extensive validation of significant findings, with a combined total of more than 75,000 cases and controls. We identify 71 new associations, for a total of 163 IBD loci, that meet genome-wide significance thresholds. Most loci contribute to both phenotypes, and both directional (consistently favouring one allele over the course of human history) and balancing (favouring the retention of both alleles within populations) selection effects are evident. Many IBD loci are also implicated in other immune-mediated disorders, most notably with ankylosing spondylitis and psoriasis. We also observe considerable overlap between susceptibility loci for IBD and mycobacterial infection. Gene co-expression network analysis emphasizes this relationship, with pathways shared between host responses to mycobacteria and those predisposing to IBD.", 
            "authors": [
                "Luke Jostins", 
                "Stephan Ripke", 
                "Rinse K Weersma", 
                "Richard H Duerr", 
                "Dermot P Mcgovern", 
                "Ken Y Hui", 
                "James C Lee", 
                "L Philip Schumm", 
                "Yashoda Sharma", 
                "Carl A Anderson", 
                "Jonah Essers", 
                "Mitja Mitrovic", 
                "Kaida Ning", 
                "Isabelle Cleynen", 
                "Emilie Theatre", 
                "Sarah Spain", 
                "Soumya Raychaudhuri", 
                "Philippe Goyette", 
                "Zhi Wei", 
                "Clara \u2026", 
                ""
            ], 
            "fields": [
                "Genetics", 
                "Genome-wide association study", 
                "Meta-analysis", 
                "Polymorphism", 
                "Nature"
            ], 
            "title": "Host-microbe interactions have shaped the genetic architecture of inflammatory bowel disease", 
            "url": "http://cn.bing.com/academic/profile?id=57ab2c721c56549d3c9b5c68353fdcc9&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In this paper, we introduce PVL, an algorithm for computing the Pade approximation of Laplace-domain transfer functions of large linear networks via a Lanczos process. The PVL algorithm has significantly superior numerical stability, while retaining the same efficiency as algorithms that compute the Pade approximation directly through moment matching, such as AWE and its derivatives. As a consequence, it produces more accurate and higher-order approximations, and it renders unnecessary many of the heuristics that AWE and its derivatives had to employ. The algorithm also computes an error bound that permits to identify the true poles and zeros of the original network. We present results of numerical experiments with the PVL algorithm for several large examples. >", 
            "authors": [
                "Peter Feldmann", 
                "Roland W Freund"
            ], 
            "fields": [
                "Network analysis", 
                "Computer Aided Design", 
                "Pad\u00e9 approximant", 
                "Transfer function", 
                "Pole\u2013zero plot"
            ], 
            "title": "Efficient linear circuit analysis by Pade approximation via the Lanczos process", 
            "url": "http://cn.bing.com/academic/profile?id=dd05fc5b5e478067f927497557e83227&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper will present a novel concept of expected values of fuzzy variables, which is essentially a type of Choquet integral and coincides with that of random variables. In order to calculate the expected value of general fuzzy variable, a fuzzy simulation technique is also designed. Finally, we construct a spectrum of fuzzy expected value models, and integrate fuzzy simulation, neural network, and genetic algorithms to produce a hybrid intelligent algorithm for solving general fuzzy expected value models.", 
            "authors": [
                "Baoding Liu", 
                "Yiankui Liu"
            ], 
            "fields": [
                "Fuzzy set", 
                "Random variable", 
                "Computer network", 
                "Computational model", 
                "Artificial neural network"
            ], 
            "title": "Expected value of fuzzy variable and fuzzy expected value models", 
            "url": "http://cn.bing.com/academic/profile?id=1fb87df2f41af3070141cf3efebb4a75&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Jaynes's principle of maximum entropy and Kullbacks principle of minimum cross-entropy (minimum directed divergence) are shown to be uniquely correct methods for inductive inference when new information is given in the form of expected values. Previous justifications use intuitive arguments and rely on the properties of entropy and cross-entropy as information measures. The approach here assumes that reasonable methods of inductive inference should lead to consistent results when there are different ways of taking the same information into account (for example, in different coordinate system). This requirement is formalized as four consistency axioms. These are stated in terms of an abstract information operator and make no reference to information measures. It is proved that the principle of maximum entropy is correct in the following sense: maximizing any function but entropy will lead to inconsistency unless that function and entropy have identical maxima. In other words given information in the form of constraints on expected values, there is only one (distribution satisfying the constraints that can be chosen by a procedure that satisfies the consistency axioms; this unique distribution can be obtained by maximizing entropy. This result is established both directly and as a special case (uniform priors) of an analogous result for the principle of minimum cross-entropy. Results are obtained both for continuous probability densities and for discrete distributions.", 
            "authors": [
                "John E Shore", 
                "Rodney W Johnson"
            ], 
            "fields": [
                "Mathematical analysis", 
                "Pattern recognition", 
                "Systems modeling", 
                "Probability distribution", 
                "Computer network"
            ], 
            "title": "Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy", 
            "url": "http://cn.bing.com/academic/profile?id=c1e700232932ee369bd947a762c68f80&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In this paper, we present a real-time communication protocol for sensor networks, called SPEED. The protocol provides three types of real-time communication services, namely, real-time unicast, real-time area-multicast and real-time area-anycast. SPEED is specifically tailored to be a stateless, localized algorithm with minimal control overhead End-to-end soft real-time communication is achieved by maintaining a desired delivery speed across the sensor network through a novel combination of feedback control and non-deterministic geographic forwarding. SPEED is a highly efficient and scalable protocol for sensor networks where the resources of each node are scarce. Theoretical analysis, simulation experiments and a real implementation on Berkeley motes are provided to validate our claims.", 
            "authors": [
                "Tian He", 
                "John A Stankovic", 
                "Chenyang Lu", 
                "Tarek Abdelzaher"
            ], 
            "fields": [
                "Stateless protocol", 
                "Computer network", 
                "Quality of service", 
                "Base station", 
                "Wireless sensor network"
            ], 
            "title": "SPEED: a stateless protocol for real-time communication in sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=04d2efb79fd0f39e26944809db4897e7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Wireless sensor network localization is an important area that attracted significant research interest. This interest is expected to grow further with the proliferation of wireless sensor network applications. This paper provides an overview of the measurement techniques in sensor network localization and the one-hop localization algorithms based on these measurements. A detailed investigation on multi-hop connectivity-based and distance-based localization algorithms are presented. A list of open research problems in the area of distance-based sensor network localization is provided with discussion on possible approaches to them.", 
            "authors": [
                "Guoqiang Mao", 
                "Baris Fidan", 
                "Brian D O Anderson"
            ], 
            "fields": [
                "Angle of attack", 
                "Sensor array", 
                "Wireless sensor network", 
                "Estimation theory", 
                "Algorithm"
            ], 
            "title": "Wireless sensor network localization techniques", 
            "url": "http://cn.bing.com/academic/profile?id=61c48240f3659bdbd770294cd5ac450a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We propose using coordinates-based mechanisms in a peer-to-peer architecture to predict Internet network distance (i.e. round-trip propagation and transmission delay). We study two mechanisms. The first is a previously proposed scheme, called the triangulated heuristic, which is based on relative coordinates that are simply the distances from a host to some special network nodes. We propose the second mechanism, called global network positioning (GNP), which is based on absolute coordinates computed from modeling the Internet as a geometric space. Since end hosts maintain their own coordinates, these approaches allow end hosts to compute their inter-host distances as soon as they discover each other. Moreover, coordinates are very efficient in summarizing inter-host distances, making these approaches very scalable. By performing experiments using measured Internet distance data, we show that both coordinates-based schemes are more accurate than the existing state of the art system IDMaps, and the GNP approach achieves the highest accuracy and robustness among them.", 
            "authors": [
                "T S E Ng", 
                "Hui Zhang"
            ], 
            "fields": [
                "Solid modeling", 
                "Propagation delay", 
                "Computer network", 
                "Robustness", 
                "Economic indicator"
            ], 
            "title": "Predicting Internet network distance with coordinates-based approaches", 
            "url": "http://cn.bing.com/academic/profile?id=498ef8360a5a853473e408894f8446dd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The growing interest in mobile ad hoc network techniques has resulted in many routing protocol proposals. Scalability issues in ad hoc networks are attracting increasing attention these days. We survey the routing protocols that address scalability. The routing protocols included in the survey fall into three categories: flat routing protocols; hierarchical routing approaches; GPS augmented geographical routing schemes. The article compares the scalability properties and operational features of the protocols and discusses challenges in future routing protocol designs.", 
            "authors": [
                "Xiaoyan Hong", 
                "Kaixin Xu", 
                "Mario Gerla"
            ], 
            "fields": [
                "Wireless ad hoc network", 
                "Routing protocol", 
                "Global Positioning System", 
                "Mobile ad hoc network", 
                "Communications protocol"
            ], 
            "title": "Scalable routing protocols for mobile ad hoc networks", 
            "url": "http://cn.bing.com/academic/profile?id=8af57a5951a927fa62163ef39d7c9588&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "\u25aa Abstract\u2002We explore the extent to which neocortical circuits generalize, i.e., to what extent can neocortical neurons and the circuits they form be considered as canonical? We find that, as has long been suspected by cortical neuroanatomists, the same basic laminar and tangential organization of the excitatory neurons of the neocortex is evident wherever it has been sought. Similarly, the inhibitory neurons show characteristic morphology and patterns of connections throughout the neocortex. We offer a simple model of cortical processing that is consistent with the major features of cortical circuits: The superficial layer neurons within local patches of cortex, and within areas, cooperate to explore all possible interpretations of different cortical input and cooperatively select an interpretation consistent with their various cortical and subcortical inputs.", 
            "authors": [
                "Rodney J Douglas", 
                "Kevan A C Martin"
            ], 
            "fields": [
                "Excitation", 
                "Artificial neural network", 
                "Computer network", 
                "Central nervous system", 
                "Computation"
            ], 
            "title": "NEURONAL CIRCUITS OF THE NEOCORTEX", 
            "url": "http://cn.bing.com/academic/profile?id=0ad712824c17dc81c232bae4cefd2c0b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The distributed brain systems associated with performance of a verbal fluency task were identified in a nondirected correlational analysis of neurophysiological data obtained with positron tomography. This analysis used a recursive principal-component analysis developed specifically for large data sets. This analysis is interpreted in terms of functional connectivity, defined as the temporal correlation of a neurophysiological index measured in different brain areas. The results suggest that the variance in neurophysiological measurements, introduced experimentally, was accounted for by two independent principal components. The first, and considerably larger, highlighted an intentional brain system seen in previous studies of verbal fluency. The second identified a distributed brain system including the anterior cingulate and Wernicke's area that reflected monotonic time effects. We propose that this system has an attentional bias.", 
            "authors": [
                "K J Friston", 
                "C D Frith", 
                "Peter F Liddle", 
                "R S J Frackowiak"
            ], 
            "fields": [
                "Neurovascular bundle", 
                "Cerebral circulation", 
                "Central nervous system", 
                "Principal component analysis", 
                "Oscillation"
            ], 
            "title": "Functional Connectivity: The Principal-Component Analysis of Large (PET) Data Sets", 
            "url": "http://cn.bing.com/academic/profile?id=1e50157859a11090ab194d8a5c426245&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The chemical contamination of water from a wide range of toxic derivatives, in particular heavy metals, aromatic molecules and dyes, is a serious environmental problem owing to their potential human toxicity. Therefore, there is a need to develop technologies that can remove toxic pollutants found in wastewaters. Among all the treatments proposed, adsorption is one of the more popular methods for the removal of pollutants from the wastewater. Adsorption is a procedure of choice for treating industrial effluents, and a useful tool for protecting the environment. In particular, adsorption on natural polymers and their derivatives are known to remove pollutants from water. The increasing number of publications on adsorption of toxic compounds by modified polysaccharides shows that there is a recent increasing interest in the synthesis of new low-cost adsorbents used in wastewater treatment. The present review shows the recent developments in the synthesis of adsorbents containing polysaccharides, in particular modified biopolymers derived from chitin, chitosan, starch and cyclodextrin. New polysaccharide based-materials are described and their advantages for the removal of pollutants from the wastewater discussed. The main objective of this review is to provide recent information about the most important features of these polymeric materials and to show the advantages gained from the use of adsorbents containing modified biopolymers in waste water treatment.", 
            "authors": [
                "Gregorio Crini"
            ], 
            "fields": [
                "Grafting", 
                "Polysaccharide", 
                "Pollutant", 
                "Sewage treatment", 
                "Computer network"
            ], 
            "title": "Recent developments in polysaccharide-based materials used as adsorbents in wastewater treatment", 
            "url": "http://cn.bing.com/academic/profile?id=16b52d0d82beb70b67b997be220b8641&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The large-scale behavior of routing In the Internet has gone virtually without any formal study, the exceptions being Chinoy's (1993) analysis of the dynamics of Internet routing information, and work, similar in spirit, by Labovitz, Malan, and Jahanian (see Proc. SIGCOMM'97, 1997). We report on an analysis of 40000 end-to-end route measurements conducted using repeated \"traceroutes\" between 37 Internet sites. We analyze the routing behavior for pathological conditions, routing stability, and routing symmetry. For pathologies, we characterize the prevalence of routing loops, erroneous routing, infrastructure failures, and temporary outages. We find that the likelihood of encountering a major routing pathology more than doubled between the end of 1994 and the end of 1995, rising from 1.5% to 3.3%. For routing stability, we define two separate types of stability, \"prevalence\", meaning the overall likelihood that a particular route is encountered, and \"persistence\", the likelihood that a route remains unchanged over a long period of time. We find that Internet paths are heavily dominated by a single prevalent route, but that the time periods over which routes persist show wide variation, ranging from seconds up to days. About two-thirds of the Internet paths had routes persisting for either days or weeks. For routing symmetry, we look at the likelihood that a path through the Internet visits at least one different city in the two directions. At the end of 1995, this was the case half the time, and at least one different autonomous system was visited 30% of the time.", 
            "authors": [
                "Vern Paxson"
            ], 
            "fields": [
                "Failure analysis", 
                "Stability", 
                "Von Neumann stability analysis", 
                "Prevalence", 
                "Data analysis"
            ], 
            "title": "End-to-end routing behavior in the Internet", 
            "url": "http://cn.bing.com/academic/profile?id=6ca545403619f415ddc515c6504002db&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The design and implementation of Coda, a file system for a large-scale distributed computing environment composed of Unix workstations, is described. It provides resiliency to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, stores copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. The design of Coda optimizes for availability and performance and strives to provide the highest degree of consistency attainable in the light of these objectives. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. >", 
            "authors": [
                "Mahadev Satyanarayanan", 
                "James J Kistler", 
                "Praveen Kumar", 
                "Maria E Okasaki", 
                "Ellen H Siegel", 
                "David Steere"
            ], 
            "fields": [
                "Availability", 
                "File server", 
                "Coda", 
                "Computer network", 
                "Workstation"
            ], 
            "title": "Coda: a highly available file system for a distributed workstation environment", 
            "url": "http://cn.bing.com/academic/profile?id=e3915f901bc4a442cfab7f839e3c5792&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In a packet network, the terms bandwidth and throughput often characterize the amount of data that the network can transfer per unit of time. Bandwidth estimation is of interest to users wishing to optimize end-to-end transport performance, overlay network routing, and peer-to-peer file distribution. Techniques for accurate bandwidth estimation are also important for traffic engineering and capacity planning support. Existing bandwidth estimation tools measure one or more of three related metrics: capacity, available bandwidth, and bulk transfer capacity. Currently available bandwidth estimation tools employ a variety of strategies to measure these metrics. In this survey we review the recent bandwidth estimation literature focusing on underlying techniques and methodologies as well as open source bandwidth measurement tools.", 
            "authors": [
                "Ravi S Prasad", 
                "Constantinos Dovrolis", 
                "Margaret Murray", 
                "Kc Claffy"
            ], 
            "fields": [
                "Packet switching", 
                "Bandwidth allocation", 
                "Routing protocol", 
                "Channel capacity", 
                "Public domain software"
            ], 
            "title": "Bandwidth estimation: metrics, measurement techniques, and tools", 
            "url": "http://cn.bing.com/academic/profile?id=e3957696a15d1bc79ce134c3f8d5339b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Although individual use of computers is fairly widespread, in meetings we tend to leave them behind. At Xerox PARC, an experimental meeting room called the Colab has been created to study computer support of collaborative problem solving in face-to-face meetings. The long-term goal is to understand how to build computer tools to make meetings more effective.", 
            "authors": [
                "Mark Stefik", 
                "Gregg Foster", 
                "Daniel G Bobrow", 
                "Kenneth M Kahn", 
                "Stan Lanning", 
                "Lucy Suchman"
            ], 
            "fields": [
                "Local area network", 
                "Collaboration", 
                "Distributed computing", 
                "User interface", 
                "Computer network"
            ], 
            "title": "Beyond the chalkboard: computer support for collaboration and problem solving in meetings", 
            "url": "http://cn.bing.com/academic/profile?id=41fe42f2b720ee8dc517bdc110b058ab&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The emergence of a feature-analyzing function from the development rules of simple, multilayered networks is explored. It is shown that even a single developing cell of a layered network exhibits a remarkable set of optimization properties that are closely related to issues in statistics, theoretical physics, adaptive signal processing, the formation of knowledge representation in artificial intelligence, and information theory. The network studied is based on the visual system. These results are used to infer an information-theoretic principle that can be applied to the network as a whole, rather than a single cell. The organizing principle proposed is that the network connections develop in such a way as to maximize the amount of information that is preserved when signals are transformed at each processing stage, subject to certain constraints. The operation of this principle is illustrated for some simple cases. >", 
            "authors": [
                "Ralph Linsker"
            ], 
            "fields": [
                "Data processing", 
                "Artificial intelligence", 
                "Visual system", 
                "Electronic circuit", 
                "Communication"
            ], 
            "title": "Self-organization in a perceptual network", 
            "url": "http://cn.bing.com/academic/profile?id=38495ef14cda45c9d1978ad21ca9fc72&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Distributed systems for locating people and equipment will be at the heart of tomorrow's active offices. Computer and communications systems continue to proliferate in the office and home. Systems are varied and complex, involving wireless networks and mobile computers. However, systems are underused because the choices of control mechanisms and application interfaces are too diverse. It is therefore pertinent to consider which mechanisms might allow the user to manipulate systems in simple and ubiquitous ways, and how computers can be made more aware of the facilities in their surroundings. Knowledge of the location of people and equipment within an organization is such a mechanism. Annotating a resource database with location information allows location-based heuristics for control and interaction to be constructed. This approach is particularly attractive because location techniques can be devised that are physically unobtrusive and do not rely on explicit user action. The article describes the technology of a system for locating people and equipment, and the design of a distributed system service supporting access to that information. The application interfaces made possible by or that benefit from this facility are presented. >", 
            "authors": [
                "A C Harter", 
                "Andy Hopper"
            ], 
            "fields": [
                "Mobile computing", 
                "Distributed database", 
                "Distributed computing", 
                "Wireless network", 
                "Network interface"
            ], 
            "title": "A distributed location system for the active office", 
            "url": "http://cn.bing.com/academic/profile?id=1343eff4b3ab0da0b850219698cb505b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Low-power integration of sensing, communication, and computation requires a new approach to wireless design. Flexible interfaces and primitive accelerators enable aggressive system-level optimizations. Mica's flexible design serves as a building block for creating efficient application-specific protocols. Instead of defining narrow, standardized application interfaces, Mica provides a set of richly interconnected primitives (such as data serializers and timing extractors) to facilitate cross-layer optimizations. To explore novel systems approaches, researchers can develop customized protocols tailored to their application; Mica does not require use of predefined protocols.", 
            "authors": [
                "Jason L Hill", 
                "David E Culler"
            ], 
            "fields": [
                "Computer network", 
                "Embedded system", 
                "Network interface", 
                "Sensor"
            ], 
            "title": "Mica: a wireless platform for deeply embedded networks", 
            "url": "http://cn.bing.com/academic/profile?id=9163c3414a08aeeac34cf5b191b3716d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: The description of the Internet topology is an important open problem, recently tackled with the introduction of scale-free networks. In this paper we focus on the topological and dynamical properties of real Internet maps in a three years time interval. We study higher order correlation functions as well as the dynamics of several quantities. We find that the Internet is characterized by non-trivial correlations among nodes and different dynamical regimes. We point out the importance of node hierarchy and aging in the Internet structure and growth. Our results provide hints towards the realistic modeling of the Internet evolution.", 
            "authors": [
                "Romualdo Pastorsatorras", 
                "Alexei Vazquez", 
                "Alessandro Vespignani"
            ], 
            "fields": [
                "Internet topology", 
                "Computer network", 
                "Physics", 
                "Network mapping", 
                "Correlation function"
            ], 
            "title": "Dynamical and correlation properties of the Internet", 
            "url": "http://cn.bing.com/academic/profile?id=e2fa289624590bb8c190a3032d0aa5fd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Most ad hoc mobile devices today operate on batteries. Hence, power consumption becomes an important issue. To maximize the lifetime of ad hoc mobile networks, the power consumption rate of each node must be evenly distributed, and the overall transmission power for each connection request must be minimized. These two objectives cannot be satisfied simultaneously by employing routing algorithms proposed in previous work. We present a new power-aware routing protocol to satisfy these two constraints simultaneously; we also compare the performance of different types of power-related routing algorithms via simulation. Simulation results confirm the need to strike a balance in attaining service availability performance of the whole network vs. the lifetime of ad hoc mobile devices.", 
            "authors": [
                "C K Toh"
            ], 
            "fields": [
                "Mobile device", 
                "Satisfiability", 
                "Wireless ad hoc network", 
                "Wireless network", 
                "Mobile computing"
            ], 
            "title": "Maximum battery life routing to support ubiquitous mobile computing in wireless ad hoc networks", 
            "url": "http://cn.bing.com/academic/profile?id=f6495591fe6644d1d99b866c893b7bcd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A \u201cmajority consensus\u201d algorithm which represents a new solution to the update synchronization problem for multiple copy databases is presented. The algorithm embodies distributed control and can function effectively in the presence of communication and database site outages. The correctness of the algorithm is demonstrated and the cost of using it is analyzed. Several examples that illustrate aspects of the algorithm operation are included in the Appendix.", 
            "authors": [
                "Robert H Thomas"
            ], 
            "fields": [
                "Clock synchronization", 
                "Distributed database", 
                "Distributed computing", 
                "Computer network", 
                "Computation"
            ], 
            "title": "A Majority consensus approach to concurrency control for multiple copy databases", 
            "url": "http://cn.bing.com/academic/profile?id=03ca8eac3c022acc7db0e05599fffdc7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: Game theory is one of the key paradigms behind many scientific disciplines from biology to behavioral sciences to economics. In its evolutionary form and especially when the interacting agents are linked in a specific social network the underlying solution concepts and methods are very similar to those applied in non-equilibrium statistical physics. This review gives a tutorial-type overview of the field for physicists. The first three sections introduce the necessary background in classical and evolutionary game theory from the basic definitions to the most important results. The fourth section surveys the topological complications implied by non-mean-field-type social network structures in general. The last three sections discuss in detail the dynamic behavior of three prominent classes of models: the Prisoner's Dilemma, the Rock-Scissors-Paper game, and Competing Associations. The major theme of the review is in what sense and how the graph structure of interactions can modify and enrich the picture of long term behavioral patterns emerging in evolutionary games.", 
            "authors": [
                "Gyorgy Szabo", 
                "Gabor Fath"
            ], 
            "fields": [
                "Evolutionary game theory", 
                "Dynamics", 
                "Behavioural sciences", 
                "Computer network", 
                "Prisoner's dilemma"
            ], 
            "title": "Evolutionary games on graphs", 
            "url": "http://cn.bing.com/academic/profile?id=55a91de8bd5a4af6eeed437f1862d6c8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A description is given of Coda, a file system for a large-scale distributed computing environment composed of Unix workstations. It provides resilience to server and network failures through the use of two distinct but complementary mechanisms. One mechanism, server replication, involves storing copies of a file at multiple servers. The other mechanism, disconnected operation, is a mode of execution in which a caching site temporarily assumes the role of a replication site. Disconnected operation is particularly useful for supporting portable workstations. The design of Coda optimizes availability and performance and provides the highest degree of consistency attainable. Measurements from a prototype show that the performance cost of providing high availability in Coda is reasonable. >", 
            "authors": [
                "Mahadev Satyanarayanan"
            ], 
            "fields": [
                "High availability", 
                "Multidisciplinary design optimization", 
                "Workstation", 
                "Psychological resilience", 
                "Coda"
            ], 
            "title": "Coda: a highly available file system for a distributed workstation environment", 
            "url": "http://cn.bing.com/academic/profile?id=85423b0a9bd528a97c846654b64f0081&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Over the last decade, we have seen a revolution in connectivity between computers, and a resulting paradigm shift from centralized to highly distributed systems. With massive scale also comes massive instability, as node and link failures become the norm rather than the exception. For such highly volatile systems, decentralized gossip-based protocols are emerging as an approach to maintaining simplicity and scalability while achieving fault-tolerant information dissemination. In this paper, we study the problem of computing aggregates with gossip-style protocols. Our first contribution is an analysis of simple gossip-based protocols for the computation of sums, averages, random samples, quantiles, and other aggregate functions, and we show that our protocols converge exponentially fast to the true answer when using uniform gossip. Our second contribution is the definition of a precise notion of the speed with which a node's data diffuses through the network. We show that this diffusion speed is at the heart of the approximation guarantees for all of the above problems. We analyze the diffusion speed of uniform gossip in the presence of node and link failures, as well as for flooding-based mechanisms. The latter expose interesting connections to random walks on graphs.", 
            "authors": [
                "David Kempe", 
                "Alin Dobra", 
                "Johannes Gehrke"
            ], 
            "fields": [
                "Fault tolerance", 
                "Mountain pass", 
                "Sampling", 
                "Paradigm shift", 
                "Distributed computing"
            ], 
            "title": "Gossip-based computation of aggregate information", 
            "url": "http://cn.bing.com/academic/profile?id=79c3118f9ff4c0fb7d2f7d0ecb48a9d5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Considering the urgency of the need for standards which would allow constitution of heterogeneous computer networks, ISO created a new subcommittee for \"Open Systems Interconnection\" (ISO/ TC97/SC 16) in 1977. The first priority of subcommittee 16 was to develop an architecture for open systems interconnection which could serve as a framework for the definition of standard protocols. As a result of 18 months of studies and discussions, SC16 adopted a layered architecture comprising seven layers (Physical, Data Link, Network, Transport, Session, Presentation, and Application). In July 1979 the specifications of this architecture, established by SC16, were passed under the name of \"OSI Reference Model\" to Technical Committee 97 \"Data Processing\" along with recommendations to start officially, on this basis, a set of protocols standardization projects to cover the most urgent needs. These recommendations were adopted by T.C97 at the end of 1979 as the basis for the following development of standards for Open Systems Interconnectlon within ISO. The OSI Reference Model was also recognized by CCITT Rapporteur's Group on \"Layered Model for Public Data Network Services.\" This paper presents the model of architecture for Open Systems Interconnection developed by SC16. Some indications are also given on the initial set of protocols which will-likely be developed in this OSI Reference Model.", 
            "authors": [
                "Hubert Zimmermann"
            ], 
            "fields": [
                "Data processing", 
                "Computer network", 
                "Computer-aided manufacturing", 
                "Reference model", 
                "Symmetric multiprocessor system"
            ], 
            "title": "OSI Reference Model--The ISO Model of Architecture for Open Systems Interconnection", 
            "url": "http://cn.bing.com/academic/profile?id=15057c0df2c2cfbf3c7129410bbf5f6b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Only a small proportion of the mouse genome is transcribed into mature messenger RNA transcripts. There is an international collaborative effort to identify all full-length mRNA transcripts from the mouse, and to ensure that each is represented in a physical collection of clones. Here we report the manual annotation of 60,770 full-length mouse complementary DNA sequences. These are clustered into 33,409 'transcriptional units', contributing 90.1% of a newly established mouse transcriptome database. Of these transcriptional units, 4,258 are new protein-coding and 11,665 are new non-coding messages, indicating that non-coding RNA is a major component of the transcriptome. 41% of all transcriptional units showed evidence of alternative splicing. In protein-coding transcripts, 79% of splice variations altered the protein product. Whole-transcriptome analyses resulted in the identification of 2,431 sense-antisense pairs. The present work, completely supported by physical clones, provides the most comprehensive survey of a mammalian transcriptome so far, and is a valuable resource for functional genomics.", 
            "authors": [
                "Y Okazaki", 
                "Masaaki Furuno", 
                "Takeya Kasukawa", 
                "Jun Adachi", 
                "Hidemasa Bono", 
                "Shinji Kondo", 
                "Itoshi Nikaido", 
                "Naoki Osato", 
                "Rintaro Saito", 
                "H Suzuki", 
                "Itaru Yamanaka", 
                "Hidenori Kiyosawa", 
                "Kasumi Yagi", 
                "Yasuhiro Tomaru", 
                "Yoshihisa Hasegawa", 
                "A Nogami", 
                "Christian Schonbach", 
                "Takashi Gojobori", 
                "Richard M Baldar\u2026", 
                ""
            ], 
            "fields": [
                "Environmental science", 
                "Cancer", 
                "Evolution", 
                "DNA sequencing", 
                "Drug discovery"
            ], 
            "title": "Analysis of the mouse transcriptome based on functional annotation of 60,770 full-length cDNAs.", 
            "url": "http://cn.bing.com/academic/profile?id=816248bd3248e548696ce6718a049185&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The design and correctness of a communication facility for a distributed computer system are reported on. The facility provides support for  fault-tolerant process groups  in the form of a family of reliable multicast protocols that can be used in both local- and wide-area networks. These protocols attain high levels of concurrency, while respecting application-specific delivery ordering constraints, and have varying cost and performance that depend on the degree of ordering desired. In particular, a protocol that enforces causal delivery orderings is introduced and shown to be a valuable alternative to conventional asynchronous communication protocols. The facility also ensures that the processes belonging to a fault-tolerant process group will observe consistent orderings of events affecting the group as a whole, including process failures, recoveries, migration, and dynamic changes to group properties like member rankings. A review of several uses for the protocols in the ISIS system, which supports fault-tolerant resilient objects and bulletin boards, illustrates the significant simplification of higher level algorithms made possible by our approach.", 
            "authors": [
                "Kenneth P Birman", 
                "Thomas A Joseph"
            ], 
            "fields": [
                "Reliable multicast", 
                "Asynchronous communication", 
                "Technical report", 
                "Atomic broadcast", 
                "Computer network"
            ], 
            "title": "Reliable communication in the presence of failures", 
            "url": "http://cn.bing.com/academic/profile?id=96d29cf7612ce047fdd1f39462753537&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The process of categorizing packets into \"flows\" in an Internet router is called packet classification. All packets belonging to the same flow obey a predefined rule and are processed in a similar manner by the router. For example, all packets with the same source and destination IP addresses may be defined to form a flow. Packet classification is needed for non-best-effort services, such as firewalls and quality of service; services that require the capability to distinguish and isolate traffic in different flows for suitable processing. In general, packet classification on multiple fields is a difficult problem. Hence, researchers have proposed a variety of algorithms which, broadly speaking, can be categorized as basic search algorithms, geometric algorithms, heuristic algorithms, or hardware-specific search algorithms. In this tutorial we describe algorithms that are representative of each category, and discuss which type of algorithm might be suitable for different applications.", 
            "authors": [
                "P Gupta", 
                "Nick Mckeown"
            ], 
            "fields": [
                "Quality of service", 
                "Heuristic", 
                "Best-effort delivery", 
                "Packet switching", 
                "Search algorithm"
            ], 
            "title": "Algorithms for packet classification", 
            "url": "http://cn.bing.com/academic/profile?id=c15d4b9c8322fbc2aa46aa34b6bed08e&encoded=0&v=paper_preview&mkt=zh-cn"
        }
    ], 
    "num": 172
}