{
    "data": [
        {
            "abstract": "Room-temperature ultraviolet lasing in semiconductor nanowire arrays has been demonstrated. The self-organized,   oriented zinc oxide nanowires grown on sapphire substrates were synthesized with a simple vapor transport and condensation process. These wide band-gap semiconductor nanowires form natural laser cavities with diameters varying from 20 to 150 nanometers and lengths up to 10 micrometers. Under optical excitation, surface-emitting lasing action was observed at 385 nanometers, with an emission linewidth less than 0.3 nanometer. The chemical flexibility and the one-dimensionality of the nanowires make them ideal miniaturized laser light sources. These short-wavelength nanolasers could have myriad applications, including optical computing, information storage, and microanalysis.", 
            "authors": [
                "Michael H Huang", 
                "Samuel S Mao", 
                "H Feick", 
                "Haoquan Yan", 
                "Yiying Wu", 
                "Hannes Kind", 
                "E R Weber", 
                "Richard E Russo", 
                "Peidong Yang"
            ], 
            "fields": [
                "Self-organization", 
                "Engineering", 
                "Zinc", 
                "Self-assembly", 
                "Nanowire"
            ], 
            "title": "Room-Temperature Ultraviolet Nanowire Nanolasers", 
            "url": "http://cn.bing.com/academic/profile?id=9abbe7c568cc00c0740ec120de8cfa8d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This review describes a new paradigm of electronics based on the spin degree of freedom of the electron. Either adding the spin degree of freedom to conventional charge-based electronic devices or using the spin alone has the potential advantages of nonvolatility, increased data processing speed, decreased electric power consumption, and increased integration densities compared with conventional semiconductor devices. To successfully incorporate spins into existing semiconductor technology, one has to resolve technical issues such as efficient injection, transport, control and manipulation, and detection of spin polarization as well as spin-polarized currents. Recent advances in new materials engineering hold the promise of realizing spintronic devices in the near future. We review the current state of the spin-based devices, efforts in new materials fabrication, issues in spin transport, and optical spin manipulation.", 
            "authors": [
                "S A Wolf", 
                "D D Awschalom", 
                "R A Buhrman", 
                "J M Daughton", 
                "S Von Molnar", 
                "Michael L Roukes", 
                "Almadena Chtchelkanova", 
                "D M Treger"
            ], 
            "fields": [
                "Data processing", 
                "Efficiency", 
                "Density", 
                "Semiconductor device", 
                "Electronics"
            ], 
            "title": "Spintronics: A Spin-Based Electronics Vision for the Future", 
            "url": "http://cn.bing.com/academic/profile?id=d275bfb431e0df09f89e96092db25126&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A method of second-order accuracy is described for integrating the equations of ideal compressible flow. The method is based on the integral conservation laws and is dissipative, so that it can be used across shocks. The heart of the method is a one-dimensional Lagrangean scheme that may be regarded as a second-order sequel to Godunov\u2019s method. The second-order accuracy is achieved by taking the distributions of the state quantities inside a gas slab to be linear, rather than uniform as in Godunov\u2019s method. The Lagrangean results are remapped with least-squares accuracy onto the desired Euler grid in a separate step. Several monotonicity algorithms are applied to ensure positivity, monotonicity and nonlinear stability. Higher dimensions are covered through time splitting. Numerical results for one-dimensional and two-dimensional flows are presented, demonstrating the efficiency of the method. The paper concludes with a summary of the results of the whole series \u201cTowards the Ultimate Conservative Difference Scheme.\u201d This paper describes a method of second-order accuracy for integrating the equations of ideal compressible flow (ICF). The method is based on the integral conservation laws and is dissipative, so that it can be used across shocks. The heart of the method is a one-dimensional Lagrangean scheme, the results of which are remapped onto the desired Euler grid in a separate step. Higher dimensions are covered through time splitting. The Lagrangean scheme may be regarded as a second-order sequel to Godunov\u2019s [I] first-order Lagrangean scheme. As in the latter, the gas is divided into slabs, and the interaction of these slabs at their interfaces is considered in detail. Whereas in Godunov\u2019s method the distribution of the state quantities inside a slab are taken to be uniform, in the present method these distributions are taken to be linear. The information contained in the slopes of the distributions makes it possible to attain second-order accuracy in the method. As in Godunov\u2019s method, the interaction of slabs is evaluated essentially on the basis of the characteristic equations, with due care taken to account for the discontinuities in the interaction flow. The convective difference scheme, hidden in the Lagrangean scheme, for integrating the characteristic equations is a so-called upstream-centered (upwind) scheme and has been discussed as \u201cscheme II\u201d in the pre", 
            "authors": [
                "B Van Leer"
            ], 
            "fields": [
                "Finite difference", 
                "Equation of state", 
                "Physics", 
                "Second-order logic", 
                "Mathematical logic"
            ], 
            "title": "Towards the ultimate conservative difference scheme. V. A second-order sequel to Godunov`s method", 
            "url": "http://cn.bing.com/academic/profile?id=907f30ef6e5df9eef5ecc13efba96f1d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Finite difference schemes providing an improved representation of a range of scales (spectral-like resolution) in the evaluation of first, second, and higher order derivatives are presented and compared with well-known schemes. The schemes may be used on non-uniform meshes and a variety of boundary conditions may be imposed. Schemes are also presented for derivatives at mid-cell locations, for accurate interpolation and for spectral-like filtering. Applications to fluid mechanics problems are discussed. 0 1992 Academic Press. Inc.", 
            "authors": [
                "Sanjiva K Lele"
            ], 
            "fields": [
                "Fluid mechanics", 
                "Higher-order logic", 
                "Turbulence", 
                "Second derivative", 
                "Spectral method"
            ], 
            "title": "Compact finite difference schemes with spectral-like resolution", 
            "url": "http://cn.bing.com/academic/profile?id=e89c329d9a95751e71f9d6d526d2f204&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Where conservation resources are limited and conservation targets are diverse, robust yet flexible priority-setting frameworks are vital. Priority-setting is especially important for geographically widespread species with distinct populations subject to multiple threats that operate on different spatial and temporal scales. Marine turtles are widely distributed and exhibit intra-specific variations in population sizes and trends, as well as reproduction and morphology. However, current global extinction risk assessment frameworks do not assess conservation status of spatially and biologically distinct marine turtle Regional Management Units (RMUs), and thus do not capture variations in population trends, impacts of threats, or necessary conservation actions across individual populations. To address this issue, we developed a new assessment framework that allowed us to evaluate, compare and organize marine turtle RMUs according to status and threats criteria. Because conservation priorities can vary widely (i.e. from avoiding imminent extinction to maintaining long-term monitoring efforts) we developed a \u2018\u2018conservation priorities portfolio\u2019\u2019 system using categories of paired risk and threats scores for all RMUs (n = 58). We performed these assessments and rankings globally, by species, by ocean basin, and by recognized geopolitical bodies to identify patterns in risk, threats, and data gaps at different scales. This process resulted in characterization of risk and threats to all marine turtle RMUs, including identification of the world\u2019s 11 most endangered marine turtle RMUs based on highest risk and threats scores. This system also highlighted important gaps in available information that is crucial for accurate conservation assessments. Overall, this priority-setting framework can provide guidance for research and conservation priorities at multiple relevant scales, and should serve as a model for conservation status assessments and prioritysetting for widespread, long-lived taxa.", 
            "authors": [
                "Bryan P Wallace", 
                "Andrew D Dimatteo", 
                "Alan B Bolten", 
                "Milani Chaloupka", 
                "Brian J Hutchinson", 
                "F Alberto Abreugrobois", 
                "Jeanne A Mortimer", 
                "Jeffrey A Seminoff", 
                "Diego F Amorocho", 
                "Karen A Bjorndal", 
                "Jerome Bourjea", 
                "Brian W Bowen", 
                "Raquel Briseno Duenas", 
                "Paolo Casale", 
                "Bmm Choudhury", 
                "Alice Costa", 
                "Peter H Dutton", 
                ""
            ], 
            "fields": [
                "Reproduction", 
                "Turtle", 
                "Population size", 
                "Oceanic basin", 
                "Marine conservation"
            ], 
            "title": "Global Conservation Priorities for Marine Turtles", 
            "url": "http://cn.bing.com/academic/profile?id=86dca6cba7af4f4f9d682bcc9cd1c028&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Optimal shape design of structural elements based on boundary variations results in final designs that are topologically equivalent to the initial choice of design, and general, stable computational schemes for this approach often require some kind of remeshing of the finite element approximation of the analysis problem. This paper presents a methodology for optimal shape design where both these drawbacks can be avoided. The method is related to modern production techniques and consists of computing the optimal distribution in space of an anisotropic material that is constructed by introducing an infimum of periodically distributed small holes in a given homogeneous, i~otropic material, with the requirement that the resulting structure can carry the given loads as well as satisfy other design requirements. The computation of effective material properties for the anisotropic material is carried out using the method of homogenization. Computational results are presented and compared with results obtained by boundary variations.", 
            "authors": [
                "Martin P Bendsoe", 
                "Noboru Kikuchi"
            ], 
            "fields": [
                "Mathematical optimization", 
                "Computer Science", 
                "Engineering", 
                "Shape", 
                "Homogenization"
            ], 
            "title": "Generating optimal topologies in structural design using a homogenization method", 
            "url": "http://cn.bing.com/academic/profile?id=c55c17eaea913a72f0d49c312caf294d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A level set approach for computing solutions to incompressible two-phase flow is presented. The interface between the two fluids is considered to be sharp and is described as the zero level set of a smooth function. A new treatment of the level set method allows us to efficiently maintain the level set function as the signed distance from the interface. We never have to explicitly reconstruct or find the zero level set. Consequently, we are able to handle arbitrarily complex topologies, large density and viscosity ratios, and surface tension, on relatively coarse grids. We use a second order projection method along with a second order upwinded procedure for advecting the momentum and level set equations. We consider the motion of air bubbles and water drops. We also compute flows with multiple fluids such as air, oil, and water.", 
            "authors": [
                "Mark Sussman", 
                "Peter Smereka", 
                "Stanley Osher"
            ], 
            "fields": [
                "Algorithm", 
                "Equations of motion", 
                "Discretization", 
                "Surface tension", 
                "Compressibility"
            ], 
            "title": "A level set approach for computing solutions to incompressible two-phase flow", 
            "url": "http://cn.bing.com/academic/profile?id=ef8c19984ac24e85714ac142c1700486&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Recent years have witnessed many breakthroughs in research on graphene (the first two-dimensional atomic crystal) as well as a significant advance in the mass production of this material. This one-atom-thick fabric of carbon uniquely combines extreme mechanical strength, exceptionally high electronic and thermal conductivities, impermeability to gases, as well as many other supreme properties, all of which make it highly attractive for numerous applications. Here we review recent progress in graphene research and in the development of production methods, and critically analyse the feasibility of various graphene applications.", 
            "authors": [
                "K S Novoselov", 
                "V I Fal Ko", 
                "L Colombo", 
                "P R Gellert", 
                "M G Schwab", 
                "K Kim"
            ], 
            "fields": [
                "Bilayer graphene", 
                "Physics", 
                "Nature", 
                "Technology", 
                "Optical modulator"
            ], 
            "title": "A roadmap for graphene", 
            "url": "http://cn.bing.com/academic/profile?id=b006350e34cb32004c299a6b46ae82a4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A new type of semiconductor laser is studied, in which injected carriers in the active region are quantum mechanically confined in two or three dimensions (2D or 3D). Effects of such confinements on the lasing characteristics are analyzed. Most important, the threshold current of such laser is predicted to be far less temperature sensitive than that of conventional lasers, reflecting the reduced dimensionality of electronic state. In the case of 3D\u2010QW laser, the temperature dependence is virtually eliminated. An experiment on 2D quantum well lasers is performed by placing a conventional laser in a strong magnetic field (30 T) and has demonstrated the predicted increase of T 0 value from 144 to 313\u2009\u00b0C.", 
            "authors": [
                "Yasuhiko Arakawa", 
                "Hiroyuki Sakaki"
            ], 
            "fields": [
                "Temperature coefficient", 
                "Magnetic field", 
                "Engineering", 
                "Quantum efficiency", 
                "Data"
            ], 
            "title": "Multidimensional quantum well laser and temperature dependence of its threshold current", 
            "url": "http://cn.bing.com/academic/profile?id=84a5899f5d47d4b5eb8cf39b68e9d753&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The goal of this paper is to present a critical survey of existing literature on human and machine recognition of faces. Machine recognition of faces has several applications, ranging from static matching of controlled photographs as in mug shots matching and credit card verification to surveillance video images. Such applications have different constraints in terms of complexity of processing requirements and thus present a wide range of different technical challenges. Over the last 20 years researchers in psychophysics, neural sciences and engineering, image processing analysis and computer vision have investigated a number of issues related to face recognition by humans and machines. Ongoing research activities have been given a renewed emphasis over the last five years. Existing techniques and systems have been tested on different sets of images of varying complexities. But very little synergism exists between studies in psychophysics and the engineering literature. Most importantly, there exists no evaluation or benchmarking studies using large databases with the image quality that arises in commercial and law enforcement applications In this paper, we first present different applications of face recognition in commercial and law enforcement sectors. This is followed by a brief overview of the literature on face recognition in the psychophysics community. We then present a detailed overview of move than 20 years of research done in the engineering community. Techniques for segmentation/location of the face, feature extraction and recognition are reviewed. Global transform and feature based methods using statistical, structural and neural classifiers are summarized. >", 
            "authors": [
                "R Chellappa", 
                "Charles L Wilson", 
                "Saad A Sirohey"
            ], 
            "fields": [
                "Image processing", 
                "Feature extraction", 
                "Pattern recognition", 
                "Engineering", 
                "Verification"
            ], 
            "title": "Human and machine recognition of faces: a survey", 
            "url": "http://cn.bing.com/academic/profile?id=00872569f230e36e1febd3829c307f9b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "CHARMM (Chemistry at HARvard Molecular Mechanics) is a highly versatile and widely used molecu- lar simulation program. It has been developed over the last three decades with a primary focus on molecules of bio- logical interest, including proteins, peptides, lipids, nucleic acids, carbohydrates, and small molecule ligands, as they occur in solution, crystals, and membrane environments. For the study of such systems, the program provides a large suite of computational tools that include numerous conformational and path sampling methods, free energy estima- tors, molecular minimization, dynamics, and analysis techniques, and model-building capabilities. The CHARMM program is applicable to problems involving a much broader class of many-particle systems. Calculations with CHARMM can be performed using a number of different energy functions and models, from mixed quantum mechanical-molecular mechanical force fields, to all-atom classical potential energy functions with explicit solvent and various boundary conditions, to implicit solvent and membrane models. The program has been ported to numer- ous platforms in both serial and parallel architectures. This article provides an overview of the program as it exists today with an emphasis on developments since the publication of the original CHARMM article in 1983.", 
            "authors": [
                "Bernard R Brooks", 
                "Charles L Brooks", 
                "Alexander D Mackerell", 
                "L Nilsson", 
                "Robert J Petrella", 
                "Benoit Roux", 
                "Yangdon Won", 
                "Georgios Archontis", 
                "C Bartels", 
                "Stefan Boresch", 
                "Amedeo Caflisch", 
                "L Caves", 
                "Qiang Cui", 
                "A R Dinner", 
                "M Feig", 
                "Stefan Fischer", 
                "Jiali Gao", 
                "Milan Hodoscek", 
                "Wonpil Im", 
                "Karol Kuczera", 
                "Theoharis Lazaridis", 
                ""
            ], 
            "fields": [
                "Potential energy", 
                "Quantum mechanics", 
                "Nucleic acid", 
                "Boundary value problem", 
                "Computer simulation"
            ], 
            "title": "CHARMM: The biomolecular simulation program", 
            "url": "http://cn.bing.com/academic/profile?id=893debaf3847e58d4d68a2862f054d2b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Watershed models are powerful tools for simulating the effect of watershed processes and management on soil and water resources. However, no comprehensive guidance is available to facilitate model evaluation in terms of the accuracy of simulated data compared to measured flow and constituent values. Thus, the objectives of this research were to: (1) determine recommended model evaluation techniques (statistical and graphical), (2) review reported ranges of values and corresponding performance ratings for the recommended statistics, and (3) establish guidelines for model evaluation based on the review results and project-specific considerations; all of these objectives focus on simulation of streamflow and transport of sediment and nutrients. These objectives were achieved with a thorough review of relevant literature on model application and recommended model evaluation methods. Based on this analysis, we recommend that three quantitative statistics, Nash-Sutcliffe efficiency (NSE), percent bias (PBIAS), and ratio of the root mean square error to the standard deviation of measured data (RSR), in addition to the graphical techniques, be used in model evaluation. The following model evaluation performance ratings were established for each recommended statistic. In general, model simulation can be judged as satisfactory if NSE > 0.50 and RSR < 0.70, and if PBIAS 25% for streamflow, PBIAS 55% for sediment, and PBIAS 70% for N and P. For PBIAS, constituent-specific performance ratings were determined based on uncertainty of measured data. Additional considerations related to model evaluation guidelines are also discussed. These considerations include: single-event simulation, quality and quantity of measured data, model calibration procedure, evaluation time step, and project scope and magnitude. A case study illustrating the application of the model evaluation guidelines is also provided. Keywords. Accuracy, Model calibration and validation, Simulation, Watershed model. omputer-based watershed models can save time and money because of their ability to perform long- term simulation of the effects of watershed pro- cesses and management activities on water quality, water quantity, and soil quality. These models also facilitate the simulation of various conservation program effects and aid policy design to mitigate water and soil quality degrada- tion by determining suitable conservation programs for par- ticular watersheds and agronomic settings. In order to use model outputs for tasks ranging from regulation to research, models should be scientifically sound, robust, and defensible (U.S. EPA, 2002).", 
            "authors": [
                "Daniel N Moriasi", 
                "Jeffrey G Arnold", 
                "M W Van Liew", 
                "Ronald L Bingner", 
                "R D Harmel", 
                "Tamie L Veith"
            ], 
            "fields": [
                "Watershed", 
                "Biological engineering", 
                "Simulation", 
                "Engineering", 
                "Agriculture"
            ], 
            "title": "MODEL EVALUATION GUIDELINES FOR SYSTEMATIC QUANTIFICATION OF ACCURACY IN WATERSHED SIMULATIONS", 
            "url": "http://cn.bing.com/academic/profile?id=1f1bc8b64e1a9da1aff48b13e8eb0e2c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The need for an integrated social constructivist approach towards the study of science and technology is outlined. Within such a programme both scientific facts and technological artefacts are to be understood as social constructs. Literature on the sociology of science, the science-technology relationship, and technology studies is reviewed. The empirical programme of relativism within the sociology of scientific knowledge and a recent study of the social construction of technological artefacts are combined to produce the new approach. The concepts of `interpretative flexibility' and `closure mechanism', and the notion of `social group' are developed and illustrated by reference to a study of solar physics and a study of the development of the bicycle. The paper concludes by setting out some of the terrain to be explored in future studies.", 
            "authors": [
                "Trevor Pinch", 
                "Wiebe E Bijker"
            ], 
            "fields": [
                "Engineering", 
                "Sociology", 
                "Sociology of scientific knowledge", 
                "Industrial history", 
                "Technology"
            ], 
            "title": "The Social Construction of Facts and Artefacts: or How the Sociology of Science and the Sociology of Technology might Benefit Each Other", 
            "url": "http://cn.bing.com/academic/profile?id=e8d267b9941e9fe1b271389aac762334&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background: Synesthesia is a condition in which the stimulation of one sense elicits an additional experience, often in a different (i.e., unstimulated) sense. Although only a small proportion of the population is synesthetic, there is growing evidence to suggest that neurocognitively-normal individuals also experience some form of synesthetic association between the stimuli presented to different sensory modalities (i.e., between auditory pitch and visual size, where lower frequency tones are associated with large objects and higher frequency tones with small objects). While previous research has highlighted crossmodal interactions between synesthetically corresponding dimensions, the possible role of synesthetic associations in multisensory integration has not been considered previously. Methodology: Here we investigate the effects of synesthetic associations by presenting pairs of asynchronous or spatially discrepant visual and auditory stimuli that were either synesthetically matched or mismatched. In a series of three psychophysical experiments, participants reported the relative temporal order of presentation or the relative spatial locations of the two stimuli. Principal Findings: The reliability of non-synesthetic participants\u2019 estimates of both audiovisual temporal asynchrony and spatial discrepancy were lower for pairs of synesthetically matched as compared to synesthetically mismatched audiovisual stimuli. Conclusions: Recent studies of multisensory integration have shown that the reduced reliability of perceptual estimates regarding intersensory conflicts constitutes the marker of a stronger coupling between the unisensory signals. Our results therefore indicate a stronger coupling of synesthetically matched vs. mismatched stimuli and provide the first psychophysical evidence that synesthetic congruency can promote multisensory integration. Synesthetic crossmodal correspondences therefore appear to play a crucial (if unacknowledged) role in the multisensory integration of auditory and visual information.", 
            "authors": [
                "Cesare Parise", 
                "Charles Spence"
            ], 
            "fields": [
                "Psychophysics", 
                "Audio signal processing", 
                "Physics", 
                "Vision", 
                "Engineering"
            ], 
            "title": "\u2018When Birds of a Feather Flock Together\u2019: Synesthetic Correspondences Modulate Audiovisual Integration in Non-Synesthetes", 
            "url": "http://cn.bing.com/academic/profile?id=525d0627335118531476807cf4a81710&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper presents a practical design perspective on multivariable feedback control problems. It reviews the basic issue-feedback design in the face of uncertainties-and generalizes known single-input, single-output (SISO) statements and constraints of the design problem to multiinput, multioutput (MIMO) cases. Two major MIMO design approaches are then evaluated in the context of these results.", 
            "authors": [
                "John Doyle", 
                "G Stein"
            ], 
            "fields": [
                "Computer Aided Design", 
                "Operation", 
                "Engineering", 
                "Generalization error", 
                "Stochastic process"
            ], 
            "title": "Multivariable feedback design: Concepts for a classical/modern synthesis", 
            "url": "http://cn.bing.com/academic/profile?id=fedba5ab75eac0e79b47934b8a1b8665&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This work was partially supported by salaries from the NOAA Coral Reef Conservation Program to the NOAA Coral Reef Conservation Program authors. NOAA provided funding to Caribbean ReefCheck investigators to undertake surveys of bleaching and mortality. Otherwise, no funding from outside authors' institutions was necessary for the undertaking of this study. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript.", 
            "authors": [
                "C Mark Eakin", 
                "Jessica A Morgan", 
                "Scott F Heron", 
                "Tyler B Smith", 
                "Gang Liu", 
                "Lorenzo Alvarezfilip", 
                "Bart Baca", 
                "Erich Bartels", 
                "C Bastidas", 
                "Claude Bouchon", 
                "Marilyn E Brandt", 
                "Andrew W Bruckner", 
                "Lucy Bunkleywilliams", 
                "Andrew J D Cameron", 
                "Billy Causey", 
                "Mark Chiappone", 
                "Tyler Christensen", 
                "M James C Crabbe", 
                "Owen Day", 
                ""
            ], 
            "fields": [
                "Sea surface temperature", 
                "Engineering", 
                "Environmental monitoring", 
                "Coral bleaching", 
                "Coral reef"
            ], 
            "title": "Caribbean corals in crisis: record thermal stress, bleaching, and mortality in 2005", 
            "url": "http://cn.bing.com/academic/profile?id=fdb5332a930f8e6be32da8ae48465cea&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A fuzzy logic system (FLS) is unique in that it is able to simultaneously handle numerical data and linguistic knowledge. It is a nonlinear mapping of an input data (feature) vector into a scalar output, i.e., it maps numbers into numbers. Fuzzy set theory and fuzzy logic establish the specifics of the nonlinear mapping. This tutorial paper provides a guided tour through those aspects of fuzzy sets and fuzzy logic that are necessary to synthesize an FLS. It does this by starting with crisp set theory and dual logic and demonstrating how both can be extended to their fuzzy counterparts. Because engineering systems are, for the most part, causal, we impose causality as a constraint on the development of the FLS. After synthesizing a FLS, we demonstrate that it can be expressed mathematically as a linear combination of fuzzy basis functions, and is a nonlinear universal function approximator, a property that it shares with feedforward neural networks. The fuzzy basis function expansion is very powerful because its basis functions can be derived from either numerical data or linguistic knowledge, both of which can be cast into the forms of IF-THEN rules. >", 
            "authors": [
                "J M Mendel"
            ], 
            "fields": [
                "Feedforward neural network", 
                "Feature vector", 
                "Reason", 
                "Fuzzy logic", 
                "Fuzzy set"
            ], 
            "title": "Fuzzy logic systems for engineering: a tutorial", 
            "url": "http://cn.bing.com/academic/profile?id=b4e342d6baa3e988c1f631c271ae5ddd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Single nucleotide polymorphism (SNP) discovery and genotyping are essential to genetic mapping. There remains a need for a simple, inexpensive platform that allows high-density SNP discovery and genotyping in large populations. Here we describe the sequencing of restriction-site associated DNA (RAD) tags, which identified more than 13,000 SNPs, and mapped three traits in two model organisms, using less than half the capacity of one Illumina sequencing run. We demonstrated that different marker densities can be attained by choice of restriction enzyme. Furthermore, we developed a barcoding system for sample multiplexing and fine mapped the genetic basis of lateral plate armor loss in threespine stickleback by identifying recombinant breakpoints in F2 individuals. Barcoding also facilitated mapping of a second trait, a reduction of pelvic structure, by in silico re-sorting of individuals. To further demonstrate the ease of the RAD sequencing approach we identified polymorphic markers and mapped an induced mutation in Neurospora crassa. Sequencing of RAD markers is an integrated platform for SNP discovery and genotyping. This approach should be widely applicable to genetic mapping in a variety of organisms.", 
            "authors": [
                "Nathan A Baird", 
                "Paul D Etter", 
                "Tressa S Atwood", 
                "Mark Currey", 
                "Anthony L Shiver", 
                "Zachary A Lewis", 
                "Eric U Selker", 
                "William A Cresko", 
                "Eric A Johnson"
            ], 
            "fields": [
                "Medicine", 
                "Physics", 
                "Chemistry", 
                "Genomic library", 
                "Genome"
            ], 
            "title": "Rapid SNP Discovery and Genetic Mapping Using Sequenced RAD Markers", 
            "url": "http://cn.bing.com/academic/profile?id=2a188f902fdfbee22babdd1150e298f8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The traffic-adaptive medium access protocol (TRAMA) is introduced for energy-efficient collision-free channel access in wireless sensor networks. TRAMA reduces energy consumption by ensuring that unicast and broadcast transmissions incur no collisions, and by allowing nodes to assume a low-power, idle state whenever they are not transmitting or receiving. TRAMA assumes that time is slotted and uses a distributed election scheme based on information about traffic at each node to determine which node can transmit at a particular time slot. Using traffic information, TRAMA avoids assigning time slots to nodes with no traffic to send, and also allows nodes to determine when they can switch off to idle mode and not listen to the channel. TRAMA is shown to be fair and correct, in that no idle node is an intended receiver and no receiver suffers collisions. An analytical model to quantify the performance of TRAMA is presented and the results are verified by simulation. The performance of TRAMA is evaluated through extensive simulations using both synthetic-as well as sensor-network scenarios. The results indicate that TRAMA outperforms contention-based protocols (CSMA, 802.11 and S-MAC) and also static scheduled-access protocols (NAMA) with significant energy savings.", 
            "authors": [
                "Venkatesh Rajendran", 
                "Katia Obraczka", 
                "J J Garcialunaaceves"
            ], 
            "fields": [
                "Efficient energy use", 
                "Wireless sensor network", 
                "Wireless ad hoc network", 
                "Engineering"
            ], 
            "title": "Energy-efficient, collision-free medium access control for wireless sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=2acba7a9cb6313a3d2c009e527882160&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The efficiency of a Carnot engine is treated for the case where the power output is limited by the rates of heat transfer to and from the working substance. It is shown that the efficiency, \u03b7, at maximum power output is given by the expression \u03b7 = 1 \u2212 (T2/T1)1/2 where T1 and T2 are the respective temperatures of the heat source and heat sink. It is also shown that the efficiency of existing engines is well described by the above result.", 
            "authors": [
                "F L Curzon", 
                "Boye Ahlborn"
            ], 
            "fields": [
                "Carnot cycle", 
                "Heat transfer", 
                "Energy transfer", 
                "Engineering", 
                "Engine"
            ], 
            "title": "Efficiency of a Carnot engine at maximum power output", 
            "url": "http://cn.bing.com/academic/profile?id=c8942372252c6770beea3a62d3fb0a10&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "While Eulerian schemes work well for most gas flows, they have been shown to admit nonphysical oscillations near some material interfaces. In contrast, Lagrangian schemes work well at multimaterial interfaces, but suffer from their own difficulties in problems with large deformations and vorticity characteristic of most gas flows. We believe that the most robust schemes will combine the best properties of Eulerian and Lagrangian schemes. In this paper, we propose a new numerical method for treating interfaces in Eulerian schemes that maintains a Heaviside profile of the density with no numerical smearing along the lines of earlier work and most Lagrangian schemes. We use a level set function to track the motion of a multimaterial interface in an Eulerian framework. In addition, the use of ghost cells (actually ghost nodes in our finite difference framework) and a new isobaric fix technique allows us to keep the density profile from smearing out, while still keeping the scheme robust and easy to program with simple extensions to multidimensions and multilevel time integration, e.g., Runge?Kutta methods. In contrast, previous methods used ill-advised dimensional splitting for multidimensional problems and suffered from great complexity when used in conjunction with multilevel time integrators.", 
            "authors": [
                "Ronald Fedkiw", 
                "Tariq D Aslam", 
                "Barry Merriman", 
                "Stanley Osher"
            ], 
            "fields": [
                "Level set", 
                "Lagrangian", 
                "Interface", 
                "Engineering", 
                "Finite difference"
            ], 
            "title": "A Non-oscillatory Eulerian Approach to Interfaces in Multimaterial Flows (the Ghost Fluid Method)", 
            "url": "http://cn.bing.com/academic/profile?id=dc958c4c65ecbe4a906aa0699bd93cab&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Quantitation of near infrared spectroscopic data in a scattering medium such as tissue requires knowledge of the optical pathlength in the medium. This can now be estimated directly from the time of flight of picosecond length light pulses. Monte Carlo modelling of light pulses in tissue has shown that the mean value of the time dispersed light pulse correlates with the pathlength used in quantitative spectroscopic calculations. This result has been verified in a phantom material. Time of flight measurements of pathlength across the rat head give a pathlength of 5.3+or-0.3 times the head diameter.", 
            "authors": [
                "D T Delpy", 
                "M Cope", 
                "P Van Der Zee", 
                "Simon R Arridge", 
                "S Wray", 
                "Jeremy C Wyatt"
            ], 
            "fields": [
                "Monte Carlo method", 
                "Radiology", 
                "Technology", 
                "Time of flight", 
                "Near-infrared spectroscopy"
            ], 
            "title": "Estimation of optical pathlength through tissue from direct time of flight measurement", 
            "url": "http://cn.bing.com/academic/profile?id=db23481bf8a63d7099628d45337d0d05&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "As part of a World Health Organization-led effort to update the empirical evidence base for the leishmaniases, national experts provided leishmaniasis case data for the last 5 years and information regarding treatment and control in their respective countries and a comprehensive literature review was conducted covering publications on leishmaniasis in 98 regional level between 2007 and 2011. Two questionnaires regarding epidemiology and drug access were completed by experts and national program managers. Visceral and cutaneous leishmaniasis incidence ranges were estimated by country and epidemiological region based on reported incidence, underreporting rates if available, and the judgment of national and international experts. Based on these estimates, approximately 0.2 to 0.4 cases and 0.7 to 1.2 million VL and CL cases, respectively, occur each year. More than 90% of global VL cases occur in six countries: India, Bangladesh, Sudan, South Sudan, Ethiopia and Brazil. Cutaneous leishmaniasis is more widely distributed, with about one-third of cases occurring in each of three epidemiological regions, the Americas, the Mediterranean basin, and western Asia from the Middle East to Central Asia. The ten countries with the highest estimated case counts, Afghanistan, Algeria, Colombia, Brazil, Iran, Syria, Ethiopia, North Sudan, Costa Rica and Peru, together account for 70 to 75% of global estimated CL incidence. Mortality data were extremely sparse and generally represent hospital-based deaths only. Using an overall case-fatality rate of 10%, we reach a tentative estimate of 20,000 to 40,000 leishmaniasis deaths per year. Although the information is very poor in a number of countries, this is the first in-depth exercise to better estimate the real impact of leishmaniasis. These data should help to define control strategies and reinforce leishmaniasis advocacy. Funding: The Spanish Agency for International Cooperation for Development (AECID) has provided generous support to the WHO Leishmaniasis program since 2005. This support permitted among many other activities regional meetings with the AFRO, EURO, PAHO and SEARO countries, and provided for short term contracts for IDV, MdB, MH and JS related to the preparation of the country profiles. Sanofi provided a grant for a regional meeting with the EMRO countries and various activities related to the control of cutaneous Leishmaniasis in the EMRO region. The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript. Competing Interests: The authors have declared that no competing interests exist. * E-mail: alvarj@who.int . These authors contributed equally to this work \" For a full list of the members of the WHO Leishmaniasis Control Team please see the Acknowledgments section.", 
            "authors": [
                "Jorge Alvar", 
                "Ivan Dario Velez", 
                "Caryn Bern", 
                "Merce Herrero", 
                "Philippe Desjeux", 
                "Jorge Cano", 
                "J Jannin", 
                "Margriet Den Boer"
            ], 
            "fields": [
                "Engineering", 
                "Case fatality rate", 
                "Global health", 
                "Middle East", 
                "Medicine"
            ], 
            "title": "Leishmaniasis Worldwide and Global Estimates of Its Incidence", 
            "url": "http://cn.bing.com/academic/profile?id=7a5a7e42f038952b5447f2f8d99629eb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Automatic estimation of salient object regions across images, without any prior assumption or knowledge of the contents of the corresponding scenes, enhances many computer vision and computer graphics applications. We introduce a regional contrast based salient object detection algorithm, which simultaneously evaluates global contrast differences and spatial weighted coherence scores. The proposed algorithm is simple, efficient, naturally multi-scale, and produces full-resolution, high-quality saliency maps. These saliency maps are further used to initialize a novel iterative version of GrabCut, namely SaliencyCut, for high quality unsupervised salient object segmentation. We extensively evaluated our algorithm using traditional salient object detection datasets, as well as a more challenging Internet image dataset. Our experimental results demonstrate that our algorithm consistently outperforms 15 existing salient object detection and segmentation methods, yielding higher precision and better recall rates. We also show that our algorithm can be used to efficiently extract salient object masks from Internet images, enabling effective sketch-based image retrieval (SBIR) via simple shape comparisons. Despite such noisy internet images, where the saliency regions are ambiguous, our saliency guided image retrieval achieves a superior retrieval rate compared with state-of-the-art SBIR methods, and additionally provides important target object region information.", 
            "authors": [
                "Mingming Cheng", 
                "Niloy J Mitra", 
                "Xiaolei Huang", 
                "Philip H S Torr", 
                "Shimin Hu"
            ], 
            "fields": [
                "Histogram", 
                "Extraction", 
                "Technology", 
                "Image retrieval", 
                "Image segmentation"
            ], 
            "title": "Global Contrast Based Salient Region Detection", 
            "url": "http://cn.bing.com/academic/profile?id=e15271e43a3788076a5b28565625fbe5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Pervasive micro-sensing and actuation may revolutionize the way in which we understand and manage complex physical systems: from airplane wings to complex ecosystems. The capabilities for detailed physical monitoring and manipulation offer enormous opportunities for almost every scientific discipline, and it will alter the feasible granularity of engineering. We identify opportunities and challenges for distributed signal processing in networks of these sensing elements and investigate some of the architectural challenges posed by systems that are massively distributed, physically-coupled, wirelessly networked, and energy limited.", 
            "authors": [
                "Deborah Estrin", 
                "Lewis Girod", 
                "Gregory J Pottie", 
                "Mani Srivastava"
            ], 
            "fields": [
                "Wireless", 
                "Ecosystem", 
                "Distributed computing", 
                "Intelligent sensor", 
                "Signal processing"
            ], 
            "title": "Instrumenting the world with wireless sensor networks", 
            "url": "http://cn.bing.com/academic/profile?id=bed04798b2890bae57b704158b5af84d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Large increases in productivity are typically realized as organizations gain experience in production. These \"learning curves\" have been found in many organizations. Organizations vary considerably in the rates at which they learn. Some organizations show remarkable productivity gains, whereas others show little or no learning. Reasons for the variation observed in organizational learning curves include organizational \"forgetting,\" employee turnover, transfer of knowledge from other products and other organizations, and economies of scale.", 
            "authors": [
                "Linda Argote", 
                "Dennis Epple"
            ], 
            "fields": [
                "Productivity", 
                "Mass production", 
                "Engineering", 
                "Empirical research", 
                "Economies of scale"
            ], 
            "title": "LEARNING CURVES IN MANUFACTURING", 
            "url": "http://cn.bing.com/academic/profile?id=7008e1509ec68b01512606c3c781da52&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The theory for a sensitive spectroscopy based on the photothermal deflection of a laser beam is developed. We consider cw and pulsed cases of both transverse and collinear photothermal deflection spectroscopy for solids, liquids, gases, and thin films. The predictions of the theory are experimentally verified, its implications for imaging and microscopy are given, and the sources of noise are analyzed. The sensitivity and versatility of photothermal deflection spectroscopy are compared with thermal lensing and photoacoustic spectroscopy.", 
            "authors": [
                "W B Jackson", 
                "Nabil M Amer", 
                "A C Boccara", 
                "Danielle Fournier"
            ], 
            "fields": [
                "Thin film", 
                "Refractive index", 
                "Spectroscopy", 
                "Engineering", 
                "Optical imaging"
            ], 
            "title": "Photothermal deflection spectroscopy and detection", 
            "url": "http://cn.bing.com/academic/profile?id=a95aa3212c0c8497af69b801e2c2818c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Finite-time stability is defined for equilibria of continuous but non-Lipschitzian autonomous systems. Continuity, Lipschitz continuity, and Holder continuity of the settling-time function are studied and illustrated with several examples. Lyapunov and converse Lyapunov results involving scalar differential inequalities are given for finite-time stability. It is shown that the regularity properties of the Lyapunov function and those of the settling-time function are related. Consequently, converse Lyapunov results can only assure the existence of continuous Lyapunov functions. Finally, the sensitivity of finite-time-stable systems to perturbations is investigated.", 
            "authors": [
                "S P Bhat", 
                "Dennis S Bernstein"
            ], 
            "fields": [
                "Engineering", 
                "Stability", 
                "Lipschitz continuity", 
                "Lyapunov function", 
                "Aerospace Engineering"
            ], 
            "title": "Finite-Time Stability of Continuous Autonomous Systems", 
            "url": "http://cn.bing.com/academic/profile?id=8d5b166c569be7d7c73c7919627a6c9a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We construct an explicit realistic SU(5) model in which softly broken supersymmetry is used to protect the Higgs doublets from quadratic mass renormalization. The model requires one natural but incredibly accurate adjustment of parameters. We argue that such an adjustment will be required in any supersymmetric GUT in which baryon number is not conserved.", 
            "authors": [
                "Savas Dimopoulos", 
                "Howard Georgi"
            ], 
            "fields": [
                "Perturbation theory", 
                "Physics", 
                "Engineering", 
                "Renormalization group", 
                "Supersymmetry"
            ], 
            "title": "Softly Broken Supersymmetry and SU(5)", 
            "url": "http://cn.bing.com/academic/profile?id=58a0a38fcc3e2ee45c51e243ad95ba49&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background: MicroRNAs (miRNA) are small non-coding RNAs that regulate translation of mRNA and protein. Loss or enhanced expression of miRNAs is associated with several diseases, including cancer. However, the identification of circulating miRNA in healthy donors is not well characterized. Microvesicles, also known as exosomes or microparticles, circulate in the peripheral blood and can stimulate cellular signaling. In this study, we hypothesized that under normal healthy conditions, microvesicles contain miRNAs, contributing to biological homeostasis. Methodology/Principal Findings: Microvesicles were isolated from the plasma of normal healthy individuals. RNA was isolated from both the microvesicles and matched mononuclear cells and profiled for 420 known mature miRNAs by realtime PCR. Hierarchical clustering of the data sets indicated significant differences in miRNA expression between peripheral blood mononuclear cells (PBMC) and plasma microvesicles. We observed 71 miRNAs co-expressed between microvesicles and PBMC. Notably, we found 33 and 4 significantly differentially expressed miRNAs in the plasma microvesicles and mononuclear cells, respectively. Prediction of the gene targets and associated biological pathways regulated by the detected miRNAs was performed. The majority of the miRNAs expressed in the microvesicles from the blood were predicted to regulate cellular differentiation of blood cells and metabolic pathways. Interestingly, a select few miRNAs were also predicted to be important modulators of immune function. Conclusions: This study is the first to identify and define miRNA expression in circulating plasma microvesicles of normal subjects. The data generated from this study provides a basis for future studies to determine the predictive role of peripheral blood miRNA signatures in human disease and will enable the definition of the biological processes regulated by these miRNA.", 
            "authors": [
                "Melissa Piper Hunter", 
                "Noura Ismail", 
                "Xiaoli Zhang", 
                "Baltazar D Aguda", 
                "Eun Joo Lee", 
                "Lianbo Yu", 
                "Tao Xiao", 
                "J Schafer", 
                "Meiling Ting Lee", 
                "Thomas D Schmittgen", 
                "S Patrick Nanasinkam", 
                "David Jarjoura", 
                "Clay B Marsh"
            ], 
            "fields": [
                "RNA extraction", 
                "microRNA", 
                "Metabolic pathway", 
                "Gene targeting", 
                "Hierarchical clustering"
            ], 
            "title": "Detection of microRNA Expression in Human Peripheral Blood Microvesicles", 
            "url": "http://cn.bing.com/academic/profile?id=f41daa935f64df37cf385101f8922ae7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "VLSI communication networks are wire-limited, i.e. the cost of a network is not a function of the number of switches required, but rather a function of the wiring density required to construct the network. Communication networks of varying dimensions are analyzed under the assumption of constant wire bisection. Expressions for the latency, average case throughput, and hot-spot throughput of k-ary n-cube networks with constant bisection that agree closely with experimental measurements are derived. It is shown that low-dimensional networks (e.g. tori) have lower latency and higher hot-spot throughput than high-dimensional networks (e.g. binary n-cubes) with the same bisection width. >", 
            "authors": [
                "W J Dally"
            ], 
            "fields": [
                "Message passing", 
                "Communication", 
                "Index term", 
                "Software performance testing", 
                "Electrical equipment"
            ], 
            "title": "Performance analysis of k-ary n-cube interconnection networks", 
            "url": "http://cn.bing.com/academic/profile?id=f45ef6ef649873639b2eee06135b21a8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The mechanisms underlying conscious visual perception are often studied with either binocular rivalry or perceptual rivalry stimuli. Despite existing research into both types of rivalry, it remains unclear to what extent their underlying mechanisms involve common computational rules. Computational models of binocular rivalry mechanisms are generally tested against Levelt\u2019s four propositions, describing the psychophysical relation between stimulus strength and alternation dynamics in binocular rivalry. Here we use a bistable rotating structure-from-motion sphere, a generally studied form of perceptual rivalry, to demonstrate that Levelt\u2019s propositions also apply to the alternation dynamics of perceptual rivalry. Importantly, these findings suggest that bistability in structure-from-motion results from active cross-inhibition between neural populations with computational principles similar to those present in binocular rivalry. Thus, although the neural input to the computational mechanism of rivalry may stem from different cortical neurons and different cognitive levels the computational principles just prior to the production of visual awareness appear to be common to the two types of rivalry. Citation: Klink PC, van Ee R, van Wezel RJA (2008) General Validity of Levelt\u2019s Propositions Reveals Common Computational Mechanisms for Visual Rivalry. PLoS", 
            "authors": [
                "P Christiaan Klink", 
                "Raymond Van Ee", 
                "Richard J A Van Wezel"
            ], 
            "fields": [
                "Psychophysics", 
                "Computer simulation", 
                "Medicine", 
                "Cognitive neuroscience", 
                "Biology"
            ], 
            "title": "General Validity of Levelt's Propositions Reveals Common Computational Mechanisms for Visual Rivalry", 
            "url": "http://cn.bing.com/academic/profile?id=fb395659f4484a4d82371631e4deb053&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Recently, two novel concepts have emerged in cancer biology: the role of so-called \u2018\u2018cancer stem cells\u2019\u2019 in tumor initiation, and the involvement of an epithelial-mesenchymal transition (EMT) in the metastatic dissemination of epithelial cancer cells. Using a mammary tumor progression model, we show that cells possessing both stem and tumorigenic characteristics of \u2018\u2018cancer stem cells\u2019\u2019 can be derived from human mammary epithelial cells following the activation of the Ras-MAPK pathway. The acquisition of these stem and tumorigenic characters is driven by EMT induction.", 
            "authors": [
                "Annepierre Morel", 
                "Marjory Lievre", 
                "Clemence Thomas", 
                "George Hinkal", 
                "Stephane Ansieau", 
                "Alain Puisieux"
            ], 
            "fields": [
                "Biology", 
                "Flow cytometry", 
                "Physics", 
                "Engineering", 
                "Chemistry"
            ], 
            "title": "Generation of Breast Cancer Stem Cells through Epithelial-Mesenchymal Transition", 
            "url": "http://cn.bing.com/academic/profile?id=45904336c55a7e3ba1ae557273a837eb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "In this paper, the analysis and design of master-slave teleoperation systems are discussed. The goal of this paper is to build a superior master-slave system that can provide good maneuverability. We first analyze a one degree-of-freedom system including operator and object dynamics. Second, some ideal responses of master-slave systems are defined and a quantitative index of maneuverability is given, based on the concept of ideal responses. Third, we propose new control schemes of master-slave manipulators that provide the ideal kinesthetic coupling such that the operator can maneuver the system as though he/she were directly manipulating the remote object himself/herself. The proposed control scheme requires accurate dynamic models of the master and slave arms, but neither parameters of the remote object nor the operator dynamics is necessary. Finally, the proposed control scheme is introduced to a prototype master-slave system and the experimental results show the validity of the proposed scheme. >", 
            "authors": [
                "Yasuyoshi Yokokohji", 
                "Tsuneo Yoshikawa"
            ], 
            "fields": [
                "Laboratory", 
                "ARM architecture", 
                "Remote control", 
                "Computer performance", 
                "Robotics"
            ], 
            "title": "Bilateral control of master-slave manipulators for ideal kinesthetic coupling-formulation and experiment", 
            "url": "http://cn.bing.com/academic/profile?id=c1ed37ea4a614d655621034db486d809&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Classifier systems are massively parallel, message-passing, rule-based systems that learn through credit assignment (the bucket brigade algorithm) and rule discovery (the genetic algorithm). They typically operate in environments that exhibit one or more of the following characteristics: (1) perpetually novel events accompanied by large amounts of noisy or irrelevant data; (2) continual, often real-time, requirements for action; (3) implicitly or inexactly defined goals; and (4) sparse payoff or reinforcement obtainable only through long action sequences. Classifier systems are designed to absorb new information continuously from such environments, devising sets of competing hypotheses (expressed as rules) without disturbing significantly capabilities already acquired. This paper reviews the definition, theory, and extant applications of classifier systems, comparing them with other machine learning techniques, and closing with a discussion of advantages, problems, and possible extensions of classifier systems.", 
            "authors": [
                "L B Booker", 
                "Deborah E Goldberg", 
                "John H Holland"
            ], 
            "fields": [
                "Artificial intelligence", 
                "Engineering", 
                "Computer Science", 
                "Genetic algorithm"
            ], 
            "title": "Classifier systems and genetic algorithms", 
            "url": "http://cn.bing.com/academic/profile?id=1e8f36eb5b02bd8f609192c1afb9e4fc&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Erbium-doped fiber amplifiers are modeled using the propagation and rate equations of a homogeneous two-level laser medium. Numerical methods are used to analyze the effects of optical modes and erbium confinement on amplifier performance, and to calculate both the gain and amplified spontaneous emission (ASE) spectra. Fibers with confined erbium doping are completely characterized from easily measured parameters: the ratio of the linear ion density to fluorescence lifetime, and the absorption of gain spectra. Analytical techniques then allow accurate evaluation of gain, saturation, and noise in low-gain amplifiers (G >", 
            "authors": [
                "C R Giles", 
                "E Desurvire"
            ], 
            "fields": [
                "Numerical analysis", 
                "Materials Science", 
                "Closed-form expression", 
                "Amplified spontaneous emission", 
                "Fluorescence-lifetime imaging microscopy"
            ], 
            "title": "Modeling erbium-doped fiber amplifiers", 
            "url": "http://cn.bing.com/academic/profile?id=a41361f38b43b81363fb7626bcc63289&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: Networks have become a key approach to understanding systems of interacting objects, unifying the study of diverse phenomena including biological organisms and human society. One crucial step when studying the structure and dynamics of networks is to identify communities: groups of related nodes that correspond to functional subunits such as protein complexes or social spheres. Communities in networks often overlap such that nodes simultaneously belong to several groups. Meanwhile, many networks are known to possess hierarchical organization, where communities are recursively grouped into a hierarchical structure. However, the fact that many real networks have communities with pervasive overlap, where each and every node belongs to more than one group, has the consequence that a global hierarchy of nodes cannot capture the relationships between overlapping groups. Here we reinvent communities as groups of links rather than nodes and show that this unorthodox approach successfully reconciles the antagonistic organizing principles of overlapping communities and hierarchy. In contrast to the existing literature, which has entirely focused on grouping nodes, link communities naturally incorporate overlap while revealing hierarchical organization. We find relevant link communities in many networks, including major biological networks such as protein-protein interaction and metabolic networks, and show that a large social network contains hierarchically organized community structures spanning inner-city to regional scales while maintaining pervasive overlap. Our results imply that link communities are fundamental building blocks that reveal overlap and hierarchical organization in networks to be two aspects of the same phenomenon.", 
            "authors": [
                "Yongyeol Ahn", 
                "James P Bagrow", 
                "Sune Lehmann"
            ], 
            "fields": [
                "Hierarchical control system", 
                "Protein\u2013protein interaction", 
                "Bioinformatics", 
                "Biological network", 
                "Overlay"
            ], 
            "title": "Link communities reveal multiscale complexity in networks.", 
            "url": "http://cn.bing.com/academic/profile?id=72433b2ed6da315088c2bf3399380931&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A simple linear stability analysis is presented which demonstrates that the nominally flat surface of an elastically stressed body is unstable with respect to the growth of perturbations with wavelengths greater than a critical wavelength. For a solid, constrained in one dimension and subject to a uniform dilatation, this wavelength scales as y/Z/u', where y is the surface energy, E is Young's modulus, and D is the nominal stress associated with the constrained dilatation. The maximally unstable mode depends on the manner of matter transport (surface diffusion and evaporation/condensation are considered). The predicted wavelength of the instability is consistent with observations of thin InGaAs films grown on GaAs.", 
            "authors": [
                "David J Srolovitz"
            ], 
            "fields": [
                "Film", 
                "Surface diffusion", 
                "Materials Science", 
                "Surface energy", 
                "Evaporation"
            ], 
            "title": "ON THE STABILITY OF SURFACES OF STRESSED SOLIDS", 
            "url": "http://cn.bing.com/academic/profile?id=df103f9f72e891d9aa33d439e2668871&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#A review of fracture mechanics and its relevance to designing with plastics is given. The basic philosophy of assuming the presence of flaws is developed, and the relevant equations are given. Data is presented for several polymers illustrating thickness and viscoelastic effects. A method of analysis for impact testing is then described together with some data. The unity of the data from a wide range of tests is emphasized.", 
            "authors": [
                "J G Williams"
            ], 
            "fields": [
                "Polymer", 
                "Engineering"
            ], 
            "title": "Fracture mechanics of polymers", 
            "url": "http://cn.bing.com/academic/profile?id=3976cf73fbad9317bc6ebab114092f3d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Summary 1. Increasingly, river managers are turning from hard engineering solutions to ecologically based restoration activities in order to improve degraded waterways. River restoration projects aim to maintain or increase ecosystem goods and services while protecting downstream and coastal ecosystems. There is growing interest in applying river restoration techniques to solve environmental problems, yet little agreement exists on what constitutes a successful river restoration effort. 2. We propose five criteria for measuring success, with emphasis on an ecological perspective. First, the design of an ecological river restoration project should be based on a specified guiding image of a more dynamic, healthy river that could exist at the site. Secondly, the river\u2019s ecological condition must be measurably improved. Thirdly, the river system must be more self-sustaining and resilient to external perturbations so that only minimal follow-up maintenance is needed. Fourthly, during the construction phase, no lasting harm should be inflicted on the ecosystem. Fifthly, both pre- and postassessment must be completed and data made publicly available. 3. Determining if these five criteria have been met for a particular project requires development of an assessment protocol. We suggest standards of evaluation for each of the five criteria and provide examples of suitable indicators. 4. Synthesis and applications . Billions of dollars are currently spent restoring streams and rivers, yet to date there are no agreed upon standards for what constitutes ecologically beneficial stream and river restoration. We propose five criteria that must be met for a river restoration project to be considered ecologically successful. It is critical that the broad restoration community, including funding agencies, practitioners and citizen restoration groups, adopt criteria for defining and assessing ecological success in restoration. Standards are needed because progress in the science and practice of river restoration has been hampered by the lack of agreed upon criteria for judging ecological success. Without well-accepted criteria that are ultimately supported by funding and implementing agencies, there is little incentive for practitioners to assess and report restoration outcomes. Improving methods and weighing the ecological benefits of various restoration approaches require organized national-level reporting systems.", 
            "authors": [
                "Margaret A Palmer", 
                "E S Bernhardt", 
                "J D Allan", 
                "P S Lake", 
                "Greg R Alexander", 
                "S Brooks", 
                "J Carr", 
                "S Clayton", 
                "C N Dahm", 
                "J Follstad Shah", 
                "David L Galat", 
                "Simone Loss", 
                "Peter Goodwin", 
                "David D Hart", 
                "Brooke A Hassett", 
                "R Jenkinson", 
                "G M Kondolf", 
                "Rebecca Lave", 
                "J L Meyer", 
                "T K Odonnell", 
                "Leonilde Pagano", 
                "Elizabeth B Sudduth"
            ], 
            "fields": [
                "Information technology", 
                "Engineering", 
                "Ecological succession", 
                "Community", 
                "System"
            ], 
            "title": "Standards for ecologically successful river restoration", 
            "url": "http://cn.bing.com/academic/profile?id=e91e10838da98eed225534c1a8660c59&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The purity of wafer surfaces is an essential requisite for the successful fabrication of VLSI and ULSI silicon circuits. Wafer cleaning chemistry has remained essentially unchanged in the past 25 years and is based on hot alkaline and acidic hydrogen peroxide solutions, a process known as \"RCA Standard Clean.\" This is still the primary method used in the industry. What has changed is its implementation with optimized equipment: from simple immersion to centrifugal spraying, megasonic techniques, and enclosed system processing that allow simultaneous removal of both contaminant films and particles. Improvements in wafer drying by use of isopropanol vapor or by \"slow-pull\" out of hot deionized water are being investigated. Several alternative cleaning methods are also being tested, including choline solutions, chemical vapor etching, and UV/ozone treatments. The evolution of silicon wafer cleaning processes and technology is traced and reviewed from the 1950s to August 1989. The importance of clean substrate surfaces in the fabrication of semiconductor microelectronic devices has been recognized since the early days of the 1950s. As the requirements for increased device performance and reliability have become more stringent in the era of VLSI and ULSI silicon circuit technology, techniques to avoid contamination and processes to generate very clean wafer surfaces have become critically important. Besides, over 50% of yield losses in integrated circuit fabrication are generally accepted to be due to microcontamination. Trace ibmpurities, such as sodium ions, metals, and particles, are especially detrimental if present on semiconductor surfaces during high-temperature processing (thermal oxidation, diffusion, epitaxial growth) because they may spread and diffuse into the semiconductor interior. Impurities must also be removed from surfaces before and/or after lower temperature steps, such as chemical vapor deposition, dopant implanting, and plasma reactions. Postcleaning after photoresist stripping is necessary for every mask level throughout the production process. Many wafer cleaning techniques have been tested and several are being used. The generally most successful approach for silicon wafers without metallization uses wetchemical treatments based on hydrogen peroxide chemistry. This process has remained essentially unchanged during the past 25 years, but important advances have been made in its technical implementation. The evolution of the cleaning technology for premetallized silicon wafers from its beginning to the present time will be traced in this paper.", 
            "authors": [
                "Werner Kern"
            ], 
            "fields": [
                "Film", 
                "Silicon", 
                "Drying", 
                "Centrifugal pump", 
                "Engineering"
            ], 
            "title": "The Evolution of Silicon Wafer Cleaning Technology", 
            "url": "http://cn.bing.com/academic/profile?id=56f613e60c8f59bf4f3d9753923fac79&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The design of advanced, nanostructured materials at the molecular level is of tremendous interest for the scientific and engineering communities because of the broad application of these materials in the biomedical field. Among the available techniques, the layer-by-layer assembly method introduced by Decher and co-workers in 1992 has attracted extensive attention because it possesses extraordinary advantages for biomedical applications: ease of preparation, versatility, capability of incorporating high loadings of different types of biomolecules in the films, fine control over the materials\u2019 structure, and robustness of the products under ambient and physiological conditions. In this context, a systematic review of current research on biomedical applications of layer-by-layer assembly is presented. The structure and bioactivity of biomolecules in thin films fabricated by layer-by-layer assembly are introduced. The applications of layer-bylayer assembly in biomimetics, biosensors, drug delivery, protein and cell adhesion, mediation of cellular functions, and implantable materials are addressed. Future developments in the field of biomedical applications of layer-by-layer assembly are also discussed.", 
            "authors": [
                "Z Tang", 
                "Y Wang", 
                "Paul Podsiadlo", 
                "Nicholas A Kotov"
            ], 
            "fields": [
                "Sensor", 
                "Materials Science", 
                "Biosensor", 
                "Chemistry", 
                "Layer by layer"
            ], 
            "title": "Biomedical Applications of Layer\u2010by\u2010Layer Assembly: From Biomimetics to Tissue Engineering", 
            "url": "http://cn.bing.com/academic/profile?id=3adf7b89cffb2675549dc6906352d756&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The drift region properties of 6H- and 3C-SiC-based Schottky rectifiers and power MOSFETs that result in breakdown voltages from 50 to 5000 V are defined. Using these values, the output characteristics of the devices are calculated and compared with those of Si devices. It is found that due to very low drift region resistance, 5000-V SiC Schottky rectifiers and power MOSFETs can deliver on-state current density of 100 A/cm/sup 2/ at room temperature with a forward drop of only 3.85 and 2.95 V, respectively. Both devices are expected to have excellent switching characteristics and ruggedness due to the absence of minority-carrier injection. A thermal analysis shows that 5000-V, 6H-, and 3C-SiC MOSFETs and Schottky rectifiers would be approximately 20 and 18 times smaller than corresponding Si devices, and that operation at higher temperatures and at higher breakdown voltages than conventional Si devices is possible. Also, a significant reduction in the die size is expected. >", 
            "authors": [
                "M Bhatnagar", 
                "B J Baliga"
            ], 
            "fields": [
                "Current density", 
                "Materials Science", 
                "Electrical equipment", 
                "Compounds of carbon", 
                "Propulsion"
            ], 
            "title": "Comparison of 6H-SiC, 3C-SiC, and Si for power devices", 
            "url": "http://cn.bing.com/academic/profile?id=449cb29d78418aefcbd99d0c674da19f&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The nature of the silent discharge (dielectric barrier discharge) is reviewed. Theoretical models for describing its discharge physics and ensuing plasma chemistry are presented. The phenomena leading to gas breakdown in such electrode configurations at about atmospheric pressure are discussed. The current transport takes place within a large numer of short-lived microdischarges, the plasma conditions of which are investigated. The theoretical predictions are compared to measurements. Two entirely different applications of silent discharges are treated: industrial ozone production and the formation of excimers to generate ultraviolet radiation. The special capability of the silent discharge for large-scale industrial processing is demonstrated. >", 
            "authors": [
                "Baldur Eliasson", 
                "Ulrich Kogelschatz"
            ], 
            "fields": [
                "Electric discharge", 
                "Dielectric", 
                "Dielectric barrier discharge", 
                "Atmospheric model", 
                "Electrode"
            ], 
            "title": "Modeling and applications of silent discharge plasmas", 
            "url": "http://cn.bing.com/academic/profile?id=57d7116f184dc1b1ec2e42dca1b1a04d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A study of a matrix pencil method for estimating frequencies and damping factors of exponentially damped and/or undamped sinusoids in noise is presented. Comparison of this method to a polynomial method (SVD-Prony method) shows that the matrix pencil method and the polynomial method are two special cases of a matrix prediction approach and that the pencil method is more efficient in computation and less restrictive about signal probes. It is found through perturbation analysis and simulation that, for signals with unknown damping factors, the pencil method is less sensitive to noise than the polynomial method. An expression of the Cramer-Rao bound for the exponential signals is presented. >", 
            "authors": [
                "Yingbo Hua", 
                "T K Sarkar"
            ], 
            "fields": [
                "Computational model", 
                "Phase noise", 
                "Cram\u00e9r\u2013Rao bound", 
                "Noise", 
                "Polynomial"
            ], 
            "title": "Matrix pencil method for estimating parameters of exponentially damped/undamped sinusoids in noise", 
            "url": "http://cn.bing.com/academic/profile?id=9f16172cdd4a1b94e0d03ee39b87a2c4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "AbstractThe question to be addressed is: Why do private firms perform basic research with their own money? Interest in this question derives from both analytical and utilitarian considerations. There is empirical evidence in the United States, which provides the main context for this paper. Supporting the view that basic research makes a significant contribution to the productivity growth of the economy [4,7]. It is widely held that social returns from basic research are significant and higher than private returns and it is for this reason that most such activities continue to be financed by the taxpayer. This also implies that measures aimed at increasing basic research by the private sector will be welfare improving. In the United States, the federal government in the years since the Second World War has provided the vast majority of all funds devoted to basic research. Although the federal share has been declining in recent years, and although that share is at its lowest level in about 20 years, it still constitutes about two-thirds of the total [10]\u2026", 
            "authors": [
                "Nathan Rosenberg"
            ], 
            "fields": [
                "Engineering", 
                "Innovation", 
                "Science, technology and society"
            ], 
            "title": "Why do firms do basic research (with their own money)", 
            "url": "http://cn.bing.com/academic/profile?id=1cda5521e9e8470a3acb541c0cb93ac4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We report first-principles calculations of the current-voltage ( I-V ) characteristics of a molecular device and compare with experiment. We find that the shape of the I-V curve is largely determined by the electronic structure of the molecule, while the presence of single atoms at the molecule-electrode interface play a key role in determining the absolute value of the current. The results show that such simulations would be useful for the design of future microelectronic devices for which the Boltzmann-equation approach is no longer applicable.", 
            "authors": [
                "M Di Ventra", 
                "Sokrates T Pantelides", 
                "N D Lang"
            ], 
            "fields": [
                "Boltzmann equation", 
                "Electronic structure", 
                "Electrode", 
                "Engineering", 
                "Interface"
            ], 
            "title": "First-Principles Calculation of Transport Properties of a Molecular Device", 
            "url": "http://cn.bing.com/academic/profile?id=9b6cfd9585b317356104433cf57b2db9&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background#R##N#The estimated number of new HIV infections in the United States reflects the leading edge of the epidemic. Previously, CDC estimated HIV incidence in the United States in 2006 as 56,300 (95% CI: 48,200\u201364,500). We updated the 2006 estimate and calculated incidence for 2007\u20132009 using improved methodology.#R##N##R##N#Methodology#R##N#We estimated incidence using incidence surveillance data from 16 states and 2 cities and a modification of our previously described stratified extrapolation method based on a sample survey approach with multiple imputation, stratification, and extrapolation to account for missing data and heterogeneity of HIV testing behavior among population groups.#R##N##R##N#Principal Findings#R##N#Estimated HIV incidence among persons aged 13 years and older was 48,600 (95% CI: 42,400\u201354,700) in 2006, 56,000 (95% CI: 49,100\u201362,900) in 2007, 47,800 (95% CI: 41,800\u201353,800) in 2008 and 48,100 (95% CI: 42,200\u201354,000) in 2009. From 2006 to 2009 incidence did not change significantly overall or among specific race/ethnicity or risk groups. However, there was a 21% (95% CI:1.9%\u201339.8%; p = 0.017) increase in incidence for people aged 13\u201329 years, driven by a 34% (95% CI: 8.4%\u201360.4%) increase in young men who have sex with men (MSM). There was a 48% increase among young black/African American MSM (12.3%\u201383.0%; p<0.001). Among people aged 13\u201329, only MSM experienced significant increases in incidence, and among 13\u201329 year-old MSM, incidence increased significantly among young, black/African American MSM. In 2009, MSM accounted for 61% of new infections, heterosexual contact 27%, injection drug use (IDU) 9%, and MSM/IDU 3%.#R##N##R##N#Conclusions/Significance#R##N#Overall, HIV incidence in the United States was relatively stable 2006\u20132009; however, among young MSM, particularly black/African American MSM, incidence increased. HIV continues to be a major public health burden, disproportionately affecting several populations in the United States, especially MSM and racial and ethnic minorities. Expanded, improved, and targeted prevention is necessary to reduce HIV incidence.", 
            "authors": [
                "Joseph Prejean", 
                "Ruiguang Song", 
                "Angela Hernandez", 
                "Rebecca Ziebell", 
                "Timothy A Green", 
                "Frances J Walker", 
                "Lillian S Lin", 
                "Qian An", 
                "Jonathan Mermin", 
                "Amy Lansky", 
                "H Irene Hall"
            ], 
            "fields": [
                "Young adult", 
                "Confidence interval", 
                "Public health", 
                "Missing data", 
                "Imputation"
            ], 
            "title": "Estimated HIV Incidence in the United States, 2006\u20132009", 
            "url": "http://cn.bing.com/academic/profile?id=7f31dff3823429eff4fd90fb6c874a2c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background: Recent evidence suggests that there is a link between metabolic diseases and bacterial populations in the gut. The aim of this study was to assess the differences between the composition of the intestinal microbiota in humans with type 2 diabetes and non-diabetic persons as control. Methods and Findings: The study included 36 male adults with a broad range of age and body-mass indices (BMIs), among which 18 subjects were diagnosed with diabetes type 2. The fecal bacterial composition was investigated by real-time quantitative PCR (qPCR) and in a subgroup of subjects (N=20) by tag-encoded amplicon pyrosequencing of the V4 region of the 16S rRNA gene. The proportions of phylum Firmicutes and class Clostridia were significantly reduced in the diabetic group compared to the control group (P=0.03). Furthermore, the ratios of Bacteroidetes to Firmicutes as well as the ratios of Bacteroides-Prevotella group to C. coccoides-E. rectale group correlated positively and significantly with plasma glucose concentration (P=0.04) but not with BMIs. Similarly, class Betaproteobacteria was highly enriched in diabetic compared to non-diabetic persons (P=0.02) and positively correlated with plasma glucose (P=0.04). Conclusions: The results of this study indicate that type 2 diabetes in humans is associated with compositional changes in intestinal microbiota. The level of glucose tolerance should be considered when linking microbiota with metabolic diseases such as obesity and developing strategies to control metabolic diseases by modifying the gut microbiota.", 
            "authors": [
                "Nadja Larsen", 
                "Finn K Vogensen", 
                "Frans Van Den Berg", 
                "Dennis Nielsen", 
                "Anne Sofie Andreasen", 
                "B K Pedersen", 
                "Waleed Abu Alsoud", 
                "Soren J Sorensen", 
                "Lars H Hansen", 
                "Mogens Jakobsen"
            ], 
            "fields": [
                "Medicine", 
                "Physics", 
                "Bacteria", 
                "Engineering", 
                "Obesity"
            ], 
            "title": "Gut Microbiota in Human Adults with Type 2 Diabetes Differs from Non-Diabetic Adults", 
            "url": "http://cn.bing.com/academic/profile?id=2ba92d27fd53e12541fd809101c3e3eb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#Starting with specific constitutive equations, methods of evaluating material properties from experimental data are outlined and then illustrated for some polymeric materials; these equations have been derived from thermodynamic principles, and are very similar to the Boltzmann superposition integral form of linear theory. The experimental basis for two equations under uniaxial loading and the influence of environmental factors on the properties are first examined. It is then shown that creep and recovery data can be conveiently used to evaluate properties in one equation, while two-step relaxation data serve the same purpose for the second equation. Methods of reducing data to accomplish this characterization and to determine the accuracy of the theory are illustrated using existing data on nitrocellulose film, fiber-reinforced phenolic resin, and polyisobutylene. Finally, a set of three-dimensional constitutive equations is proposed which is consistent with nonlinear behavior of some metals and plastics, and which enables all properties to be evaluated from uniaxial creep and recovery data.", 
            "authors": [
                "R A Schapery"
            ], 
            "fields": [
                "Engineering", 
                "Polymer"
            ], 
            "title": "On the characterization of nonlinear viscoelastic materials", 
            "url": "http://cn.bing.com/academic/profile?id=06ab2107e12d62b322087056117a2c9d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The explosive growth of e-commerce and online environments has made the issue of information search and selection increasingly serious; users are overloaded by options to consider and they may not have the time or knowledge to personally evaluate these options. Recommender systems have proven to be a valuable way for online users to cope with the information overload and have become one of the most powerful and popular tools in electronic commerce. Correspondingly, various techniques for recommendation generation have been proposed. During the last decade, many of them have also been successfully deployed in commercial environments. Recommender Systems Handbook, an edited volume, is a multi-disciplinary effort that involves world-wide experts from diverse fields, such as artificial intelligence, human computer interaction, information technology, data mining, statistics, adaptive user interfaces, decision support systems, marketing, and consumer behavior. Theoreticians and practitioners from these fields continually seek techniques for more efficient, cost-effective and accurate recommender systems. This handbook aims to impose a degree of order on this diversity, by presenting a coherent and unified repository of recommender systems major concepts, theories, methodologies, trends, challenges and applications. Extensive artificial applications, a variety of real-world applications, and detailed case studies are included. Recommender Systems Handbook illustrates how this technology can support the user in decision-making, planning and purchasing processes. It works for well known corporations such as Amazon, Google, Microsoft and AT&T. This handbook is suitable for researchers and advanced-level students in computer science as a reference.", 
            "authors": [
                "Francesco Ricci", 
                "Lior Rokach", 
                "Bracha Shapira", 
                "Paul B Kantor"
            ], 
            "fields": [
                "Engineering", 
                "Recommender system", 
                "Computer Science"
            ], 
            "title": "Recommender Systems Handbook", 
            "url": "http://cn.bing.com/academic/profile?id=14c269e5ce5e97add7f09adcd118380e&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Nanofluids, a new class of solid/liquid suspensions, offer scientific challenges because their measured thermal conductivity is one order of magnitude greater than predictions. It has long been known that liquid molecules close to a solid surface form layered solid-like structures, but little is known about the connection between this nanolayer and the thermal properties of the suspensions. Here, we have modified the Maxwell equation for the effective thermal conductivity of solid/liquid suspensions to include the effect of this ordered nanolayer. Because this ordered nanolayer has a major impact on nanofluid thermal conductivity when the particle diameter is less than 10 nm, a new direction is indicated for development of next-generation coolants.", 
            "authors": [
                "Wenhua Yu", 
                "Seungmok Choi", 
                "Energy Technology"
            ], 
            "fields": [
                "Heat exchanger", 
                "Engineering", 
                "Maxwell's equations", 
                "Fluid mechanics", 
                "Layers"
            ], 
            "title": "The role of interfacial layers in the enhanced thermal conductivity of nanofluids: A renovated Maxwell model", 
            "url": "http://cn.bing.com/academic/profile?id=d95dc425343e37fd7b4604b558911ae9&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The author surveys the literature on trade and foreign direct investment--especially wholly-owned subsidiaries of multinational firms and international joint ventures--as channels for technology transfer. He also discusses licensing and other arm's length channels of technology transfer. He concludes: 1) How trade encourages growth depends on whether knowledge spillover is national or international. Spillover is more likely to be national for developing countries than for industrial countries. 2) Local policy often makes pure foreign direct investment infeasible, so foreign firms choose licensing or joint ventures. The jury is still out on whether licensing or joint ventures lead to more learning by local firms. 3) Policies designed to attract foreign direct investment are proliferating. Several plant-level studies have failed to find positive spillover from foreign direct investment to firms competing directly with subsidiaries of multinationals. (However, these studies treat foreign direct investment as exogenous and assume spillover to be horizontal-when it may be vertical.) All such studies do find the subsidiaries of multinationals to be more productive than domestic firms, so foreign direct investment does result in host countries using resources more effectively. 4) Absorptive capacity in the host country is essential for getting significant benefits from foreign direct investment. Without adequate human capital or investments in research and development, spillover fails to materialize. 5) A country's policy on protection of intellectual property rights affects the type of industry it attracts. Firms for which such rights are crucial (such as pharmaceutical firms) are unlikely to invest directly in countries where such protections are weak, or will not invest in manufacturing and research and development activities. Policy on intellectual property rights also influences whether technology transfer comes through licensing, joint ventures, or the establishment of wholly-owned subsidiaries.", 
            "authors": [
                "Kamal Saggi"
            ], 
            "fields": [
                "Empirical evidence", 
                "Commercial policy", 
                "Returns to scale", 
                "Economic policy", 
                "Public policy"
            ], 
            "title": "Trade, foreign direct investment, and international technology transfer: a survey", 
            "url": "http://cn.bing.com/academic/profile?id=1b52f1a9278f8bc2dee5ff1d509acac5&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The analysis of the small-signal stability of conventional power systems is well established, but for inverter based microgrids there is a need to establish how circuit and control features give rise to particular oscillatory modes and which of these have poor damping. This paper develops the modeling and analysis of autonomous operation of inverter-based microgrids. Each sub-module is modeled in state-space form and all are combined together on a common reference frame. The model captures the detail of the control loops of the inverter but not the switching action. Some inverter modes are found at relatively high frequency and so a full dynamic model of the network (rather than an algebraic impedance model) is used. The complete model is linearized around an operating point and the resulting system matrix is used to derive the eigenvalues. The eigenvalues (termed \"modes\") indicate the frequency and damping of oscillatory components in the transient response. A sensitivity analysis is also presented which helps identifying the origin of each of the modes and identify possible feedback signals for design of controllers to improve the system stability. With experience it is possible to simplify the model (reduce the order) if particular modes are not of interest as is the case with synchronous machine models. Experimental results from a microgrid of three 10-kW inverters are used to verify the results obtained from the model", 
            "authors": [
                "N Pogaku", 
                "Milan Prodanovic", 
                "T C Green"
            ], 
            "fields": [
                "Stability", 
                "Electrical impedance", 
                "High frequency", 
                "Synchronous motor", 
                "Control system"
            ], 
            "title": "Modeling, Analysis and Testing of Autonomous Operation of an Inverter-Based Microgrid", 
            "url": "http://cn.bing.com/academic/profile?id=875e2403f72b0687f1c8e038f2eebb35&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The adaptive hypothesis predicts that contextual factors and past thermal history modify building occupants' thermal expectations and preferences. One of the predictions of the adaptive hypothesis is that people in warm climate zones prefer warmer indoor temperatures than people living in cold climate zones. This is contrary to the static assumptions underlying the current ASHRAE comfort standard 55-92. To examine the adaptive hypothesis and its implications for Standard 55-92, the ASHRAE RP-884 project assembled a quality-controlled database from thermal comfort field experiments worldwide (circa 21,000 observations from 160 buildings). Our statistical analysis examined the semantics of thermal comfort in terms of thermal sensation,", 
            "authors": [
                "Richard De Dear", 
                "Gail Brager"
            ], 
            "fields": [
                "Thermal comfort", 
                "Architecture", 
                "Climate", 
                "Energy conservation", 
                "Ventilation"
            ], 
            "title": "Developing an adaptive model of thermal comfort and preference", 
            "url": "http://cn.bing.com/academic/profile?id=d2bd004c6c9cf8ed576c2840db667903&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Fluid dynamic turbulence is one of the most challenging computational physics problems because of the extremely wide range of time and space scales involved, the strong nonlinearity of the governing equations, and the many practical and important applications. While most linear fluid instabilities are well understood, the nonlinear interactions among them makes even the relatively simple limit of homogeneous isotropic turbulence difficult to treat physically, mathematically, and computationally. Turbulence is modeled computationally by a two-stage bootstrap process. The first stage, direct numerical simulation, attempts to resolve the relevant physical time and space scales but its application is limited to diffusive flows with a relatively small Reynolds number (Re). Using direct numerical simulation to provide a database, in turn, allows calibration of phenomenological turbulence models for engineering applications. Large eddy simulation incorporates a form of turbulence modeling applicable when the large-scale flows of interest are intrinsically time dependent, thus throwing common statistical models into question. A promising approach to large eddy simulation involves the use of high-resolution monotone computational fluid dynamics algorithms such as flux-corrected transport or the piecewise parabolic method which have intrinsic subgrid turbulence models coupled naturally to the resolved scales in the computed flow. The physical considerations underlying and evidence supporting this monotone integrated large eddy simulation approach are discussed.", 
            "authors": [
                "Jay P Boris", 
                "F F Grinstein", 
                "E S Oran", 
                "Ronald L Kolbe"
            ], 
            "fields": [
                "Dynamics", 
                "Scale", 
                "Resolution", 
                "Direct numerical simulation", 
                "Reynolds number"
            ], 
            "title": "New insights into large eddy simulation", 
            "url": "http://cn.bing.com/academic/profile?id=b30117c33dff5c9bdb294a8ea4b3350b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An experimental technique is described for observing the effects of switching transients in digital MOS circuits that perturb analog circuits integrated on the same die by means of coupling through the substrate. Various approaches to reducing substrate crosstalk (the use of physical separation of analog and digital circuits, guard rings, and a low-inductance substrate bias) are evaluated experimentally for a CMOS technology with a substrate comprising an epitaxial layer grown on a heavily doped bulk wafer. Observations indicate that reducing the inductance in the substrate bias is the most effective. Device simulations are used to show how crosstalk propagates via the heavily doped bulk and to predict the nature of substrate crosstalk in CMOS technologies integrated in uniform, lightly doped bulk substrates, showing that in such cases the substrate noise is highly dependent on layout geometry. A method of including substrate effects in SPICE simulations for circuits fabricated on epitaxial, heavily doped substrates is developed. >", 
            "authors": [
                "Dong Su", 
                "M J Loinaz", 
                "Shoichi Masui", 
                "B A Wooley"
            ], 
            "fields": [
                "Reduction", 
                "Mathematical model", 
                "Crosstalk", 
                "Fabrication", 
                "Decoupling"
            ], 
            "title": "Experimental results and modeling techniques for substrate noise in mixed-signal integrated circuits", 
            "url": "http://cn.bing.com/academic/profile?id=d51fefa4bcf7e94a36675f23eb8033cb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Synthetic biologists engineer complex artificial biological systems to investigate natural biological phenomena and for a variety of applications. We outline the basic features of synthetic biology as a new engineering discipline, covering examples from the latest literature and reflecting on the features that make it unique among all other existing engineering fields. We discuss methods for designing and constructing engineered cells with novel functions in a framework of an abstract hierarchy of biological devices, modules, cells, and multicellular systems. The classical engineering strategies of standardization, decoupling, and abstraction will have to be extended to take into account the inherent characteristics of biological devices and modules. To achieve predictability and reliability, strategies for engineering biology must include the notion of cellular context in the functional definition of devices and modules, use rational redesign and directed evolution for system optimization, and focus on accomplishing tasks using cell populations rather than individual cells. The discussion brings to light issues at the heart of designing complex living systems and provides a trajectory for future development.#N##N#Mol Syst Biol. 2: 2006.0028", 
            "authors": [
                "Ernesto Andrianantoandro", 
                "Subhayu Basu", 
                "David Karig", 
                "Ron Weiss"
            ], 
            "fields": [
                "Synthetic biology", 
                "Directed evolution", 
                "Biological system", 
                "Context-dependent memory", 
                "Protein engineering"
            ], 
            "title": "Synthetic biology: new engineering rules for an emerging discipline", 
            "url": "http://cn.bing.com/academic/profile?id=4940e341869f03e6471c8691f40d68c6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The L3 experiment is one of the six large detectors designed for the new generation of electron-positron accelerators. It is the only detector that concentrates its efforts on limited goals of measuring electrons, muons and photons. By not attempting to identify hadrons, L3 has been able to provide an order of magnitude better resolution for electrons, muons and photons. Vertices and hadron jets are also studied. The construction of L3 has involved much state of the art technology in new principles of vertex detection and in new crystals for large scale electromagnetic shower detection and ultraprecise muon detection. This paper presents a summary of the construction of L3.", 
            "authors": [
                "B Adeva", 
                "M Aguilarbenitez", 
                "H Akbari", 
                "J Alcaraz", 
                "A Aloisio", 
                "J Alvareztaviel", 
                "G Alverson", 
                "M G Alviggi", 
                "H Anderhub", 
                "A L Anderson", 
                "A M Angelov", 
                "T Angelov", 
                "G Antchev", 
                "L Antonov", 
                "D Antreasyan", 
                "A Arefiev", 
                "I Atanasov", 
                "B Auroy", 
                "R Ayad", 
                "O L Ayranov", 
                "T Azemoon", 
                "T Aziz", 
                "U Bachmann", 
                "P Bahler", 
                "J A Bakken", 
                "L Baksay", 
                "H \u2026", 
                ""
            ], 
            "fields": [
                "Bibliography", 
                "Engineering", 
                "Communication", 
                "Physics", 
                "Data acquisition"
            ], 
            "title": "The construction of the L3 experiment", 
            "url": "http://cn.bing.com/academic/profile?id=4255df77d03cb41cdb9244040ef9ac70&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Many-particle charged-particle plasma simulations using spatial meshes for the electromagnetic field solutions, particle-in-cell (PIC) merged with Monte Carlo collision (MCC) calculations, are coming into wide use for application to partially ionized gases. The author emphasizes the development of PIC computer experiments since the 1950s starting with one-dimensional (1-D) charged-sheet models, the addition of the mesh, and fast direct Poisson equation solvers for 2-D and 3-D. Details are provided for adding the collisions between the charged particles and neutral atoms. The result is many-particle simulations with many of the features met in low-temperature collision plasmas; for example, with applications to plasma-assisted materials processing, but also related to warmer plasmas at the edges of magnetized fusion plasmas. >", 
            "authors": [
                "C K Birdsall"
            ], 
            "fields": [
                "Real-time computing", 
                "Computer experiment", 
                "Computational electromagnetics", 
                "Monte Carlo method", 
                "Engineering"
            ], 
            "title": "Particle-in-cell charged-particle simulations, plus Monte Carlo collisions with neutral atoms, PIC-MCC", 
            "url": "http://cn.bing.com/academic/profile?id=b63da1428bc201302928fae8ce18ff8a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Tissue engineering has shown great promise for creating biological alternatives for implants. In this ap- proach, scaffolding plays a pivotal role. Hydroxyapatite mimics the natural bone mineral and has shown good bone- bonding properties. This paper describes the preparation and morphologies of three-dimensional porous composites from poly(L-lactic acid) (PLLA) or poly(D,L-lactic acid-co- glycolic acid) (PLGA) solution and hydroxyapatite (HAP). A thermally induced phase separation technique was used to create the highly porous composite scaffolds for bone-tissue engineering. Freeze drying of the phase-separated polymer/ HAP/solvent mixtures produced hard and tough foams with a co-continuous structure of interconnected pores and a polymer/HAP composite skeleton. The microstructure of the pores and the walls was controlled by varying the poly- mer concentration, HAP content, quenching temperature, polymer, and solvent utilized. The porosity increased with decreasing polymer concentration and HAP content. Foams with porosity as high as 95% were achieved. Pore sizes rang- ing from several microns to a few hundred microns were obtained. The composite foams showed a significant im- provement in mechanical properties over pure polymer foams. They are promising scaffolds for bone-tissue engi- neering. \u00a9 1999 John Wiley & Sons, Inc. J Biomed Mater Res, 44, 446-455, 1999.", 
            "authors": [
                "Ruiyun Zhang", 
                "Peter X Ma"
            ], 
            "fields": [
                "Three-dimensional space", 
                "Biological engineering", 
                "Biomedical Sciences", 
                "Composite material", 
                "Composite number"
            ], 
            "title": "Poly(\u03b1-hydroxyl acids)/hydroxyapatite porous composites for bone-tissue engineering. I. Preparation and morphology", 
            "url": "http://cn.bing.com/academic/profile?id=3e2a17d19cfa400f95a3aa4bf6ec3751&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Spectroscopic measurements and laser performance of Ti:Al2O3 are discussed in detail. Data on absorption and fluorescence spectra and fluorescence lifetime as a function of temperature are presented. Laser characteristics observed with pulsed-dye-laser, frequency-doubled Nd:YAG-laser, and argon-ion-laser pumping are covered and show that nearly quantum-limited conversion of pump radiation can be achieved, along with tuning over the wavelength range 660\u2013986 nm.", 
            "authors": [
                "Peter F Moulton"
            ], 
            "fields": [
                "Luminescence", 
                "Stimulated emission", 
                "Dye laser", 
                "Temperature", 
                "Oxide minerals"
            ], 
            "title": "Spectroscopic and laser characteristics of Ti:Al 2 O 3", 
            "url": "http://cn.bing.com/academic/profile?id=4ecbde870b29ce7f467534291dcf18df&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The present invention provides stretchable, and optionally printable, semiconductors and electronic circuits capable of providing good performance when stretched, compressed, flexed or otherwise deformed. Stretchable semiconductors and electronic circuits of the present invention preferred for some applications are flexible, in addition to being stretchable, and thus are capable of significant elongation, flexing, bending or other deformation along one or more axes. Further, stretchable semiconductors and electronic circuits of the present invention may be adapted to a wide range of device configurations to provide fully flexible electronic and optoelectronic devices.", 
            "authors": [
                "John A Rogers", 
                "Dahlyoung Khang", 
                "Yugang Sun", 
                "Etienne Menard"
            ], 
            "fields": [
                "Engineering", 
                "Deformation", 
                "International System of Units", 
                "Diode", 
                "p\u2013n junction"
            ], 
            "title": "Stretchable Form of Single Crystal Silicon for High Performance Electronics on Rubber Substrates", 
            "url": "http://cn.bing.com/academic/profile?id=7469df5af691f35b03aed7eb090f7c30&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N#The theory of flow through fractured rock and homogeneous anisotropic porous media is used to determine when a fractured rock behaves as a continuum. A fractured rock can be said to behave like an equivalent porous medium when (1) there is an insignificant change in the value of the equivalent permeability with a small addition or subtraction to the test volume and (2) an equivalent permeability tensor exists which predicts the correct flux when the direction of a constant gradient is changed. Field studies of fracture geometry are reviewed and a realistic, two-dimensional fracture system model is developed. The shape, size, orientation, and location of fractures in an impermeable matrix are random variables in the model. These variables are randomly distributed according to field data currently available in the literature. The fracture system models are subjected to simulated flow tests. The results of the flow tests are plotted as permeability \u2018ellipses.\u2019 The size and shape of these permeability ellipses show that fractured rock does not always behave as a homogeneous, anisotropic porous medium with a symmetric permeability tensor. Fracture systems behave more like porous media when (1) fracture density is increased, (2) apertures are constant rather than distributed, (3) orientations are distributed rather than constant, and (4) larger sample sizes are tested. Preliminary results indicate the use of this new tool, when perfected, will greatly enhance our ability to analyze field data on fractured rock systems. The tool can be used to distinguish between fractured systems which can be treated as porous media and fractured systems which must be treated as a collection of discrete fracture flow paths.", 
            "authors": [
                "J C S Long", 
                "J S Remer", 
                "C R Wilson", 
                "P A Witherspoon"
            ], 
            "fields": [
                "Field research", 
                "Systems modeling", 
                "Permeability", 
                "Porosity", 
                "Random variable"
            ], 
            "title": "Porous media equivalents for networks of discontinuous fractures", 
            "url": "http://cn.bing.com/academic/profile?id=a7db738495e053fe00d8ce9d1299d27c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We introduce a new numerical method, called \"SURFER,\" for the simulation of two- and three-dimensional flows with several fluid phases and free interfaces between them. We consider incompressible fluids obeying the Navier-Stokes equation with Newtonian viscosity in the bulk of each phase. Capillary forces are taken into account even when interfaces merge or break up. Fluid interfaces are advanced in time using an exactly volume conserving variant of the volume of fluid algorithm, thus allowing for full symmetry between fluid phases. The Navier-Stokes equation is solved using staggered finite differences on a MAC grid and a split-explicit time differencing scheme, while incompressibility is enforced using an iterative multigrid Poisson solver. Capillary effects are represented as a stress tensor computed from gradients of the volume fraction function. This formulation is completely independent of the topology of interfaces and relatively easy to implement in 3D. It also allows exact momentum conservation in the discretized algorithm. Numerical spurious effects or \"parasite currents\" are noticed and compared to similar effects in Boltzmann lattice gas methods for immiscible fluids. Simulations of droplets pairs colliding in 2D and in 3D are shown. Interface reconnection is performed easily, despite the large value of capillary forces during reconnection.", 
            "authors": [
                "Bruno Lafaurie", 
                "Carlo Nardone", 
                "Ruben Scardovelli", 
                "Stephane Zaleski", 
                "Gianluigi Zanetti"
            ], 
            "fields": [
                "Volume of fluid method", 
                "Engineering", 
                "Upwind scheme", 
                "Compressibility", 
                "Surface tension"
            ], 
            "title": "Modelling Merging and Fragmentation in Multiphase Flows with SURFER", 
            "url": "http://cn.bing.com/academic/profile?id=f48ce46b85b051321f080dbfb5636a75&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Paralysis following spinal cord injury, brainstemstroke, amyotrophic#R##N#lateral sclerosis and other disorders can disconnect the brain from the#R##N#body, eliminating the ability to perform volitional movements. A#R##N#neural interface system could restore mobility and independence#R##N#for people with paralysis by translating neuronal activity directly into#R##N#control signals for assistive devices. We have previously shown that#R##N#people with long-standing tetraplegia can use a neural interface#R##N#system to move and click a computer cursor and to control physical#R##N#devices. Able-bodied monkeys have used a neural interface system#R##N#to control a robotic arm, but it is unknown whether people with#R##N#profound upper extremity paralysis or limb loss could use cortical#R##N#neuronal ensemble signals to direct useful arm actions. Here we#R##N#demonstrate the ability of two people with long-standing tetraplegia#R##N#to use neural interface system-based control of a robotic arm to#R##N#perform three-dimensional reach and graspmovements. Participants#R##N#controlled the arm and hand over a broad space without explicit#R##N#training, using signals decoded from a small, local population of#R##N#motor cortex (MI) neurons recorded from a 96-channel microelectrode#R##N#array. One of the study participants, implanted with the#R##N#sensor 5 years earlier, also used a robotic arm to drink coffee from a#R##N#bottle. Although robotic reach and grasp actions were not as fast or#R##N#accurate as those of an able-bodied person, our results demonstrate#R##N#the feasibility for people with tetraplegia, years after injury to the#R##N#central nervous system, to recreate useful multidimensional control#R##N#of complex devices directly from a small sample of neural signals.", 
            "authors": [
                "Leigh R Hochberg", 
                "Daniel Bacher", 
                "Beata Jarosiewicz", 
                "Nicolas Y Masse", 
                "John D Simeral", 
                "Joern Vogel", 
                "Sami Haddadin", 
                "Jie Liu", 
                "Sydney S Cash", 
                "Patrick Van Der Smagt", 
                "John P Donoghue"
            ], 
            "fields": [
                "ARM architecture", 
                "Microelectrode", 
                "Calibration", 
                "Movement", 
                "Technology"
            ], 
            "title": "Reach and grasp by people with tetraplegia using a neurally controlled robotic arm", 
            "url": "http://cn.bing.com/academic/profile?id=004cd6976b08989e09f36df555f818e7&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Biodegradable polymers have been widely used as scaffolding materials to regenerate new tissues. To mimic natural extracellular matrix architecture, a novel highly po- rous structure, which is a three-dimensional interconnected fibrous network with a fiber diameter ranging from 50 to 500 nm, has been created from biodegradable aliphatic polyes- ters in this work. A porosity as high as 98.5% has been achieved. These nano-fibrous matrices were prepared from the polymer solutions by a procedure involving thermally induced gelation, solvent exchange, and freeze-drying. The effects of polymer concentration, thermal annealing, solvent exchange, and freezing temperature before freeze-drying on the nano-scale structures were studied. In general, at a high gelation temperature, a platelet-like structure was formed. At a low gelation temperature, the nano-fibrous structure was formed. Under the conditions for nano-fibrous matrix formation, the average fiber diameter (160-170 nm) did not change statistically with polymer concentration or gelation temperature. The porosity decreased with polymer concen- tration. The mechanical properties (Young's modulus and tensile strength) increased with polymer concentration. A surface-to-volume ratio of the nano-fibrous matrices was two to three orders of magnitude higher than those of fi- brous nonwoven fabrics fabricated with the textile technol- ogy or foams fabricated with a particulate-leaching tech- nique. This synthetic analogue of natural extracellular ma- trix combined the advantages of synthetic biodegradable polymers and the nano-scale architecture of extracellular matrix, and may provide a better environment for cell at- tachment and function. \u00a9 1999 John Wiley & Sons, Inc. J Biomed Mater Res, 46, 60-72, 1999.", 
            "authors": [
                "Peter X Ma", 
                "Ruiyun Zhang"
            ], 
            "fields": [
                "Biomedical Sciences", 
                "Biodegradation", 
                "Morphology", 
                "Tissue engineering", 
                "Biological engineering"
            ], 
            "title": "Synthetic nano-scale fibrous extracellular matrix.", 
            "url": "http://cn.bing.com/academic/profile?id=6622b51de6742da4578023d8e8f8ed19&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Mesenchymal stem cell-mediated tissue regeneration is a promising approach for regenerative medicine for a wide range of applications. Here we report a new population of stem cells isolated from the root apical papilla of human teeth (SCAP, stem cells from apical papilla). Using a minipig model, we transplanted both human SCAP and periodontal ligament stem cells (PDLSCs) to generate a root/periodontal complex capable of supporting a porcelain crown, resulting in normal tooth function. This work integrates a stem cell-mediated tissue regeneration strategy, engineered materials for structure, and current dental crown technologies. This hybridized tissue engineering approach led to recovery of tooth strength and appearance.", 
            "authors": [
                "Wataru Sonoyama", 
                "Yi Liu", 
                "Dianji Fang", 
                "Takayoshi Yamaza", 
                "Byoungmoo Seo", 
                "Chunmei Zhang", 
                "He Liu", 
                "Stan Gronthos", 
                "Cunyu Wang", 
                "Songtao Shi", 
                "Songlin Wang"
            ], 
            "fields": [
                "Regenerative medicine", 
                "Periodontal fiber", 
                "Stem cell", 
                "Engineering", 
                "Tissue engineering"
            ], 
            "title": "Mesenchymal Stem Cell-Mediated Functional Tooth Regeneration in Swine", 
            "url": "http://cn.bing.com/academic/profile?id=3d19fcefa31237ca23c908072ea2adfc&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background. Worldwide, grapes and their derived products have a large market. The cultivated grape species Vitis vinifera has potential to become a model for fruit trees genetics. Like many plant species, it is highly heterozygous, which is an additional challenge to modern whole genome shotgun sequencing. In this paper a high quality draft genome sequence of a cultivated clone of V. vinifera Pinot Noir is presented. Principal Findings. We estimate the genome size of V. vinifera to be 504.6 Mb. Genomic sequences corresponding to 477.1 Mb were assembled in 2,093 metacontigs and 435.1 Mb were anchored to the 19 linkage groups (LGs). The number of predicted genes is 29,585, of which 96.1% were assigned to LGs. This assembly of the grape genome provides candidate genes implicated in traits relevant to grapevine cultivation, such as those influencing wine quality, via secondary metabolites, and those connected with the extreme susceptibility of grape to pathogens. Single nucleotide polymorphism (SNP) distribution was consistent with a diffuse haplotype structure across the genome. Of around 2,000,000 SNPs, 1,751,176 were mapped to chromosomes and one or more of them were identified in 86.7% of anchored genes. The relative age of grape duplicated genes was estimated and this made possible to reveal a relatively recent Vitisspecific large scale duplication event concerning at least 10 chromosomes (duplication not reported before). Conclusions. Sanger shotgun sequencing and highly efficient sequencing by synthesis (SBS), together with dedicated assembly programs, resolved a complex heterozygous genome. A consensus sequence of the genome and a set of mapped marker loci were generated. Homologous chromosomes of Pinot Noir differ by 11.2% of their DNA (hemizygous DNA plus chromosomal gaps). SNP markers are offered as a tool with the potential of introducing a new era in the molecular breeding of grape.", 
            "authors": [
                "Riccardo Velasco", 
                "Andrey Zharkikh", 
                "Michela Troggio", 
                "Dustin A Cartwright", 
                "Alessandro Cestaro", 
                "Dmitry Pruss", 
                "Massimo Pindo", 
                "Lisa M Fitzgerald", 
                "Silvia Vezzulli", 
                "Julia Reid", 
                "Giulia Malacarne", 
                "Diana Iliev", 
                "Giuseppina Coppola", 
                "Bryan Wardell", 
                "Diego Micheletti", 
                "Teresita Macalma", 
                "Marco Facci", 
                "Jeff T Mitchell", 
                "Michele Per\u2026", 
                ""
            ], 
            "fields": [
                "Heterozygote advantage", 
                "Physics", 
                "Shotgun sequencing", 
                "Single-nucleotide polymorphism", 
                "Sequence alignment"
            ], 
            "title": "A High Quality Draft Consensus Sequence of the Genome of a Heterozygous Grapevine Variety", 
            "url": "http://cn.bing.com/academic/profile?id=c9025ef391659c6601e77dfd7a570de2&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The factors that make speech recognition difficult are examined, and the potential of neural computers for this purpose is discussed. A speaker-adaptive system that transcribes dictation using an unlimited vocabulary is presented that is based on a neural network processor for the recognition of phonetic units of speech. The acoustic preprocessing, vector quantization, neural network model, and shortcut learning algorithm used are described. The utilization of phonotopic maps and of postprocessing in symbolic forms are discussed. Hardware implementations and performance of the neural networks are considered. >", 
            "authors": [
                "Teuvo Kohonen"
            ], 
            "fields": [
                "Natural language", 
                "Speech recognition", 
                "Artificial intelligence", 
                "Hidden Markov model", 
                "Pattern recognition"
            ], 
            "title": "The 'neural' phonetic typewriter", 
            "url": "http://cn.bing.com/academic/profile?id=fe17c1aa41ac7987c40aed4802c065c6&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A description is given of the research progress in developing a vertical-cavity surface-emitting (SE) injection laser based on GaAlAs/GaAs and GaInAsP/InP systems. Ultimate laser characteristics, device design, state-of-the-art performances, possible device improvement, and future prospects will also be discussed. The authors propose a vertical-cavity surface emitting semiconductor laser. To reduce the threshold current, they improved the laser reflector and introduced a circular buried heterostructure. The microcavity structure, which is 7 mu m long and 6 mu m in diameter, was realized with a threshold of 6 mA. Thus, possibilities of an extremely low threshold current SE laser device and a densely packed two-dimensional array are suggested. >", 
            "authors": [
                "Kenichi Iga", 
                "Fumio Koyama", 
                "Susumu Kinoshita"
            ], 
            "fields": [
                "Materials Science", 
                "Design", 
                "Engineering", 
                "Ocean current", 
                "Electric current"
            ], 
            "title": "Surface emitting semiconductor lasers", 
            "url": "http://cn.bing.com/academic/profile?id=d8de1cb7b37a140888744bdc1db416e3&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "As next-generation sequencing projects generate massive genome-wide sequence variation data, bioinformatics tools are being developed to provide computational predictions on the functional effects of sequence variations and narrow down the search of casual variants for disease phenotypes. Different classes of sequence variations at the nucleotide level are involved in human diseases, including substitutions, insertions, deletions, frameshifts, and non-sense mutations. Frameshifts and non-sense mutations are likely to cause a negative effect on protein function. Existing prediction tools primarily focus on studying the deleterious effects of single amino acid substitutions through examining amino acid conservation at the position of interest among related sequences, an approach that is not directly applicable to insertions or deletions. Here, we introduce a versatile alignment-based score as a new metric to predict the damaging effects of variations not limited to single amino acid substitutions but also in-frame insertions, deletions, and multiple amino acid substitutions. This alignment-based score measures the change in sequence similarity of a query sequence to a protein sequence homolog before and after the introduction of an amino acid variation to the query sequence. Our results showed that the scoring scheme performs well in separating disease-associated variants (n=21,662) from common polymorphisms (n=37,022) for UniProt human protein variations, and also in separating deleterious variants (n=15,179) from neutral variants (n=17,891) for UniProt non-human protein variations. In our approach, the area under the receiver operating characteristic curve (AUC) for the human and non-human protein variation datasets is ,0.85. We also observed that the alignment-based score correlates with the deleteriousness of a sequence variation. In summary, we have developed a new algorithm, PROVEAN (Protein Variation Effect Analyzer), which provides a generalized approach to predict the functional effects of protein sequence variations including single or multiple amino acid substitutions, and in-frame insertions and deletions. The PROVEAN tool is available online at http://provean.jcvi.org.", 
            "authors": [
                "Yongwook Choi", 
                "Gregory E Sims", 
                "Sean Murphy", 
                "Jason R Miller", 
                "Agnes P Chan"
            ], 
            "fields": [
                "Computational biology", 
                "Engineering", 
                "Physics", 
                "Medicine", 
                "Protein sequencing"
            ], 
            "title": "Predicting the functional effect of amino acid substitutions and indels.", 
            "url": "http://cn.bing.com/academic/profile?id=a48db16e8ffb29bb5f6c61e1441d627a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Large catchment basins may be viewed as ecosystems in which natural and cultural attributes interact. Contemporary river ecology emphasizes the four-dimensional nature of the river continuum and the propensity for riverine biodiversity and bioproduction to be largely controlled by habitat maintenance processes, such as cut and fill alluviation mediated by catchment water yield. Stream regulation reduces annual flow amplitude, increases baseflow variation and changes temperature, mass transport and other important biophysical patterns and attributes. As a result, ecological connectivity between upstream and downstream reaches and between channels, ground waters and floodplains may be severed. Native biodiversity and bioproduction usually are reduced or changed and non-native biota proliferate. Regulated rivers regain normative attributes as distance from the dam increases and in relation to the mode of dam operation. Therefore, dam operations can be used to restructure altered temperature and flow regimes which, coupled with pollution abatement and management of non-native biota, enables natural processes to restore damaged habitats along the river\u2019s course. The expectation is recovery of depressed populations of native species. The protocol requires: restoring peak flows needed to reconnect and periodically reconfigure channel and floodplain habitats; stabilizing baseflows to revitalize food-webs in shallow water habitats; reconstituting seasonal temperature patterns (e.g. by construction of depth selective withdrawal systems on storage dams); maximizing dam passage to allow recovery of fish metapopulation structure; instituting a management belief system that relies upon natural habitat restoration and maintenance, as opposed to artificial propagation, installation of artificial instream structures (river engineering) and predator control; and, practising adaptive ecosystem management. Our restoration protocol should be viewed as an hypothesis derived from the principles of river ecology. Although restoration to aboriginal state is not expected, nor necessarily desired, recovering some large portion of the lost capacity to sustain native biodiversity and bioproduction is possible by management for processes that maintain normative habitat conditions. The cost may be less than expected because the river can do most of the work.", 
            "authors": [
                "Jack A Stanford", 
                "John Ward", 
                "William J Liss", 
                "Christopher A Frissell", 
                "Richard N Williams", 
                "James A Lichatowich", 
                "Charles C Coutant"
            ], 
            "fields": [
                "Ecosystem", 
                "Food web", 
                "Mass transfer", 
                "Habitat", 
                "Waves and shallow water"
            ], 
            "title": "A General Protocol for Restoration of Regulated Rivers", 
            "url": "http://cn.bing.com/academic/profile?id=8612a0eba4e96293dd62b8db9f0d485d&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We present a result applicable to classification learning algorithms that generate decision trees or rules using the information entropy minimization heuristic for discretizing continuous-valued attributes. The result serves to give a better understanding of the entropy measure, to point out that the behavior of the information entropy heuristic possesses desirable properties that justify its usage in a formal sense, and to improve the efficiency of evaluating continuous-valued attributes for cut value selection. Along with the formal proof, we present empirical results that demonstrate the theoretically expected reduction in evaluation effort for training data sets from real-world domains.", 
            "authors": [
                "Usama Fayyad", 
                "Keki B Irani"
            ], 
            "fields": [
                "Engineering", 
                "Discretization", 
                "Computer Science", 
                "Decision tree", 
                "Biological classification"
            ], 
            "title": "On the Handling of Continuous-Valued Attributes in Decision Tree Generation", 
            "url": "http://cn.bing.com/academic/profile?id=7b9e66874deaf7e8106e5a0411115798&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "An LQG (linear quadratic Gaussian) control-design problem involving a constraint on H/sup infinity / disturbance attenuation is considered. The H/sup infinity / performance constraint is embedded within the optimization process by replacing the covariance Lyapunov equation by a Riccati equation whose solution leads to an upper bound on L/sub 2/ performance. In contrast to the pair of separated Riccati equations of standard LQG theory, the H/sup infinity /-constrained gains are given by a coupled system of three modified Riccati equations. The coupling illustrates the breakdown of the separation principle for the H/sup infinity /-constrained problem. Both full- and reduced-order design problems are considered with an H/sup infinity / attenuation constraint involving both state and control variables. An algorithm is developed for the full-order design problem and illustrative numerical results are given. >", 
            "authors": [
                "D S Bernstein", 
                "W M Haddad"
            ], 
            "fields": [
                "Riccati equation", 
                "Engineering", 
                "Optimal control", 
                "Aerospace Engineering"
            ], 
            "title": "LQG control with an H/sup infinity / performance bound: a Riccati equation approach", 
            "url": "http://cn.bing.com/academic/profile?id=c76f545fa33ae5b03a51664c89288624&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The growth of a radiation pulse traversing a medium with an inverted population is described by nonlinear, time\u2010dependent photon transport equations, which account for the effect of the radiation on the medium as well as vice versa. The equations are solved in closed form for an arbitrary input pulse and an arbitrary initial distribution of inverted population. The solutions are discussed in detail for the particular cases of a square pulse and a Lorentzian pulse, both with a uniform initial population inversion.", 
            "authors": [
                "Lee M Frantz", 
                "John S Nodvik"
            ], 
            "fields": [
                "Laser", 
                "Amplifier", 
                "Convection\u2013diffusion equation", 
                "Operational amplifier", 
                "Engineering"
            ], 
            "title": "Theory of Pulse Propagation in a Laser Amplifier", 
            "url": "http://cn.bing.com/academic/profile?id=c3e766d34c8e833f4150e52bca5d9f40&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Background. A useful DNA barcode requires sufficient sequence variation to distinguish between species and ease of application across a broad range of taxa. Discovery of a DNA barcode for land plants has been limited by intrinsically lower rates of sequence evolution in plant genomes than that observed in animals. This low rate has complicated the trade-off in finding a locus that is universal and readily sequenced and has sufficiently high sequence divergence at the species-level. Methodology/Principal Findings. Here, a global plant DNA barcode system is evaluated by comparing universal application and degree of sequence divergence for nine putative barcode loci, including coding and non-coding regions, singly and in pairs across a phylogenetically diverse set of 48 genera (two species per genus). No single locus could discriminate among species in a pair in more than 79% of genera, whereas discrimination increased to nearly 88% when the non-coding trnH-psbA spacer was paired with one of three coding loci, including rbcL. In silico trials were conducted in which DNA sequences from GenBank were used to further evaluate the discriminatory power of a subset of these loci. These trials supported the earlier observation that trnH-psbA coupled with rbcL can correctly identify and discriminate among related species. Conclusions/ Significance. A combination of the non-coding trnH-psbA spacer region and a portion of the coding rbcL gene is recommended as a two-locus global land plant barcode that provides the necessary universality and species discrimination. Citation: Kress WJ, Erickson DL (2007) A Two-Locus Global DNA Barcode for Land Plants: The Coding rbcL Gene Complements the Non-Coding trnH", 
            "authors": [
                "W John Kress", 
                "David L Erickson"
            ], 
            "fields": [
                "Engineering", 
                "Plastid", 
                "Sequence analysis", 
                "Biology", 
                "Sequence alignment"
            ], 
            "title": "A Two-Locus Global DNA Barcode for Land Plants: The Coding rbcL Gene Complements the Non-Coding trnH-psbA Spacer Region", 
            "url": "http://cn.bing.com/academic/profile?id=4659c56dd2e7e017f4a07046838cad45&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "BACKGROUND: The increased use of meta-analysis in systematic reviews of healthcare interventions has highlighted several types of bias that can arise during the completion of a randomised controlled trial. Study publication bias has been recognised as a potential threat to the validity of meta-analysis and can make the readily available evidence unreliable for decision making. Until recently, outcome reporting bias has received less attention. METHODOLOGY/PRINCIPAL FINDINGS: We review and summarise the evidence from a series of cohort studies that have assessed study publication bias and outcome reporting bias in randomised controlled trials. Sixteen studies were eligible of which only two followed the cohort all the way through from protocol approval to information regarding publication of outcomes. Eleven of the studies investigated study publication bias and five investigated outcome reporting bias. Three studies have found that statistically significant outcomes had a higher odds of being fully reported compared to non-significant outcomes (range of odds ratios: 2.2 to 4.7). In comparing trial publications to protocols, we found that 40-62% of studies had at least one primary outcome that was changed, introduced, or omitted. We decided not to undertake meta-analysis due to the differences between studies. CONCLUSIONS: Recent work provides direct empirical evidence for the existence of study publication bias and outcome reporting bias. There is strong evidence of an association between significant results and publication; studies that report positive or significant results are more likely to be published and outcomes that are statistically significant have higher odds of being fully reported. Publications have been found to be inconsistent with their protocols. Researchers need to be aware of the problems of both types of bias and efforts should be concentrated on improving the reporting of trials.", 
            "authors": [
                "Kerry Dwan", 
                "Douglas G Altman", 
                "Juan A Arnaiz", 
                "Jill Myerow Bloom", 
                "Anwen Chan", 
                "Eugenia Cronin", 
                "Evelyne Decullier", 
                "Philippa Easterbrook", 
                "Erik Von Elm", 
                "Carrol Gamble", 
                "Davina Ghersi", 
                "John P A Ioannidis", 
                "John Simes", 
                "Paula R Williamson"
            ], 
            "fields": [
                "Odds ratio", 
                "Systematic review", 
                "Medicine", 
                "Randomized controlled trial", 
                "Biology"
            ], 
            "title": "Systematic review of the empirical evidence of study publication bias and outcome reporting bias.", 
            "url": "http://cn.bing.com/academic/profile?id=38161e5d74afd49c0ea769a9ff4036f0&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#This paper describes a distributed coordination scheme with local information exchange for multiple vehicle systems. We introduce second-order consensus protocols that take into account motions of the information states and their derivatives, extending first-order protocols from the literature. We also derive necessary and sufficient conditions under which consensus can be reached in the context of unidirectional information exchange topologies. This work takes into account the general case where information flow may be unidirectional due to sensors with limited fields of view or vehicles with directed, power-constrained communication links. Unlike the first-order case, we show that having a (directed) spanning tree is a necessary rather than a sufficient condition for consensus seeking with second-order dynamics. This work focuses on a formal analysis of information exchange topologies that permit second-order consensus. Given its importance to the stability of the coordinated system, an analysis of the consensus term control gains is also presented, specifically the strength of the information states relative to their derivatives. As an illustrative example, consensus protocols are applied to coordinate the movements of multiple mobile robots. Copyright \u00a9 2006 John Wiley & Sons, Ltd.", 
            "authors": [
                "Wei Ren", 
                "Ella M Atkins"
            ], 
            "fields": [
                "Information exchange", 
                "Engineering", 
                "Mechanical Engineering", 
                "Graph theory"
            ], 
            "title": "Distributed multi\u2010vehicle coordinated control via local information exchange", 
            "url": "http://cn.bing.com/academic/profile?id=f3314ad721e92d9859376fd89c8109fe&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper examines information system evolution in small firms. It focuses on applications growth, and uses the experiences of six small manufacturing firms to identify motivators and inhibitors of growth. Many factors were identified. Motivators of growth included improved enthusiasm for the technology. Inadequate resources and limited education about information systems were among the factors that inhibited application growth.", 
            "authors": [
                "Paul B Cragg", 
                "Malcolm King"
            ], 
            "fields": [
                "Engineering", 
                "Information system", 
                "Manufacturing", 
                "Motivation"
            ], 
            "title": "Small-firm computing: motivators and inhibitors", 
            "url": "http://cn.bing.com/academic/profile?id=7c1abcafcc844c72f69074a4f2ff964a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Summary New empirical scoring functions have been developed to estimate the binding affinity of a given protein-ligand complex with known three-dimensional structure. These scoring functions include terms accounting for van der Waals interaction, hydrogen bonding, deformation penalty, and hydrophobic effect. A special feature is that three different algorithms have been implemented to calculate the hydrophobic effect term, which results in three parallel scoring functions. All three scoring functions are calibrated through multivariate regression analysis of a set of 200 protein-ligand complexes and they reproduce the binding free energies of the entire training set with standard deviations of 2.2 kcal/mol, 2.1 kcal/mol, and 2.0 kcal/mol, respectively. These three scoring functions are further combined into a consensus scoring function, X-CSCORE. When tested on an independent set of 30 protein-ligand complexes, X-CSCORE is able to predict their binding free energies with a standard deviation of 2.2 kcal/mol. The potential application of X-CSCORE to molecular docking is also investigated. Our results show that this consensus scoring function improves the docking accuracy considerably when compared to the conventional force field computation used for molecular docking.", 
            "authors": [
                "Renxiao Wang", 
                "Luhua Lai", 
                "Shaomeng Wang"
            ], 
            "fields": [
                "Docking", 
                "Score", 
                "Force field", 
                "Standard deviation", 
                "Hydrogen bond"
            ], 
            "title": "Further development and validation of empirical scoring functions for structure-based binding affinity prediction", 
            "url": "http://cn.bing.com/academic/profile?id=5f7785808e3961f3973eaf3d7a7876c4&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We describe a two-step algorithm for estimating dynamic games under the assumption that behavior is consistent with Markov perfect equilibrium. In the first step, the policy functions and the law of motion for the state variables are estimated. In the second step, the remaining structural parameters are estimated using the optimality conditions for equilibrium. The second step estimator is a simple simulated minimum distance estimator. The algorithm applies to a broad class of models, including industry competition models with both discrete and continuous controls such as the Ericson and Pakes (1995) model. We test the algorithm on a class of dynamic discrete choice models with normally distributed errors and a class of dynamic oligopoly models similar to that of Pakes and McGuire (1994). Copyright The Econometric Society 2007.", 
            "authors": [
                "Patrick Bajari", 
                "C Lanier Benkard", 
                "Jonathan Levin"
            ], 
            "fields": [
                "Sequential game", 
                "Algorithm", 
                "Developing country", 
                "Step function", 
                "Discrete choice"
            ], 
            "title": "Estimating Dynamic Models of Imperfect Competition", 
            "url": "http://cn.bing.com/academic/profile?id=aa256c26daabcec32f7f4c3085f412cf&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#Engineering tissues utilizing biodegradable polymer matrices is a promising approach to the treatment of a number of diseases. However, processing techniques utilized to fabricate these matrices typically involve organic solvents and/or high temperatures. Here we describe a process for fabricating matrices without the use of organic solvents and/or elevated temperatures. Disks comprised of polymer [e.g., poly (D,L-lactic-co-glycolic acid)] and NaCl particles were compression molded at room temperature and subsequently allowed to equilibrate with high pressure CO2 gas (800 psi). Creation of a thermodynamic instability led to the nucleation and growth of gas pores in the polymer particles, resulting in the expansion of the polymer particles. The polymer particles fused to form a continuous matrix with entrapped salt particles. The NaCl particles subsequently were leached to yield macropores within the polymer matrix. The overall porosity and level of pore connectivity were regulated by the ratio of polymer/salt particles and the size of salt particles. Both the compressive modulus (159 \u00b1 130 kPa versus 289 \u00b1 25 kPa) and the tensile modulus (334 \u00b1 52 kPa versus 1100 \u00b1 236 kPa) of the matrices formed with this approach were significantly greater than those formed with a standard solvent casting/particulate leaching process. The utility of these matrices was demonstrated by engineering smooth muscle tissue in vitro with them. This novel process, a combination of high pressure gas foaming and particulate leaching techniques, allows one to fabricate matrices with a well controlled porosity and pore structure. This process avoids the potential negatives associated with the use of high temperatures and/or organic solvents in biomaterials processing. \u00a9 1998 John Wiley & Sons, Inc. J Biomed Mater Res, 42, 396\u2013402, 1998.", 
            "authors": [
                "Leatrese Harris", 
                "Byungsoo Kim", 
                "David J Mooney"
            ], 
            "fields": [
                "Biocompatible material", 
                "Biomedical Sciences", 
                "Biological engineering", 
                "Biochemistry", 
                "Carbon dioxide"
            ], 
            "title": "Open pore biodegradable matrices formed with gas foaming", 
            "url": "http://cn.bing.com/academic/profile?id=7746262a502f3bdda170843d17d8d5d2&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The emission and absorption properties of numerous host crystals doped with Yb/sup 3+/ ions have been studied. The hosts which have been selected include LiYF/sub 4/, LaF/sub 3/, SrF/sub 2/, BaF/sub 2/, KCaF/sub 3/, KY/sub 3/F/sub 10/, Rb/sub 2/NaYF/sub 6/, BaY/sub 2/F/sub 8/, Y/sub 2/SiO/sub 5/, Y/sub 3/Al/sub 5/O/sub 12/, YAlO/sub 3/, LuPO/sub 4/, Ca/sub 5/(PO/sub 4/)/sub 3/F, LiYO/sub 2/, and ScBO/sub 3/. Spectral determinations have been made of the resonant absorption and emission cross sections between 850 and 1100 nm, and the emission decay times of the upper laser level have been measured. The emission cross sections have been evaluated using the absorption cross section and principle of reciprocity, and again using the Fuchtbauer-Ladenberg formula. Agreement between the two methods is within 20% for most materials. The results are discussed in the framework of requirements for an effective diode-pumped Yb/sup 3+/ laser system. Ca/sub 5/(PO/sub 4/)/sub 3/F:Yb is predicted to exhibit the most useful laser properties and is expected to be far superior to Y/sub 3/Al/sub 5/O/sub 12/:Yb in many key microscopic parameter values. >", 
            "authors": [
                "L D Deloach", 
                "Stephen A Payne", 
                "L L Chase", 
                "L K Smith", 
                "Wayne L Kway", 
                "W F Krupke"
            ], 
            "fields": [
                "Cross section", 
                "Doping", 
                "Room temperature", 
                "Emission spectrum", 
                "Crystal"
            ], 
            "title": "Evaluation of absorption and emission properties of Yb/sup 3+/ doped crystals for laser applications", 
            "url": "http://cn.bing.com/academic/profile?id=b21294f8bb1c4ffc68faf5b6aa146266&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The emission and absorption properties of numerous host crystals doped with Yb/sup 3+/ ions have been studied. The hosts which have been selected include LiYF/sub 4/, LaF/sub 3/, SrF/sub 2/, BaF/sub 2/, KCaF/sub 3/, KY/sub 3/F/sub 10/, Rb/sub 2/NaYF/sub 6/, BaY/sub 2/F/sub 8/, Y/sub 2/SiO/sub 5/, Y/sub 3/Al/sub 5/O/sub 12/, YAlO/sub 3/, LuPO/sub 4/, Ca/sub 5/(PO/sub 4/)/sub 3/F, LiYO/sub 2/, and ScBO/sub 3/. Spectral determinations have been made of the resonant absorption and emission cross sections between 850 and 1100 nm, and the emission decay times of the upper laser level have been measured. The emission cross sections have been evaluated using the absorption cross section and principle of reciprocity, and again using the Fuchtbauer-Ladenberg formula. Agreement between the two methods is within 20% for most materials. The results are discussed in the framework of requirements for an effective diode-pumped Yb/sup 3+/ laser system. Ca/sub 5/(PO/sub 4/)/sub 3/F:Yb is predicted to exhibit the most useful laser properties and is expected to be far superior to Y/sub 3/Al/sub 5/O/sub 12/:Yb in many key microscopic parameter values. >", 
            "authors": [
                "L D Deloach", 
                "Stephen A Payne", 
                "L L Chase", 
                "L K Smith", 
                "Wayne L Kway", 
                "W F Krupke"
            ], 
            "fields": [
                "Cross section", 
                "Doping", 
                "Room temperature", 
                "Emission spectrum", 
                "Crystal"
            ], 
            "title": "Evaluation of absorption and emission properties of Yb/sup 3+/ doped crystals for laser applications", 
            "url": "http://cn.bing.com/academic/profile?id=b21294f8bb1c4ffc68faf5b6aa146266&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Models and scientific evidence suggest that firms are more successful at new-product development if there is greater communication among marketing, engineering, and manufacturing. This paper examines communication patterns for two matched product-development teams where the key difference between the groups is that one used a phase-review development process and the other used Quality Function Deployment QFD, a product-development process adopted recently at over 100 United States and Japanese firms. To our knowledge, this is the first head-to-head comparison of traditional U.S. product development processes with QFD.#R##N##R##N#Our data suggest that QFD enhances communication levels within the core team marketing, engineering, manufacturing. QFD changes communication patterns from \"up-over-down\" flows through management to more horizontal routes where core team members communicate directly with one another. On the other hand, the QFD team communicates less on planning information and less with members of the firm external to the team. If this paucity of external communication means that the team has the information it needs for product development, and the QFD process has provided an effective means for moving the information through the team, it is a positive impact of QFD. If the result means that QFD induces team insularity, even when the team needs to reach out to external information sources, it is a cause for concern.", 
            "authors": [
                "Abbie Griffin", 
                "John R Hauser"
            ], 
            "fields": [
                "Engineering", 
                "New product development", 
                "Fabrication", 
                "Software development process", 
                "Marketing"
            ], 
            "title": "Patterns of communication among marketing, engineering and manufacturing: a comparison between two new product teams", 
            "url": "http://cn.bing.com/academic/profile?id=2544a04c2cbd7330febecf3e57455ebb&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The application of the chirped-pulse amplification technique to solid-state lasers combined with the availability of broad-bandwidth materials has made possible the development of small-scale terawatt and now even petawatt (1000-terawatt) laser systems. The laser technology used to produce these intense pulses and examples of new phenomena resulting from the application of these systems to atomic and plasma physics are described.", 
            "authors": [
                "M D Perry", 
                "Gerard A Mourou"
            ], 
            "fields": [
                "Atomic physics", 
                "Physics", 
                "Chirped pulse amplification", 
                "Plasma", 
                "Design"
            ], 
            "title": "Terawatt to petawatt subpicosecond lasers", 
            "url": "http://cn.bing.com/academic/profile?id=9aba0576982df792ab9f6b3dccbb7fdd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework.", 
            "authors": [
                "Song Chun Zhu", 
                "Yingnian Wu", 
                "David Mumford"
            ], 
            "fields": [
                "Random field", 
                "Engineering", 
                "Gibbs sampling", 
                "Probability distribution", 
                "Systems modeling"
            ], 
            "title": "Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling", 
            "url": "http://cn.bing.com/academic/profile?id=ccdeb96f367bfda008157970481fd203&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Note onset detection and localization is useful in a number of analysis and indexing techniques for musical signals. The usual way to detect onsets is to look for \"transient\" regions in the signal, a notion that leads to many definitions: a sudden burst of energy, a change in the short-time spectrum of the signal or in the statistical properties, etc. The goal of this paper is to review, categorize, and compare some of the most commonly used techniques for onset detection, and to present possible enhancements. We discuss methods based on the use of explicitly predefined signal features: the signal's amplitude envelope, spectral magnitudes and phases, time-frequency representations; and methods based on probabilistic signal models: model-based change point detection, surprise signals, etc. Using a choice of test cases, we provide some guidelines for choosing the appropriate method for a given application.", 
            "authors": [
                "Juan Pablo Bello", 
                "Laurent Daudet", 
                "Samer A Abdallah", 
                "C Duxbury", 
                "Matthew E P Davies", 
                "Mark Sandler"
            ], 
            "fields": [
                "Acoustics", 
                "Time\u2013frequency representation", 
                "Time\u2013frequency analysis", 
                "Change detection", 
                "Detection theory"
            ], 
            "title": "A tutorial on onset detection in music signals", 
            "url": "http://cn.bing.com/academic/profile?id=aaf24e3baa861bb88523ad993d2c5d60&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Employing nonequilibrium molecular\u2010dynamics methods the effects of two energy loss mechanisms on viscosity, stress, and granular\u2010temperature in assemblies of nearly rigid, inelastic frictional disks undergoing steady\u2010state shearing are calculated. Energy introduced into the system through forced shearing is dissipated by inelastic normal forces or through frictional sliding during collisions resulting in a natural steady\u2010state kinetic energy density (granular\u2010temperature) that depends on the density and shear rate of the assembly and on the friction and inelasticity properties of the disks. The calculations show that both the mean deviatoric particle velocity and the effective viscosity of a system of particles with fixed friction and restitution coefficients increase almost linearly with strain rate. Particles with a velocity\u2010dependent coefficient of restitution show a less rapid increase in both deviatoric velocity and viscosity as strain rate increases. Particles with highly dissipative interactions result in anisotropicpressure and velocity distributions in the assembly, particularly at low densities. At very high densities the pressure also becomes anisotropic due to high contact forces perpendicular to the shearing direction. The mean rotational velocity of the frictional disks is nearly equal to one\u2010half the shear rate. The calculated ratio of shear stress to normal stress varies significantly with density while the ratio of shear stress to total pressure shows much less variation. The inclusion of surface friction (and thus particle rotation) decreases shear stress at low density but increases shear stress under steady shearing at higher densities.", 
            "authors": [
                "Otis R Walton", 
                "R L Braun"
            ], 
            "fields": [
                "Viscosity", 
                "Strain rate", 
                "Shear", 
                "Engineering", 
                "Kinetic energy"
            ], 
            "title": "Viscosity, granular-temperature, and stress calculations for shearing assemblies of inelastic, frictional disks", 
            "url": "http://cn.bing.com/academic/profile?id=689a946eb60dd808d7044cc7b4f9efae&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The article presents a method for both the unsupervised partitioning of a sample of data and the estimation of the possible number of inherent clusters which generate the data. This work exploits the notion that performing a nonlinear data transformation into some high dimensional feature space increases the probability of the linear separability of the patterns within the transformed space and therefore simplifies the associated data structure. It is shown that the eigenvectors of a kernel matrix which defines the implicit mapping provides a means to estimate the number of clusters inherent within the data and a computationally simple iterative procedure is presented for the subsequent feature space partitioning of the data.", 
            "authors": [
                "Mark Girolami"
            ], 
            "fields": [
                "Data structure", 
                "Computer Science", 
                "Unsupervised learning", 
                "Engineering", 
                "Eigenvalues and eigenvectors"
            ], 
            "title": "Mercer kernel-based clustering in feature space", 
            "url": "http://cn.bing.com/academic/profile?id=6af1a7792ca32b3da326e7ce317d663a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A simple technique for in situ measurements of pulsed Gaussian-beam spot sizes is reported. This technique is particularly useful for measurements on highly focused beam spots. It can also be used for absolute calibration of the threshold-energy fluences for pulsed-laser-induced effects. The thresholds for several effects in picosecond-laser-induced phase transformation on silicon-crystal surfaces are calibrated with this technique.", 
            "authors": [
                "J M Liu"
            ], 
            "fields": [
                "Crystal structure", 
                "Gaussian function", 
                "Periodic Table", 
                "Distribution", 
                "Engineering"
            ], 
            "title": "Simple technique for measurements of pulsed Gaussian-beam spot sizes", 
            "url": "http://cn.bing.com/academic/profile?id=fab10e71a1999b647be3854e7e9148dd&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "We report results on the performance of a free-electron laser operating at a wavelength of 13.7 nm where unprecedented peak and average powers for a coherent extreme-ultraviolet radiation source have been measured. In the saturation regime, the peak energy approached 170 mu J for individual pulses, and the average energy per pulse reached 70 mJ. The pulse duration was in the region of 10 fs, and peak powers of 10 GW were achieved. At a pulse repetition frequency of 700 pulses per second, the average extreme-ultraviolet power reached 20 mW. The output beam also contained a significant contribution from odd harmonics of approximately 0.6% and 0.03% for the 3rd (4.6 nm) and the 5th (2.75 nm) harmonics, respectively. At 2.75 nm the 5th harmonic of the radiation reaches deep into the water window, a wavelength range that is crucially important for the investigation of biological samples.", 
            "authors": [
                "Wolfgang Ackermann", 
                "G Asova", 
                "V Ayvazyan", 
                "A Azima", 
                "N Baboi", 
                "J Bahr", 
                "V P Balandin", 
                "B Beutner", 
                "A Brandt", 
                "A Bolzmann", 
                "Robert Brinkmann", 
                "Oleg Brovko", 
                "M Castellano", 
                "P Castro", 
                "L Catani", 
                "E Chiadroni", 
                "S Choroba", 
                "A Cianchi", 
                "J T Costello", 
                "D Cubaynes", 
                "J Dardis", 
                "Winfried Decking", 
                "H Delsimhashemi", 
                "A Delserieys", 
                "G D\u2026", 
                ""
            ], 
            "fields": [
                "3D optical data storage", 
                "High harmonic generation", 
                "Extreme ultraviolet", 
                "Wavelength", 
                "Technology"
            ], 
            "title": "Operation of a free-electron laser from the extreme ultraviolet to the water window", 
            "url": "http://cn.bing.com/academic/profile?id=1dc62cf7ff5d73730efa208cad9d7f92&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Given a multiple-antenna source and a multiple-antenna destination, a multiple-antenna relay between the source and the destination is desirable under useful circumstances. A non-regenerative multiple-antenna relay, also called non-regenerative MIMO (multi-input multi-output) relay, is designed to optimize the capacity between the source and the destination. Without a direct link between the source and the destination, the optimal canonical coordinates of the relay matrix are first established, and the optimal power allocations along these coordinates are then found. The system capacity with the optimal relay matrix is shown to be significantly higher than those with heuristic relay matrices. When a direct link is present, upper and lower bounds of the optimal system capacity are discussed.", 
            "authors": [
                "Xiaojun Tang", 
                "Yingbo Hua"
            ], 
            "fields": [
                "Multidisciplinary design optimization", 
                "Mobile ad hoc network", 
                "Upper and lower bounds", 
                "MIMO", 
                "Radio repeater"
            ], 
            "title": "Optimal Design of Non-Regenerative MIMO Wireless Relays", 
            "url": "http://cn.bing.com/academic/profile?id=07e7adc2f5153eda0d3433234ffa57d8&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "The change in refractive index Delta n produced by injection of free carriers in InP, GaAs, and InGaAsP is theoretically estimated. Bandfilling (Burstein-Moss effect), bandgap shrinkage, and free-carrier absorption (plasma effect) are included. Carrier concentrations of 10/sup 16//cm/sup 3/ to 10/sup 19//cm/sup 3/ and photon energies of 0.8 to 2.0 eV are considered. Predictions for Delta n are in reasonably good agreement with the limited experimental data available. Refractive index changes as large as 10/sup -2/ are predicted for carrier concentrations of 10/sup 8//cm/sup 3/ suggested that low-loss optical phase modulators and switches using carrier injection are feasible in these materials. >", 
            "authors": [
                "Brian R Bennett", 
                "Richard A Soref", 
                "J A Del Alamo"
            ], 
            "fields": [
                "Band gap", 
                "Arsenic", 
                "Mobile computing", 
                "Optical switch", 
                "Massless particle"
            ], 
            "title": "Carrier-induced change in refractive index of InP, GaAs and InGaAsP", 
            "url": "http://cn.bing.com/academic/profile?id=3c3b0b9aad69aa6a31df79bc24e1f551&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Power consumption is forecast by the International Technology Roadmap of Semiconductors (ITRS) to pose long-term technical challenges for the semiconductor industry. The purpose of this paper is threefold: (1)?to provide an overview of strategies for powering MEMS via non-regenerative and regenerative power supplies; (2)?to review the fundamentals of piezoelectric energy harvesting, along with recent advancements, and (3)?to discuss future trends and applications for piezoelectric energy harvesting technology. The paper concludes with a discussion of research needs that are critical for the enhancement of piezoelectric energy harvesting devices.", 
            "authors": [
                "K A Cookchennault", 
                "N Thambi", 
                "Ann Marie Sastry"
            ], 
            "fields": [
                "Energy harvesting", 
                "Energy recovery", 
                "Electric generator", 
                "Engineering"
            ], 
            "title": "Powering MEMS portable devices?a review of non-regenerative and regenerative power supply systems with special emphasis on piezoelectric energy harvesting systems", 
            "url": "http://cn.bing.com/academic/profile?id=aef27ce9aad1f9d128fb7a24af52c57a&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N#A parametric model is developed to describe relative permeability-saturation-fluid pressure functional relationships in two- or three-fluid phase porous media systems subject to monotonic saturation paths. All functions are obtained as simple closed-form expressions convenient for implementation in numerical multiphase flow models. Model calibration requires only relatively simple determinations of saturation-pressure relations in two-phase systems. A scaling procedure is employed to simplify the description of two-phase saturation-capillary head relations for arbitrary fluid pairs and experimental results for two porous media are presented to demonstrate its applicability. Extension of two-phase relations to three- phase systems is obtained under the assumption that fluid wettability follows the sequence water > nonaqueous phase liquid > air. Expressions for fluid relative permeabilities are derived from the scaled saturation-capillary head function using a flow channel distribution model to estimate effective mean fluid-conducting pore dimensions. Constraints on model application are discussed.", 
            "authors": [
                "J C Parker", 
                "R J Lenhard", 
                "T Kuppusamy"
            ], 
            "fields": [
                "Convection", 
                "Pollutant", 
                "Groundwater", 
                "Relative permeability", 
                "Materials Science"
            ], 
            "title": "A parametric model for constitutive properties governing multiphase flow in porous media", 
            "url": "http://cn.bing.com/academic/profile?id=f4e6b87260b53815adb624622657b60b&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract#R##N##R##N#SEM of etched NCH film.#R##N##R##N##R##N##R##N#Summary: More than twenty years have passed since we invented PCN, in which only a few wt.-% of silicate is randomly and homogeneously dispersed in the polymer matrix. When molded, these nanocomposites show superior properties compared to those of pristine polymers. The number of papers on PCN has increased rapidly in recent years, reaching over 500 in 2005 alone. Being pioneers of this new technology, we review its history relative to the following epochal events: #R##N##R##N##R##N##R##N#In 1985 we invented nylon 6-clay hybrid (NCH), the first PCN.#R##N##R##N##R##N##R##N##R##N#In 1989, cars equipped with a NCH part were launched.#R##N##R##N##R##N##R##N##R##N#In 1997, Gilman found revolutionary fire retardancy in NCH.#R##N##R##N##R##N##R##N##R##N#In 1997, a PP-clay nanocomposite was prepared using a compatibilizer.#R##N##R##N##R##N##R##N##R##N#In 1998, a compounding method for producing PCN was completed.#R##N##R##N##R##N##R##N##R##N#In 2002, Haraguchi invented a revolutionary nanocomposite hydrogel.#R##N##R##N##R##N##R##N##R##N#So far, only nylon-clay nanocomposites have been used in practice, but other PCN will become increasingly useful in the future.", 
            "authors": [
                "Akane Okada", 
                "Arimitsu Usuki"
            ], 
            "fields": [
                "Hybrid", 
                "Logical conjunction", 
                "Engineering", 
                "Physical property", 
                "Nanocomposite"
            ], 
            "title": "Twenty Years of Polymer\u2010Clay Nanocomposites", 
            "url": "http://cn.bing.com/academic/profile?id=a97446f7b7394d7d77a6b9a11cf70d3c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "Abstract: We present a new adaptive kernel density estimator based on linear diffusion processes. The proposed estimator builds on existing ideas for adaptive smoothing by incorporating information from a pilot density estimate. In addition, we propose a new plug-in bandwidth selection method that is free from the arbitrary normal reference rules used by existing methods. We present simulation examples in which the proposed approach outperforms existing methods in terms of accuracy and reliability.", 
            "authors": [
                "Zdravko I Botev", 
                "Joseph F Grotowski", 
                "Dirk P Kroese"
            ], 
            "fields": [
                "Kernel density estimation", 
                "Survival analysis", 
                "Density estimation", 
                "Smoothing", 
                "Industrial history"
            ], 
            "title": "Kernel density estimation via diffusion", 
            "url": "http://cn.bing.com/academic/profile?id=676d923a769d4395e718c83859b2b110&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This study provides a detailed literature review and an assessment of results of the research and development work forming the current status of nanofluid technology for heat transfer applications. Nanofluid technology is a relatively new field, and as such, the supporting studies are not extensive. Specifically, experimental results were reviewed in this study regarding the enhancement of the thermal conductivity and convective heat transfer of nanofluids relative to conventional heat transfer fluids, and assessments were made as to the state-of-the-art of verified parametric trends and magnitudes. Pertinent parameters of particle volume concentration, particle material, particle size, particle shape, base fluid material, temperature, additive, and acidity were considered individually, and experimental results from multiple research groups were used together when assessing results. To this end, published research results from many studies were recast using a common parameter to facilitate comparisons of ...", 
            "authors": [
                "Wenhua Yu", 
                "D M France", 
                "J L Routbort", 
                "Stephen U S Choi"
            ], 
            "fields": [
                "pH", 
                "Nanoparticle", 
                "Critical heat flux", 
                "Convective heat transfer", 
                "Heat transfer"
            ], 
            "title": "Review and Comparison of Nanofluid Thermal Conductivity and Heat Transfer Enhancements", 
            "url": "http://cn.bing.com/academic/profile?id=e6f2603004284624d8393c19f1cd3f21&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper considers the problem of motion planning for a car-like robot (i.e., a mobile robot with a nonholonomic constraint whose turning radius is lower-bounded). We present a fast and exact planner for our mobile robot model, based upon recursive subdivision of a collision-free path generated by a lower-level geometric planner that ignores the motion constraints. The resultant trajectory is optimized to give a path that is of near-minimal length in its homotopy class. Our claims of high speed are supported by experimental results for implementations that assume a robot moving amid polygonal obstacles. The completeness and the complexity of the algorithm are proven using an appropriate metric in the configuration space R/sup 2//spl times/S/sup 1/ of the robot. This metric is defined by using the length of the shortest paths in the absence of obstacles as the distance between two configurations. We prove that the new induced topology and the classical one are the same. Although we concentrate upon the car-like robot, the generalization of these techniques leads to new theoretical issues involving sub-Riemannian geometry and to practical results for nonholonomic motion planning. >", 
            "authors": [
                "Jeanpaul Laumond", 
                "Paul E Jacobs", 
                "Michel Taix", 
                "Richard M Murray"
            ], 
            "fields": [
                "Shortest path problem", 
                "Planning", 
                "Completeness", 
                "Engineering", 
                "Mobile robot"
            ], 
            "title": "A motion planner for nonholonomic mobile robots", 
            "url": "http://cn.bing.com/academic/profile?id=f705dadafe94e3396bf823436b5ffd6c&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "This paper develops a new framework for examining the determinants of wage distributions that emphasizes within-industry reallocation, labor market frictions, and differences in workforce composition across firms. More productive firms pay higher wages and exporting increases the wage paid by a firm with a given productivity. The opening of trade enhances wage inequality and can either raise or reduce unemployment. While wage inequality is higher in a trade equilibrium than in autarky, gradual trade liberalization first increases and later decreases inequality. Copyright 2010 The Econometric Society.", 
            "authors": [
                "Elhanan Helpman", 
                "Oleg Itskhoki", 
                "Stephen J Redding"
            ], 
            "fields": [
                "Econometrics", 
                "Risk", 
                "Industrial history", 
                "Numerical analysis", 
                "Economy"
            ], 
            "title": "INEQUALITY AND UNEMPLOYMENT IN A GLOBAL ECONOMY", 
            "url": "http://cn.bing.com/academic/profile?id=96df62251dd0c9dd0c261e41c2c91791&encoded=0&v=paper_preview&mkt=zh-cn"
        }, 
        {
            "abstract": "A new method for detecting spikes in acoustic Doppler velocimeter data sequences is suggested. The method combines three concepts: (1) that differentiation enhances the high frequency portion of a signal, (2) that the expected maximum of a random series is given by the Universal threshold, and (3) that good data cluster in a dense cloud in phase space or Poincare maps. These concepts are used to construct an ellipsoid in three-dimensional phase space, then points lying outside the ellipsoid are designated as spikes. The new method is shown to have superior performance to various other methods and it has the added advantage that it requires no parameters. Several methods for replacing sequences of spurious data are presented. A polynomial fitted to good data on either side of the spike event, then interpolated across the event, is preferred by the authors.", 
            "authors": [
                "D Goring", 
                "Vladimir Nikora"
            ], 
            "fields": [
                "Polynomial", 
                "Acoustic Doppler velocimetry", 
                "Engineering", 
                "Instrumentation", 
                "Measuring instrument"
            ], 
            "title": "Despiking Acoustic Doppler Velocimeter Data", 
            "url": "http://cn.bing.com/academic/profile?id=53cb0a615bb4e22c3a818653e43fe065&encoded=0&v=paper_preview&mkt=zh-cn"
        }
    ], 
    "num": 103
}